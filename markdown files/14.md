# Residual Prophet Inequalities

Jose Correa<sup>1</sup> , Sebastian Perez-Salazar2,3, Dana Pizarro<sup>4</sup> , Bruno Ziliotto<sup>5</sup>

<sup>1</sup>Department of Industrial Engineering, Universidad de Chile, Chile.

<sup>2</sup>Department of Computational Applied Mathematics and Operations Research, Rice

University, USA.

<sup>3</sup>Ken Kennedy Institute, Rice University, USA.

<sup>4</sup>Department of Information, Operations and Decision Sciences, TBS Business School,

France.

<sup>5</sup>Toulouse School of Economics, Universit´e Toulouse Capitole, Institut de Math´ematiques de Toulouse, CNRS UMR 5219, France.

#### Abstract

We introduce a variant of the classic prophet inequality, called residual prophet inequality (k-RPI). In the k-RPI problem, we consider a finite sequence of n nonnegative independent random values with known distributions, and a known integer 0 ≤ k ≤ n − 1. Before the gambler observes the sequence, the top k values are removed, whereas the remaining n − k values are streamed sequentially to the gambler. For example, one can assume that the top k values have already been allocated to a higherpriority agent. Upon observing a value, the gambler must decide irrevocably whether to accept or reject it, without the possibility of revisiting past values. We study two variants of k-RPI, according to whether the gambler learns online of the identity of the variable that he sees (FI model) or not (NI model). Our main result is a randomized algorithm in the FI model with competitive ratio of at least 1/(k + 2), which we show is tight. Our algorithm is data-driven and requires access only to the k + 1 largest values of a single sample from the n input distributions. In the NI model, we provide a similar algorithm that guarantees a competitive ratio of 1/(2k + 2). We further analyze independent and identically distributed instances when k = 1. We build a single-threshold algorithm with a competitive ratio of at least 0.4901, and show that no single-threshold strategy can get a competitive ratio greater than 0.5464.

Keywords: Prophet inequalities, Competitive ratio, Online algorithms

## 1 Introduction

The prophet inequality is a classical model in optimal stopping theory [\(Krengel and Sucheston,](#page-29-0) [1977;](#page-29-0) [Hill and Kertz,](#page-29-1) [1982;](#page-29-1) [Samuel-Cahn,](#page-30-0) [1984\)](#page-30-0). In its simplest form, a finite sequence of n independent and nonnegative random variables X1, . . . , X<sup>n</sup> is observed sequentially by a gambler. Upon observing the ith value X<sup>i</sup> , the gambler has to irrevocably accept the value and stop the process or reject the value and observe the next value in the sequence, if any. The gambler's goal is to devise an online algorithm that maximizes the expected accepted value. The quality of an algorithm is measured by means of the competitive ratio which is the fraction between the expected value obtained by the algorithm and the expected optimal offline value E(max<sup>i</sup> Xi), the so-called prophet value. The competitive ratio thus measures the loss experienced by a gambler, who inspects the values sequentially, with respect to a prophet who knows the entire sequence of values upfront. Surprisingly, [Samuel-Cahn](#page-30-0) [\(1984\)](#page-30-0) showed that a simple single-threshold rule guarantees a competitive ratio of at least 1/2 and this is tight. Prophet inequalities have received renewed attention due to their applicability in posted price mechanisms and auction theory [\(Chawla et al.,](#page-28-0) [2010a;](#page-28-0) [Correa et al.,](#page-28-1) [2019;](#page-28-1) [Hajiaghayi et al.,](#page-29-2) [2007a\)](#page-29-2) and have become a

cornerstone modeling tool for online algorithms in Bayesian scenarios and resource allocation [\(Gallego](#page-29-3) [and Segev,](#page-29-3) [2022;](#page-29-3) [Goyal and Udwani,](#page-29-4) [2023;](#page-29-4) [Huang and Zhang,](#page-29-5) [2020\)](#page-29-5).

In this work we introduce the residual prophet inequality (k-RPI) problem: For a fixed integer 0 ≤ k ≤ n−1, the k variables corresponding to the top k realizations in the sequence X1, . . . , X<sup>n</sup> are removed before the gambler observes the sequence. The gambler's goal is to maximize the expected accepted value among the remaining n − k variables.

The k-RPI problem can be regarded as a robust version of the classical prophet inequality problem (case k = 0), where high values are impossible to obtain due to exogenous factors. The k-RPI problem is very general and naturally relates to problems such as the postdoc problem [\(Vanderbei,](#page-30-1) [2012;](#page-30-1) [Rose,](#page-30-2) [1982\)](#page-30-2), which have applications in hiring problems [\(Abels et al.,](#page-28-2) [2023;](#page-28-2) [Arsenis and Kleinberg,](#page-28-3) [2022;](#page-28-3) [Disser et al.,](#page-28-4) [2020\)](#page-28-4). Specifically, one could imagine a gambler attempting to hire an employee in a highly competitive market where the top candidates are hired by leading companies, leaving the gambler to select the best applicant from the remaining pool (see also, [Perez-Salazar et al.](#page-29-6) [\(2024\)](#page-29-6)). Another related application is in advertising. Several platforms (e.g., YouTube, Spotify, Canva, Pandora) offer both a free version supported by ads and a premium version without ads. Essentially, users paying the premium opt out from observing ads, leaving the platform to focus on advertising the high-value users among the remaining free users.

An interesting aspect of k-RPI concerns the information structure. Note that since some variables have been removed and the gambler will only observe the remaining ones, two different information models can be considered, depending on whether the gambler knows the identity of the observed variable at each time or not:

- Full-information (FI) In this version, the gambler observes the n − k variables sequentially and upon observing a value, he also observes the identity (index) of the variable.
- No-information (NI) In this version, the gambler only observes the n−k remaining values after removing the largest k values.

Regardless of the information model (FI or NI), the gambler can only hope to accept a value comparable to the expectation of the largest value of the n − k non-removed values. This is the expectation of the (k + 1)-th largest value in the original sequence of n values, that is, the expectation of the (k + 1) order statistics E(X(k+1)).[1](#page-1-0)

Therefore, this latter value constitutes the prophet benchmark against which we will compare the performance of an online algorithm. Given an information model, the competitive ratio of an algorithm for k-RPI is the ratio between the expected value of the algorithm and E(X(k+1)). Hence, a competitive ratio of γ for NI k-RPI implies a competitive ratio of γ for FI k-RPI. Likewise, hard instances for FI k-RPI imply hard instances for the less informative model NI.

In contrast to the classic prophet inequality problem, the observed values in k-RPI are correlated. The following example demonstrates that correlation plays a significant role in k-RPI, rendering the single-threshold solutions from [Samuel-Cahn](#page-30-0) [\(1984\)](#page-30-0) and [Kleinberg and Weinberg](#page-29-7) [\(2012a\)](#page-29-7) unsuitable for direct application to k-RPI.

<span id="page-1-1"></span>Example 1. Consider the following instance of k-RPI with k = 1, n = 3, X<sup>1</sup> = 1 with probability (w.p.) 1 and X<sup>2</sup> and X<sup>3</sup> both independent and identically distributed (i.i.d.) taking value 1/ε<sup>2</sup> w.p. ε < 1/2, and 0 otherwise.

The quantity E(X(2)) is given by

$$
\mathbb{E}(X_{(2)}) = \varepsilon^2 \varepsilon^{-2} + 2\varepsilon (1 - \varepsilon) = 1 + 2\varepsilon - 2\varepsilon^2.
$$

Let us analyze the performance of the strategy with the single-threshold solutions from from [Samuel-](#page-30-0)[Cahn](#page-30-0) [\(1984\)](#page-30-0) and [Kleinberg and Weinberg](#page-29-7) [\(2012a\)](#page-29-7), that is E(X(2))/2 and the median of X(2). For the former, a case analysis shows that the gambler gets 0 when X<sup>2</sup> = X<sup>3</sup> = 0, which happens with probability (1 − ε) <sup>2</sup> and gets 1 with the remaining probability. Thus, the gambler gets in expectation:

$$
\mathbb{E}(Alg) = 1 - (1 - \varepsilon)^2 = 2\varepsilon - \varepsilon^2,
$$

<span id="page-1-0"></span><sup>1</sup>We assume that the order statistic of the variables X1, . . . , X<sup>n</sup> are ordered as X(1) ≥ · · · ≥ X(n). Note that this ordering is the reverse of the convention commonly used in the literature.

and therefore, we have

$$
\frac{\mathbb{E}(Alg)}{\mathbb{E}(X_{(2)})} = \frac{2\varepsilon - \varepsilon^2}{1 + 2\varepsilon - 2\varepsilon^2} < \frac{1}{2}.
$$

Moreover, <sup>E</sup>(Alg) <sup>E</sup>(X(2)) <sup>=</sup> <sup>O</sup>(ε) and then the gambler cannot guarantee a constant factor of <sup>E</sup>(X(2)) using this fixed threshold. Furthermore, note that the median of X(2) is 0, so using any threshold between the median and E(X(2)) will not produce a different result. In fact, any strategy that accepts value 1 is ineffective because, once the value 1 has been observed, the expectation of the second variable is 1/((2 − ε)ε) ≫ 1. Such a positive correlation between the two observed variables is what makes classic strategies fail.

### 1.1 Results and technical contributions

The previous example illustrates that correlation plays a major role for k-RPI. The examples also show that traditional and well-liked thresholds such as the median or the expectation of X(k+1) can be arbitrarily poor choices. This is contrary to the negative correlated case where we can guarantee a competitive ratio of 1/2 [\(Rinott and Samuel-Cahn,](#page-30-3) [1987,](#page-30-3) [1991,](#page-30-4) [1992\)](#page-30-5), as in the independent case. Our first contribution is a new algorithmic approach that bypasses this hardness.

#### Main Result [Lower bound on competitive ratio]

We show that in the full information model of k-RPI, there exists an algorithm with a competitive ratio of at least 1/(k + 2). Our algorithm first samples one value from each input distribution and randomly selects one of the k + 1 largest values in the sample. It then uses this value and the identities of the arrivals to perform the online selection. The randomization is independent of the input, and we note that our algorithm extends the approach of [Rubinstein et al.](#page-30-6) [\(2020\)](#page-30-6) for the classic prophet inequality problem. We present the details of our algorithm and its analysis in Section [3.](#page-4-0)

Our algorithmic solution for the FI k-RPI is robust in some key aspects. On one hand, it works against any arrival order making it highly applicable in online problems. On the other, by construction, it does not need to know the distributions of each variable, but only requires one sample from each variable. This is particularly important for applications in posted price mechanisms, where consumer valuation distributions are typically unknown, and only a limited amount of past sales data is available.

For the no-information model of the k-RPI, we prove that there exists a single-threshold strategy with a competitive ratio of at least 1/(2k + 2). The idea is similar to that of the FI k-RPI model, but the algorithm uses only one of the top k + 1 sample values—selected uniformly at random—as the threshold for making the online selection. The lack of information regarding the identities of the removed variables leads to a degradation in the competitive guarantee. Nevertheless, this guarantee can be transferred to the FI k-RPI model, showing that it is possible to achieve a constant-factor approximation of E(X(k+1)) using a single-threshold strategy.

Next, we prove that our main result for the FI model is best possible.

#### Tightness [Upper bound on competitive ratio]

For any information model of k-RPI, there is no algorithm that has a competitive ratio larger than 1/(k+ 2). To provide this negative result, we construct a hard instance in the FI k-RPI model; which will imply the negative result for the model with less information. Our instance extends the hard instance for the classic prophet inequality problem: it consists of a sequence X1, . . . , X2(k+1) of two-point distributions, where each X<sup>i</sup> ∈ {0, ai}. The values a<sup>i</sup> are positive and increase rapidly with i, while the events X<sup>i</sup> = a<sup>i</sup> occur rarely for i larger than k + 1. We provide the details in Section [4.](#page-17-0)

Our hard instance unveils that part of the hardness of the k-RPI problem stems from the order in which the values are observed by the gambler. For every information model of k-RPI, if the gambler observes the values in random order (RO), then there is an algorithm with a competitive ratio at least 1/e. The algorithm is a straightforward application of the secretary problem algorithm [Lindley](#page-29-8) [\(1961\)](#page-29-8); [Dynkin](#page-29-9) [\(1963\)](#page-29-9); [Ferguson](#page-29-10) [\(1989\)](#page-29-10); [Gilbert and Mosteller](#page-29-11) [\(1966\)](#page-29-11). Indeed, one can easily see that, in values are presented in random order, even in the NI k-RPI, the standard algorithm that scans the first (n−k)/e values without picking any and then takes the first value which surpasses all previously seen, guarantees a competitive ratio of at least 1/e. To see this, let X1, . . . , X<sup>n</sup> be the random values and let X(k+1) ≥ · · · ≥ X(n) be the n − k values observed by the gambler. From the standard analysis for the secretary

problem, we are guaranteed that the gambler accepts X(k+1) with probability at least 1/e. From this, the result follows.

Our last result is an exploration of the independent and identically distributed (i.i.d.) k-RPI problem where X1, . . . , X<sup>n</sup> are drawn from the same distribution.

#### Additional result [i.i.d. k-RPI, k = 1]

The previous observation implies that for i.i.d. random variables and arbitrary k, we can always guarantee a factor 1/e using the classic secretary algorithm. This shows a stark difference between k-RPI and its i.i.d. counterpart and indeed even for small k this improves upon our tight factor of 1/(k + 2) for k-RPI. Therefore, it is interesting to explore the gap between the i.i.d. and the independent versions of the problem, even in the case k = 1. We prove that for both information models of 1-RPI, if the values X1, . . . , X<sup>n</sup> are i.i.d., then there exists an algorithm with a competitive ratio 0.4901. Our algorithm here is more standard. We propose a single-threshold strategy for NI 1-RPI, which determines the threshold τ via Pr(X ≥ τ ) = q, where q is an input quantile. Our analysis follows a quantile-based approach, expressing both the expected value of the algorithm and the optimal value E(X(2)) as functions of quantiles. By comparing their ratio, we derive a lower bound that depends solely on q. Optimizing over q yields the desired result. We also establish that no single-threshold strategy can achieve a competitive ratio greater than 0.5464 in any information model. This result shows that the optimal competitive ratio of 1−1/e for single-threshold strategies [\(Correa et al.,](#page-28-5) [2021;](#page-28-5) [Hill and Kertz,](#page-29-1) [1982\)](#page-29-1), attained when k = 0, cannot be recovered for k ≥ 1. We present the details in Section [5.](#page-19-0)

### 1.2 Related Literature

The prophet inequality problem, as introduced by [Krengel and Sucheston](#page-29-0) [\(1977\)](#page-29-0), was resolved by using a dynamic program that gave a tight approximation ratio of 1/2. [Samuel-Cahn](#page-30-0) [\(1984\)](#page-30-0) later proved that a single-threshold strategy yields the same guarantee; this also showed that the order in which the variables are observed is immaterial. The renewed interest in prophet inequalities is due to their relevance to auctions, specifically posted priced mechanisms (PPMs) in online sales [\(Alaei,](#page-28-6) [2014;](#page-28-6) [Chawla et al.,](#page-28-7) [2010b;](#page-28-7) [D¨utting et al.,](#page-29-12) [2020;](#page-29-12) [Hajiaghayi et al.,](#page-29-13) [2007b;](#page-29-13) [Kleinberg and Weinberg,](#page-29-14) [2012b\)](#page-29-14). It was implicitly shown by [Chawla et al.](#page-28-7) [\(2010b\)](#page-28-7) and [Hajiaghayi et al.](#page-29-13) [\(2007b\)](#page-29-13) that every prophet-type inequality implies a corresponding approximation guarantee in a PPM, and the converse is true as well [\(Correa et al.,](#page-28-1) [2019\)](#page-28-1).

The closest work to ours is likely that of [Rubinstein et al.](#page-30-6) [\(2020\)](#page-30-6), where the authors used the principle of deferred decision to prove that a single sample from each distribution is sufficient to achieve a competitive ratio of 1/2 for the classic prophet inequality. This technique has also been applied to other optimal stopping problems (see, e.g., [Correa et al.](#page-28-8) [\(2022\)](#page-28-8); [Nuti and Vondr´ak](#page-29-15) [\(2023\)](#page-29-15)).

In essence, after obtaining one sample from each distribution, [Rubinstein et al.](#page-30-6) [\(2020\)](#page-30-6) sets the threshold as the maximum of these samples. Although our proof for the general case is also based on this principle, the analysis is much more intricate due to the complexity of the k-RPI problem, which necessitates a more sophisticated algorithm. Specifically, for our approach to be effective, it is insufficient to simply use a threshold based on the j-th order statistic of the sample set for some fixed j. Instead, the algorithm first selects j according to a carefully chosen distribution. Moreover, in the FI model, the algorithm must discard certain elements based on their identity, even when their values exceed the j-th order statistic. Thus, in contrast to Rubinstein's work, where j is deterministically fixed at 1, our approach introduces an additional layer of randomization, and the j-th order statistic is not exactly used as a threshold in the FI model.

There has been a growing interest in competitive versions of online selection problems [\(Ezra et al.,](#page-29-16) [2021;](#page-29-16) [Gensbittel et al.,](#page-29-17) [2024;](#page-29-17) [Immorlica et al.,](#page-29-18) [2006;](#page-29-18) [Karlin and Lei,](#page-29-19) [2015;](#page-29-19) [Ramsey,](#page-30-7) [2024\)](#page-30-7). The closest paper in this stream of literature to ours is the one by [Ezra et al.](#page-29-16) [\(2021\)](#page-29-16), where the authors consider a generalization of the prophet inequality problem with k + 1 gamblers. Gambler j observes the sequence after the first j − 1 gamblers have gone through the sequence, and they study reward guarantees under single-threshold strategies. Note that, in our case, we can imagine that there are k + 1 gamblers but the first k gamblers are all-mighty. These k gamblers are not strategic, hence we do not need a game-theoretic analysis, unlike in the aforementioned papers on competitive prophet inequalities.

## 2 Model

For 0 ≤ k ≤ n − 1, an instance of k-RPI is given by a sequence X1, . . . , X<sup>n</sup> of nonnegative independent random variables, where X<sup>i</sup> has cumulative density function (cdf) F<sup>i</sup> . Nature removes k variables corresponding to the top k realizations,[2](#page-4-1) and we denote by D the corresponding set of indices of the remaining variables. We consider two information models that determines what the gambler observes sequentially.

In the full information (FI) model, the gambler observes online the pairs (X<sup>i</sup> , i)i∈D. That is, the gambler observes both the value and the index of the random variable from which the value originates.

In the no information (NI) model, the gambler only observes online the n − k values in the sequence (Xi)i∈D. In both information models, D is unknown to the gambler upfront. Given an information model (FI or NI), the gambler wants to implement an online algorithm ALG that observes the online values according to the information model and accepts a value. Regardless of the model, and abusing notation, we denote by ALG the value accepted by the online algorithm. The expected optimal offline solution corresponds to E (maxi∈<sup>D</sup> Xi) = E(X(k+1)).

For γ > 0, we say that ALG has a competitive ratio γ if E(ALG) ≥ γ · E(X(k+1)) for any input of k-RPI. For each k, we are interested in finding the largest γ<sup>k</sup> such that there is an algorithm ALG with competitive ratio γ<sup>k</sup> for k-RPI. Note that for k = 0, we have γ<sup>0</sup> = 1/2 [\(Samuel-Cahn,](#page-30-0) [1984\)](#page-30-0). We note that an algorithm with a competitive ratio γ for the NI model implies an algorithm with competitive ratio γ for the FI model.

## <span id="page-4-0"></span>3 Lower bound on competitive ratio

In this section, we prove our main result. We assume that the distributions F1, . . . , F<sup>n</sup> are independent but not necessarily identically distributed.

<span id="page-4-2"></span>Theorem 1. For the FI model, there is an algorithm for k-RPI with competitive ratio at least 1/(k + 2).

<span id="page-4-3"></span>Theorem 2. For the NI model, there is a single-threshold algorithm for k-RPI with competitive ratio at least 1/(2k + 2).

To prove both Theorem [1](#page-4-2) and Theorem [2,](#page-4-3) we employ a randomized strategy. In the case of Theorem [2,](#page-4-3) the strategy is, in fact, a randomized threshold strategy. We highlight here that, as a corollary of Theorem [2,](#page-4-3) we obtain that in the FI model, there exists a threshold strategy with a competitive ratio of at least <sup>1</sup> 2(k+1) .

To understand the rationale behind the construction of our randomized strategies to prove Theorem [1](#page-4-2) and Theorem [2,](#page-4-3) let us recall the result obtained by [Rubinstein et al.](#page-30-6) [\(2020\)](#page-30-6) in the classic prophet inequality setting. By drawing one sample from each distribution and taking the maximum of them as a threshold, the gambler can guarantee a competitive ratio of 1/2. A natural adaptation of that algorithm to our setting is to consider as a threshold the (k+1)-th maximum of the samples. We denote by MSAk+1 such a strategy.

Unfortunately, such a strategy does not guarantee any constant competitive ratio. Indeed, consider again the instance in Example [1](#page-1-1) with k = 1.

The expected value of the algorithm MSA<sup>2</sup> is:

$$
\mathbb{E}(MSA_2) = \varepsilon^2 \mathbb{E}(MSA_2 | \tau = \varepsilon^{-2}) + (1 - \varepsilon^2) \mathbb{E}(MSA_2 | \tau \le 1)
$$
  
=  $\varepsilon^2 \cdot \varepsilon^2 \cdot \varepsilon^{-2} + (1 - \varepsilon^2) \cdot [1 - (1 - \varepsilon)^2] \cdot 1$   
=  $2\varepsilon - 2\varepsilon^3 + \varepsilon^4$ .

Given that E(X(2)) → 1 when ε → 0, we obtain that E(MSA2)/E(X(2)) → 0 as ε tends to zero.

To tackle this problem and establish the competitive ratio stated in Theorem [2,](#page-4-3) we draw one sample s<sup>i</sup> from each distribution F<sup>i</sup> and consider the following k + 1 algorithms.

Definition 1. Given i ∈ {1, . . . , k + 1}, MSA<sup>i</sup> is the strategy proceeding as follows: 1. Draw one independent sample s<sup>j</sup> ∼ F<sup>j</sup> for each j = 1, . . . , n.

<span id="page-4-1"></span><sup>2</sup> If there are several choices due to ties, Nature randomizes the choice of the k variables.

- 2. Let τ be the i-th largest value among the samples.
- 3. Select the first value x<sup>t</sup> such that x<sup>t</sup> is higher than τ .

On the other hand, to prove Theorem [1,](#page-4-2) we make use of the algorithm MSAk+1 defined above, along with the following k algorithms.

Definition 2. The strategy MSAi, for i ∈ {1, . . . , k}, proceeds as follows:

- 1. Draw one independent sample s<sup>j</sup> ∼ F<sup>j</sup> for each j = 1, . . . , n.
- 2. Let τ be the i-th largest sample value, and let j ∗ be the index of the distribution from which that sample came.
- 3. Select the first value x<sup>t</sup> such that:
  - x<sup>t</sup> is higher than τ , and
  - x<sup>t</sup> does not come from distribution F<sup>j</sup> ∗ .

For both algorithms and in the case where there are equalities between samples or between the threshold and the observed value, we break ties at random. Note that the algorithms MSA<sup>i</sup> , for i ∈ {1, . . . , k}, must determine whether the arriving value originates from the same distribution as the sample used to define the threshold, and therefore, the knowledge of the identity of each variable is necessary for the online selection. A complete analysis of these algorithms is provided in Sections [3.1](#page-5-0) and [3.2.](#page-15-0)

By the principle of deferred decision and following the formalism in [Rubinstein et al.](#page-30-6) [\(2020\)](#page-30-6), instead of considering one sample for each distribution and then looking at the real values in an online fashion, we can draw two samples from each distribution F<sup>i</sup> , namely y<sup>i</sup> and z<sup>i</sup> , and then flip a fair coin to decide which is equal to s<sup>i</sup> and which is equal to x<sup>i</sup> . This procedure correctly generates s1, . . . , s<sup>n</sup> and x1, . . . , x<sup>n</sup> as independent draws of F1, . . . Fn. From now on, we will denote by S the set of samples {s1, . . . , sn} and X the set of true values {x1, . . . , xn}.

To analyze the performance of the algorithms, we assume that for each i, y<sup>i</sup> > z<sup>i</sup> and we order all these samples in decreasing order, relabeling them as w1, . . . , w2n, so that w<sup>1</sup> ≥ w<sup>2</sup> ≥ w<sup>3</sup> ≥ · · · ≥ w2<sup>n</sup> [3](#page-5-1) . We say that (w<sup>l</sup> , w<sup>l</sup> ′ ) is a pair, or that w<sup>l</sup> is paired with w<sup>l</sup> ′ , if they originate from the same distribution.

Moreover, for each j ∈ {1, . . . , k + 1} we define ξ<sup>j</sup> as the corresponding position of the j-th value z in the sequence of w ′ s values. For example, if the first elements of the w sequence are given by

$$
y_3 \ y_5 \ y_1 \ z_5 \ y_8 \ z_8 \ z_3 \ \ldots,
$$

then ξ<sup>1</sup> = 4 and ξ<sup>2</sup> = 6. Note that ξ<sup>j</sup> can also be seen as the position at which the j-th pair (y, z) from the same distribution appears. In the subsequent analysis, we fix specific realizations of the pairs (y<sup>i</sup> , zi), which in turn determine the ξ<sup>j</sup> and the w<sup>i</sup> .

### <span id="page-5-0"></span>3.1 Proof of Theorem [1](#page-4-2)

To show Theorem [1,](#page-4-2) we consider the k + 1 algorithms MSA1, . . . , MSAk, MSAk+1 defined in Section [3,](#page-4-0) and use them to define the randomized strategy MSARAND as follows:

(1) Before the game starts, select a random number I in {1, . . . , k + 1}, such that for all i ∈ {1, . . . , k}, I = i with probability 1/(k + 2), and I = k + 1 with probability 2/(k + 2).

(2) Play MSA<sup>I</sup> , if I ∈ {1, . . . , k}, and MSAk+1, if I = k + 1.

We prove Theorem [1](#page-4-2) by showing that the strategy MSARAND has a competitive ratio <sup>1</sup> <sup>k</sup>+2 . Before proceeding to the proof of Theorem [1,](#page-4-2) we need to introduce some definitions and two technical lemmas, which we prove later.

Definition 3. Let l ∈ {1, . . . , 2k + 1}. We say that w<sup>l</sup> is blocked if there exist r, r′ ∈ {l + 1, . . . , 2k + 1} such that w<sup>r</sup> ′ = y<sup>j</sup> and w<sup>r</sup> = z<sup>j</sup> for some j. We denote by m<sup>l</sup> the smallest r that satisfies this property. For example, if k = 3 and the first 2k + 1 = 7 elements of the w sequence are given by

$$
y_3 \ y_5 \ y_1 \ z_5 \ y_8 \ z_8 \ z_1 \ \ldots,
$$

w<sup>2</sup> is blocked, since the pairs (y1, z1) and (y8, z8) appear between the 3-rd and 7-th positions. Moreover, in this case m<sup>2</sup> = 6.

<span id="page-5-1"></span><sup>3</sup> If some values are identical, Nature randomizes their order within the sequence.

The pair (y<sup>j</sup> , z<sup>j</sup> ) "blocks" w<sup>l</sup> , in the sense that no matter whether z<sup>j</sup> = w<sup>r</sup> is in X or S, no threshold below w<sup>r</sup> can guarantee selecting the value w<sup>l</sup> .

Definition 4. Let l ∈ {1, . . . , 2k + 1} and p such that w<sup>p</sup> is paired with wl. We say that w<sup>l</sup> is ill-paired if p ∈ {l + 1, . . . , 2k + 1}.

That is, we say that a value w<sup>l</sup> is ill-paired if it is paired with a value greater than or equal to w2k+1. For instance, considering the same sequence as before, w<sup>2</sup> is ill-paired since z<sup>5</sup> appears before w7.

Definition 5. For each l ∈ {1, . . . , 2k + 1}, we define the parameter δ<sup>l</sup> as follows:

δ<sup>l</sup> = 2 −2k+l−1 if w<sup>l</sup> is not blocked and not ill-paired 2 −2k+l if w<sup>l</sup> is not blocked and ill-paired 0 otherwise.

<span id="page-6-0"></span>Proposition 1. If the gambler plays according to MSAk+1, his expected reward is at least

$$
\mathbb{E}(MSA_{k+1}) \ge \frac{1}{2} \sum_{l=k+1}^{2k+1} \mathbb{P}(X_{(k+1)} = w_l)w_l \delta_l + \frac{1}{2} \sum_{l=2(k+1)}^{\xi_{k+1}} \mathbb{P}(X_{(k+1)} = w_l)w_l
$$

Note that E(X(k+1)) is equal to P<sup>ξ</sup>k+1 <sup>l</sup>=k+1 <sup>w</sup>lP(X(k+1) <sup>=</sup> <sup>w</sup>l). Consequently, when <sup>k</sup> = 0, Proposition [1](#page-6-0) recovers the result from [Rubinstein et al.](#page-30-6) [\(2020\)](#page-30-6) which states that MSA<sup>1</sup> gives a 1/2 competitive ratio (δ<sup>1</sup> = 1/2 since w<sup>1</sup> is neither blocked nor ill-paired). The challenge when k ≥ 1 arises from the fact that, for k + 1 ≤ l ≤ 2k + 1, the coefficient accompanying the term P(X(k+1) = wl)w<sup>l</sup> may be smaller than 1/(k + 2). In other words, for k + 1 ≤ l ≤ 2k + 1 the coefficient may be "too small", while for 2k + 2 ≤ l ≤ ξk+1, it is "larger than necessary" (equal to 1/2). This imbalance motivates the introduction of a randomization over the MSAk+1 and MSA<sup>i</sup> algorithms: By blending MSAk+1 with MSA<sup>i</sup> for i ∈ {1, . . . , k}, we redistribute these coefficients more evenly. To analyze such a randomization, we need first a lower bound on the performance of MSA<sup>i</sup> , i ∈ {1, . . . , k}.

<span id="page-6-1"></span>Proposition 2. The sum of the expected reward of the gambler playing according to MSA<sup>i</sup> for i ≤ k is at least

$$
\sum_{i=1}^{k} \mathbb{E}(\overline{MSA}_{i}) \geq \sum_{l=1}^{2k+1} \mathbb{P}(X_{(k+1)} = w_{l})w_{l}(1 - \delta_{l}).
$$

The coefficients accompanying the P(X(k+1) = wl)w<sup>l</sup> in the above inequality are higher than those in the expression of Proposition [1](#page-6-0) for k + 1 ≤ l ≤ 2k, while they are equal to 0 for l > 2k + 1. This supports the idea that combining algorithms enables a redistribution of coefficients. The surprising fact is that there exists a way to combine the MSA<sup>i</sup> , i ∈ {1, . . . , k + 1} in a way that all the coefficients are simultaneously higher than 1/(k + 2), yielding the competitive factor of 1/(k + 2). We prove this below.

Proof of Theorem [1.](#page-4-2) Let us consider the strategy for the gambler MSARAND consisting on playing according to MSA<sup>i</sup> with probability 1/(k + 2), for i ∈ {1, . . . , k}, and to MSAk+1 with probability 2/(k + 2).

Then, (k + 2)E(MSARAND) = P<sup>k</sup> <sup>i</sup>=1 <sup>E</sup>(MSAi) + 2E(MSAk+1), and by using Proposition [1](#page-6-0) and Proposition [2,](#page-6-1) we obtain

$$
(k+2)\mathbb{E}(\overline{MSA}_{RAND}) \ge \sum_{l=1}^{2k+1} \mathbb{P}(X_{(k+1)} = w_l)w_l(1 - \delta_l)
$$
  
+ 
$$
2\sum_{l=k+1}^{2k+1} \mathbb{P}(X_{(k+1)} = w_l)w_l\frac{\delta_l}{2} + 2\sum_{l=2(k+1)}^{\xi_{k+1}} \mathbb{P}(X_{(k+1)} = w_l)w_l\frac{1}{2}
$$

$$
= \sum_{l=k+1}^{\xi_{k+1}} \mathbb{P}(X_{(k+1)} = w_l)w_l = \mathbb{E}(X_{(k+1)}),
$$

where the equality holds because P(X(k+1) = wl) = 0 for l < k + 1. This concludes on the proof of Theorem [1.](#page-4-2)

#### 3.1.1 Proof of Proposition [1](#page-6-0)

The proof of Proposition [1](#page-6-0) is divided into two intermediary results, which we state now.

<span id="page-7-0"></span>Lemma 1.

$$
\mathbb{E}(MSA_{k+1}1_{\tau_{k+1}=w_{2k+2}}) \geq \sum_{l=k+1}^{2k+1} \mathbb{P}(X_{(k+1)}=w_l)w_l\frac{\delta_l}{2}.
$$

<span id="page-7-1"></span>Lemma 2. Assume that 2k + 2 ̸= ξk+1. Then

$$
\mathbb{E}(MSA_{k+1}1_{\tau_{k+1}\leq w_{2k+3}})\geq \frac{1}{2}\sum_{l=2(k+1)}^{\xi_{k+1}}\mathbb{P}(X_{(k+1)}=w_l)w_l.
$$

Proof of Proposition [1](#page-6-0) admitting Lemmas [1](#page-7-0) and [2.](#page-7-1) In the case where 2k + 2 ̸= ξk+1, summing the two inequalities proves Proposition [1.](#page-6-0) Assume that 2k + 2 = ξk+1. This means that the elements of {w1, . . . , w2k+2} form k + 1 pairs. Hence, if w2k+2 ∈ S, which happens with probability 1/2, there are exactly k + 1 elements larger than w2k+2 that are in X. In that case, MSAk+1 picks X(k+1). It follows that

$$
\mathbb{E}(MSA_{k+1}) \ge \frac{1}{2}\mathbb{E}(X_{(k+1)}).
$$

In particular, Proposition [1](#page-6-0) holds.

#### Proof of Lemma [1](#page-7-0)

Lemma [1](#page-7-0) is a consequence of the following lemma.

<span id="page-7-3"></span><span id="page-7-2"></span>Lemma 3. Let l ∈ {1, . . . , 2k + 1} such that w<sup>l</sup> is not blocked. a) If w<sup>l</sup> is not ill-paired, it holds that

$$
\mathbb{P}(\{MSA_{k+1} = w_l\} \cap \{\tau_{k+1} = w_{2k+2}\}) | X_{(k+1)} = w_l) \ge 2^{-2k-2+l}.
$$

b) If w<sup>l</sup> is ill-paired, then

$$
\mathbb{P}(\{MSA_{k+1} = w_l\} \cap \{\tau_{k+1} = w_{2k+2}\} | X_{(k+1)} = w_l) \ge 2^{-2k-1+l}.
$$

Proof of Lemma [3.](#page-7-2) a) We claim that when X(k+1) = w<sup>l</sup> and τk+1 = w2k+2, then MSAk+1 picks w<sup>l</sup> . Indeeed, when X(k+1) = w<sup>l</sup> , there are exactly l − 1 − k elements in {w1, . . . , wl−1} that are in S. If, in addition, τk+1 = w2k+2, then there should be exactly k − (l − 1 − k) = 2k + 1 − l elements of {wl+1, . . . , w2k+1} that are in S, meaning that they should all be in S. Under these circumstances, wl is the only element in X that is above τ2k+2 and that is not among the k best values in X, and is thus selected by MSAk+1. We deduce that

$$
\mathbb{P}(\{MSA_{k+1} = w_l\} \cap \{\tau_{k+1} = w_{2k+2}\} | X_{(k+1)} = w_l)
$$
  
= 
$$
\mathbb{P}(\tau_{k+1} = w_{2k+2} | X_{(k+1)} = w_l).
$$

Therefore, it is enough to prove P(τk+1 = w2k+2|X(k+1) = wl) ≥ 2 −2k−2+l . Given X(k+1) = wl , in order for τk+1 = w2k+2 to hold, it is necessary and sufficient that all the elements in {wl+1, . . . , w2k+2} belong to S. We claim that this event occurs with probability greater than 2 −2k+l−2 . To show that, we use the chain rule for conditional probability:

$$
\mathbb{P}(\{w_{l+1},\ldots,w_{2k+2}\}\subset S|X_{k+1}=w_l)=\mathbb{P}\left(\bigcap_{j=l+1}^{2k+2}\{w_j\in S\}\middle|X_{(k+1)}=w_l\right)
$$
$$
=\prod_{l'=l+1}^{2k+2}\mathbb{P}\left(\{w_{l'}\in S\}\middle| \bigcap_{j=l+1}^{l'-1}\{w_j\in S\},X_{(k+1)}=w_l\right).
$$

In order to establish the desired result, it is sufficient to verify that each factor in the expression above is lower bounded by 1/2. The proof is therefore divided into two steps:

Step 1: P(wl+1 ∈ S|X(k+1) = wl) ≥ 1/2.

If wl+1 is paired with an element smaller than wl+1, then the events {wl+1 ∈ S} and X(k+1) = w<sup>l</sup> are independent, and therefore

$$
\mathbb{P}(w_{l+1} \in S | X_{(k+1)} = w_l) = 1/2.
$$

Consider now the case where wl+1 is paired with some w<sup>a</sup> ≥ wl+1. Since w<sup>l</sup> is not ill-paired, we have a ̸= l, and the probability that wl+1 lies in S is equal to the probability that w<sup>a</sup> is one of the not-paired elements of {w1, . . . , wl} in X. Note that the event {X(k+1) = wl} occurs if and only if w<sup>l</sup> ∈ X and there are exactly k elements in X that are larger than w<sup>l</sup> . Therefore, if l ∈ {ξ<sup>j</sup> , . . . , ξj+1 − 1}, then among the l − 2j not-paired values in {w1, . . . , wl}, k + 1 − j belong to X, while l − k − 1 − j are in S. It follows that the probability of w<sup>a</sup> being among those elements in X is higher than 1/2, since k + 1 − j > l − k − 1 − j due to l ≤ 2k + 1. We thus conclude that

$$
\mathbb{P}(w_{l+1} \in S | X_{k+1} = w_l) > \frac{1}{2}.
$$

Step 2: For each l ′ ∈ {l + 2, . . . , 2k + 2},

$$
\mathbb{P}\left(w_{l'}\in S|\bigcap_{j=l+1}^{l'-1}\{w_j\in S\}, X_{(k+1)}=w_l\right)\geq \frac{1}{2}.
$$

Let w<sup>a</sup> such that w<sup>l</sup> ′ is paired with wa, and assume that l ′ ∈ {ξ<sup>j</sup> + 1, . . . , ξj+1}. That is, there are j pairs that arrived before w<sup>l</sup> ′ . Following the same argument than in Step 1, if w<sup>a</sup> < w<sup>l</sup> ′ , we have

$$
\mathbb{P}\left(w_{l'}\in S|\bigcap_{j=l+1}^{l'-1}\{w_j\in S\}, X_{(k+1)}=w_l\right)=\frac{1}{2}.
$$

On the other hand, w<sup>a</sup> cannot belong to {wl+1, . . . , w<sup>l</sup> ′−1} because w<sup>l</sup> is not blocked.

Finally, let us assume w<sup>a</sup> ≤ w<sup>l</sup> . In this case, among the l ′ − 1 − 2j not-paired values in {w1, . . . , w<sup>l</sup> ′−1}, k + 1 − j belong to X, while l ′ − 2 − j − k are in S. Then, it follows that the probability of w<sup>a</sup> being among those elements in X is higher than 1/2, since k + 1 − j > l′ − 2 − j − k due to l ′ ≤ 2k + 1. We thus conclude that

$$
\mathbb{P}\left(w_{l'} \in S | \bigcap_{j=l+1}^{l'-1} \{w_j \in S\}, X_{(k+1)} = w_l\right) > \frac{1}{2}.
$$

Combining Step 1 and Step 2 yields the result.

b) As in the proof of Case [a\),](#page-7-3) we have

$$
\mathbb{P}(\{MSA_{k+1} = w_l\} \cap {\tau_{k+1} = w_{2k+2}} | X_{(k+1)} = w_l) = \mathbb{P}(\tau_{k+1} = w_{2k+2} | X_{(k+1)} = w_l),
$$

and to obtain the result it is enough to show that

<span id="page-9-0"></span>
$$
\mathbb{P}(\tau_{k+1} = w_{2k+2} | X_{(k+1)} = w_l) \ge 2^{-2k-1+l}.\tag{1}
$$

Given that X(k+1) = w<sup>l</sup> , we know that w<sup>p</sup> is in S, since (wp, wl) is a pair. Then, in order to get τk+1 = w2k+2, it is necessary and sufficient that all the elements in {wl+1, . . . , w2k+2} \ {p} belong to S. This happens with probability at least 2−2k−1+<sup>l</sup> , by the same argument as in the proof of Case [a\).](#page-7-3) This proves [\(1\)](#page-9-0), and the result follows.

Proof of Lemma [1.](#page-7-0) We want to prove

$$
\mathbb{E}(MSA_{k+1}1_{\tau_{k+1}=w_{2k+2}}) \geq \sum_{l=k+1}^{2k+1} \mathbb{P}(X_{(k+1)}=w_l)w_l\frac{\delta_l}{2}.
$$

To this end, note that

$$
\mathbb{E}(MSA_{k+1}1_{\tau_{k+1}=w_{2k+2}}) = \sum_{l=k+1}^{2k+1} w_l \mathbb{P}(\{MSA_{k+1}=w_l\} \cap \{\tau_{k+1}=w_{2k+2}\} | X_{(k+1)}=w_l) \mathbb{P}(X_{(k+1)}=w_l).
$$

By Lemma [3](#page-7-2) and by the definition of δ<sup>l</sup> , we have that for each l ∈ {k + 1, . . . , 2k + 1},

> P({MSAk+1 = wl} ∩ {τk+1 = w2k+2} |X(k+1) = wl) ≥ δl 2 ,

and the result follows.

#### Proof of Lemma [2](#page-7-1)

First, we decompose the left-hand-side term in Lemma [1](#page-7-0) as follows:

$$
\mathbb{E}(MSA_{k+1}1_{\tau_{k+1}\leq w_{2k+3}})=\sum_{l=2k+3}^{\xi_{k+1}}\mathbb{E}(MSA_{k+1}|\tau_{k+1}=w_{l})\mathbb{P}(\tau_{k+1}=w_{l})
$$

Since each w<sup>l</sup> , for l ≥ 1, is equally likely to be in S or in X, the law of τk+1 is identical to the law of X(k+1). We deduce that for all l ≥ 1, P(τk+1 = wl) = P(X(k+1) = wl). Secondly, when τk+1 = w<sup>l</sup> , there are l − k − 1 ≥ k + 1 elements above w<sup>l</sup> that are in X. Hence, MSAk+1 will pick one of them, and we deduce that E(MSAk+1|τk+1 = wl) ≥ wl−1. These two observations give

<span id="page-9-1"></span>
$$
\mathbb{E}(MSA_{k+1}1_{\tau_{k+1}\leq w_{2k+3}}) \geq \sum_{l=2k+3}^{\xi_{k+1}} w_{l-1} \mathbb{P}(X_{k+1} = w_{l})
$$
$$
= \sum_{l=2(k+1)}^{\xi_{k+1}-1} w_{l} \mathbb{P}(X_{(k+1)} = w_{l+1})
$$
(2)

One of the main differences between the above inequality and the one we want to prove in Lemma [2](#page-7-1) is that the term inside the sum is P(X(k+1) = wl+1) instead of P(X(k+1) = wl). In the sequel, we relate these two quantities. First, we compute P(X(k+1) = wl).

<span id="page-10-0"></span>Lemma 4. The probability distribution of X(k+1) is given by

$$
\mathbb{P}(X_{(k+1)} = w_l) = \begin{cases} \frac{\binom{l-1-2j}{k-j}}{2^{l-2j}} & \text{if } l \in \{\xi_j+1,\dots,\xi_{j+1}-1\}, \text{ for } j \in \{0,\dots,k\} \\ \frac{\binom{\xi_j-2j}{k+1-j}}{2^{\xi_j-2j+1}} & \text{if } l = \xi_j, \text{ for } j \in \{1,\dots,k+1\}. \end{cases}
$$

Proof of Lemma [4.](#page-10-0) We divide the proof into two cases, depending on whether l = ξ<sup>j</sup> for some j ∈ {0, . . . , k + 1} or not.

Case 1: Suppose that l ∈ {ξ<sup>j</sup> + 1, . . . , ξj+1 − 1} for some j ∈ {0, . . . , k}. Note that X(k+1) = w<sup>l</sup> if and only if w<sup>l</sup> ∈ X and there are exactly k values in X that are larger than w<sup>l</sup> .

Since l ∈ {ξ<sup>j</sup> + 1, . . . , ξj+1 − 1}, we have, conditioned on w<sup>l</sup> ∈ X, that there are j + 1 values in X and j in S with probability 1. Therefore, the probability that exactly k values in X are among the l − 1 largest values is given by

$$
\binom{l-2j-1}{k-j}\frac{1}{2^{l-2j-1}}.
$$

On the other hand, P(w<sup>l</sup> ∈ X) = 1/2, and thus we conclude that in this case,

<span id="page-10-1"></span>
$$
\mathbb{P}(X_{(k+1)} = w_l) = \binom{l-2j-1}{k-j} \frac{1}{2^{l-2j}}.
$$
\n(3)

Case 2: Suppose that l = ξ<sup>j</sup> for some j ∈ {1, . . . , k + 1}. The analysis in this case is similar to that of Case 1. However, the probability of having exactly k values in X greater than w<sup>l</sup> is now given by

$$
\binom{l-2j}{k-(j-1)}\frac{1}{2^{l-2j}}.
$$

In effect, conditioned on w<sup>l</sup> ∈ X, there are j −1 values in X greater than w<sup>l</sup> and j values greater than wl in S, with probability one. Thus, we need to compute the probability that exactly k−(j−1) additional values in X come from the l − 2j remaining elements. This probability is given by the expression above.

Therefore, in this case,

<span id="page-10-2"></span>
$$
\mathbb{P}(X_{(k+1)} = w_l) = \binom{l-2j}{k-(j-1)} \frac{1}{2^{l-2j+1}} \tag{4}
$$

Combining [\(3\)](#page-10-1) and [\(4\)](#page-10-2), we obtain the desired result.

We now use the previous lemma to lower bound P(X(k+1) = wl+1) in terms of P(X(k+1) = wl). As suggested by the expression in Lemma [4,](#page-10-0) we will need to distinguish between the cases where l and l + 1 are some ξ<sup>j</sup> or not.

<span id="page-10-3"></span>Lemma 5. a) Let j ∈ {0, . . . , k} and l ∈ {ξ<sup>j</sup> + 1, . . . , ξj+1 − 1}.

$$
\mathbb{P}(X_{(k+1)} = w_{l+1}) \ge \begin{cases} \frac{1}{2} \mathbb{P}(X_{(k+1)} = w_l) & \text{if } l+1 \in \{\xi_j+1, \dots, \xi_{j+1}-1\} \\ \mathbb{P}(X_{(k+1)} = w_l) & \text{if } l+1 = \xi_{j+1}. \end{cases}
$$

<span id="page-10-4"></span>b) Assume that 2k + 2 = ξ<sup>j</sup> , for some j ∈ {1, . . . , k}. Then

$$
\mathbb{P}(X_{(k+1)} = w_{2k+3}) \ge \begin{cases} \frac{1}{2} \mathbb{P}(X_{(k+1)} = w_{2k+2}) & \text{if } 2k+3 \neq \xi_{j+1} \\ \mathbb{P}(X_{(k+1)} = w_{2k+2}) & \text{if } 2k+3 = \xi_{j+1}. \end{cases}
$$

Proof of Lemma [5.](#page-10-3) a) Assume l + 1 ∈ {ξ<sup>j</sup> + 1, . . . , ξj+1 − 1}. We have

$$
\mathbb{P}(X_{(k+1)} = w_{l+1}) = \frac{\binom{l-2j}{k-j}}{2^{l+1-2j}}
$$
$$
\geq \frac{1}{2} \cdot \frac{\binom{l-1-2j}{k-j}}{2^{l-2j}}
$$
$$
= \frac{1}{2} \cdot \mathbb{P}(X_{(k+1)} = w_l)
$$

Assume that l + 1 = ξj+1. We have

$$
\mathbb{P}(X_{(k+1)} = w_{l+1}) = \frac{\binom{\xi_{j+1} - 2j - 2}{k - j}}{2^{\xi_{j+1} - 2j - 1}}
$$
$$
= \mathbb{P}(X_{(k+1)} = w_l)
$$

b) Assume that 2k + 3 ̸= ξj+1. We have

$$
\mathbb{P}(X_{(k+1)} = w_{2k+3}) = \frac{\binom{2k+2-2j}{k-j}}{2^{2k+3-2j}}
$$
$$
= \left(\frac{k+1-j}{k+2-j}\right) \frac{\binom{2k+2-2j}{k+1-j}}{2^{2k+3-2j}}
$$
$$
\geq \frac{1}{2} \mathbb{P}(X_{(k+1)} = w_{2k+2})
$$

Assume that 2k + 3 = ξj+1. We have

$$
\mathbb{P}(X_{(k+1)} = w_{2k+3}) = \frac{\binom{2k+1-2j}{k-j}}{2^{2k+2-2j}}
$$
$$
= \frac{\frac{1}{2}\binom{2k+2-2j}{k+1-j}}{2^{2k+2-2j}}
$$
$$
= \mathbb{P}(X_{(k+1)} = w_{2k+2})
$$

We are now ready to prove Lemma [2.](#page-7-1)

Proof of Lemma [2.](#page-7-1) By inequality [\(2\)](#page-9-1), it is enough to prove that

$$
\sum_{l=2(k+1)}^{\xi_{k+1}-1} w_l \mathbb{P}(X_{(k+1)} = w_{l+1}) \geq \frac{1}{2} \sum_{l=2k+2}^{\xi_{k+1}} w_l \mathbb{P}(X_{(k+1)} = w_l).
$$

Case 1. 2k + 3 = ξ<sup>j</sup> , for some j ∈ {1, . . . , k}.

By Lemma [5](#page-10-3) [b\),](#page-10-4) we have

<span id="page-11-0"></span>
$$
\mathbb{P}(X_{(k+1)} = w_{2k+3}) \ge \frac{1}{2} \mathbb{P}(X_{(k+1)} = w_{2k+2}) + \frac{1}{2} \mathbb{P}(X_{(k+1)} = w_{2k+3})
$$
\n(5)

We deduce that

$$
\sum_{l=2(k+1)}^{\xi_{k+1}-1} w_l \mathbb{P}(X_{(k+1)} = w_{l+1}) = w_{2k+2} \mathbb{P}(X_{(k+1)} = w_{2k+3}) + \sum_{l=2k+3}^{\xi_{k+1}-1} w_l \mathbb{P}(X_{(k+1)} = w_{l+1})
$$
  
$$
\geq \frac{1}{2} w_{2k+2} \mathbb{P}(X_{(k+1)} = w_{2k+2}) + \frac{1}{2} w_{2k+3} \mathbb{P}(X_{(k+1)} = w_{2k+3})
$$

$$
+\sum_{l=2k+3}^{\xi_{k+1}-1} w_{l+1} \mathbb{P}(X_{(k+1)} = w_{l+1})
$$
  
$$
\geq \frac{1}{2} \sum_{l=2k+2}^{\xi_{k+1}} w_{l} \mathbb{P}(X_{(k+1)} = w_{l}),
$$

where in the second-to-last inequality, we used [\(5\)](#page-11-0) and the fact that w2k+2 ≥ w2k+3 and w<sup>l</sup> ≥ wl+1.

Case 2. 2k + 3 ∈ {ξ<sup>j</sup> + 1, . . . , ξj+1 − 1} for some j ∈ {0, . . . , k}. The sum P<sup>ξ</sup>k+1−<sup>1</sup> <sup>l</sup>=2k+2 <sup>w</sup>lP(X(k+1) <sup>=</sup> <sup>w</sup>l+1) can be decomposed as

$$
\sum_{l=2k+2}^{\xi_{j+1}-2} w_l \mathbb{P}(X_{(k+1)} = w_{l+1}) + w_{\xi_{j+1}-1} \mathbb{P}(X_{(k+1)} = w_{\xi_{j+1}}) + \sum_{l=\xi_{j+1}}^{\xi_{k+1}-1} w_l \mathbb{P}(X_{(k+1)} = w_{l+1}).
$$

By using Lemma [5](#page-10-3) and the fact that w<sup>l</sup> ≥ wl+1, we can lower bound the expression above by

$$
\frac{1}{2}\sum_{l=2k+2}^{\xi_{j+1}-2}w_{l}\mathbb{P}(X_{(k+1)}=w_{l})+w_{\xi_{j+1}-1}\mathbb{P}(X_{(k+1)}=w_{\xi_{j+1}-1})+\sum_{l=\xi_{j+1}}^{\xi_{k+1}-1}w_{l+1}\mathbb{P}(X_{(k+1)}=w_{l+1}),
$$

which is at least

$$
\frac{1}{2} \sum_{l=2k+2}^{\xi_{k+1}} w_l \mathbb{P}(X_{(k+1)} = w_l),
$$

as we wanted to see.

#### 3.1.2 Proof of Proposition [2](#page-6-1)

Before proving Proposition [2,](#page-6-1) we introduce one technical lemma that gives a lower bound for the probability of MSA<sup>i</sup> picking a value w<sup>l</sup> conditional on w<sup>l</sup> being the (k + 1)-largest value in the set X.

<span id="page-12-0"></span>Lemma 6. Let l ∈ {1, . . . , 2k + 1}. a) If w<sup>l</sup> is not blocked and not ill-paired, for all i ∈ {l − k, . . . , k} it holds

$$
\mathbb{P}(\overline{MSA}_i = w_l | X_{(k+1)} = w_l) \ge 2^{-k-i+l-1}.
$$

b) If w<sup>l</sup> is blocked, and that either w<sup>l</sup> is not ill-paired, or it is ill-paired and m<sup>l</sup> < p, we have

$$
\mathbb{P}(\overline{MSA}_i = w_l | X_{(k+1)} = w_l) \ge \begin{cases} 2^{-k-i+l-1} & \text{if } i \in \{l-k, \dots, m_l-k-3\}, \\ 2^{-k-i+l} & \text{if } i = m_l-k-2. \end{cases}
$$

c) Assume w<sup>l</sup> is not blocked and ill-paired. Then,

$$
\mathbb{P}(\overline{MSA_i} = w_l | X_{(k+1)} = w_l) \ge \begin{cases} 2^{-k-i+l-1} & \text{if } i \in \{l-k, \dots, p-k-2\}, \\ 2^{-k-i+l} & \text{if } i \in \{p-k, \dots, k\}. \end{cases}
$$

d) Assume w<sup>l</sup> is blocked and ill-paired, and that m<sup>l</sup> > p. Then,

$$
\mathbb{P}(\overline{MSA}_{i} = w_{l} | X_{(k+1)} = w_{l}) \geq \begin{cases} 2^{-k-i+l-1} & \text{if } i \in \{l-k, \ldots, p-k-2\}, \\ 2^{-k-i+l} & \text{if } i \in \{p-k, \ldots, m_{l}-k-3\}, \\ 2^{-k-i+l+1} & \text{if } i = m_{l}-k-2. \end{cases}
$$

Proof of Lemma [6.](#page-12-0) Let l ∈ {1, . . . , 2k + 1}.

a) The proof is very similar to the one of Lemma [3](#page-7-2) [a\),](#page-7-3) up to replacing k+1 by i. For sake of completeness, we draw the main lines. Take i ∈ {l − k, . . . , k}. We want to analyze P(MSA<sup>i</sup> = w<sup>l</sup> |X(k+1) = wl). First, note that since l is not ill-paired, w<sup>l</sup> is not paired with wk+i+1. Then,

$$
\mathbb{P}(\overline{MSA}_{i} = w_{l} | X_{(k+1)} = w_{l}) \geq \mathbb{P}(\{\overline{MSA}_{i} = w_{l}\} \cap \{\tau_{i} = w_{k+i+1}\} | X_{(k+1)} = w_{l})
$$
  
=  $\mathbb{P}(\tau_{i} = w_{k+i+1} | X_{(k+1)} = w_{l}),$ 

where the equality stems from the fact that, when X(k+1) = w<sup>l</sup> and τ<sup>i</sup> = wk+i+1, all the elements in {wl+1, . . . , wk+i} must be in S, and then MSA<sup>i</sup> picks w<sup>l</sup> , because it is not paired with the threshold τi .

Therefore, it is enough to prove that P(τ<sup>i</sup> = wk+i+1|X(k+1) = wl) ≥ 2 −k−i+l−1 . Given X(k+1) = w<sup>l</sup> , in order for τ<sup>i</sup> = wk+i+1 to hold, it is necessary and sufficient that all the elements in {wl+1, . . . , wk+i+1} belong to S. This event occurs with probability greater than 2−k−i+l−<sup>1</sup> , by a similar computation as in the proof of Lemma [3](#page-7-2) [a\).](#page-7-3)

b) Take i ∈ {l −k, . . . , m<sup>l</sup> −k −3}. In this case, k +i+ 1 < m<sup>l</sup> , hence the pair that blocks w<sup>l</sup> is smaller than wk+i+1. Moreover, since either w<sup>l</sup> is not ill-paired or m<sup>l</sup> < p, w<sup>l</sup> is not paired with wk+i+1. We can therefore replicate the same computations as in a), and thus obtain the claimed inequality.

If i = m<sup>l</sup> − k − 2, we can replicate the same computations as in a) too, which yields:

$$
\mathbb{P}(\overline{MSA}_i = w_l \cap \{ \tau_i = w_{k+i+1} \} | X_{(k+1)} = w_l) \ge 2^{-k-i+l-1}.
$$

To obtain the desired lower bound, we will consider in addition the case where τ<sup>i</sup> = wm<sup>l</sup> . Indeed, whenever X(k+1) = w<sup>l</sup> and τ<sup>i</sup> = wm<sup>l</sup> , the only element in X that is below w<sup>l</sup> and above the threshold τi is wm<sup>l</sup> 's pair, namely wm′ . By definition of MSA<sup>i</sup> , such an element is not selected, and therefore MSA<sup>i</sup> selects w<sup>l</sup> . We deduce that

$$
\mathbb{P}(\{\overline{MSA_i} = w_l\} \cap {\tau_i = w_{m_l}\} | X_{(k+1)} = w_l) = \mathbb{P}(\tau_i = w_{m_l} | X_{(k+1)} = w_l).
$$

Knowing X(k+1) = w<sup>l</sup> , in order to get τ<sup>i</sup> = wm<sup>l</sup> = wk+i+2, it is necessary and sufficient that all the elements in {wl+1, . . . , wk+i+2} \ {m′} belong to S, which happens with probability higher than 2 −k−i+l−1 , by a similar computation as in the proof of Lemma [3](#page-7-2) [a\).](#page-7-3) Then, we have

$$
\mathbb{P}(\overline{MSA}_i = w_l \cap \{ \tau_i = w_{k+i+2} \} | X_{(k+1)} = w_l) \geq 2^{-k-i+l-1}.
$$

We conclude that

$$
\mathbb{P}(\overline{MSA}_{i} = w_{l} | X_{k+1} = w_{l}) \geq \mathbb{P}(\overline{MSA}_{i} = w_{l} \cap \{\tau_{i} = w_{k+i+2}\} | X_{(k+1)} = w_{l})
$$
  
+ 
$$
\mathbb{P}(\overline{MSA}_{i} = w_{l} \cap \{\tau_{i} = w_{k+i+1}\} | X_{(k+1)} = w_{l})
$$
  
$$
\geq 2^{-k-i+l},
$$

which is the desired result.

c) If i ∈ {l − k, . . . , p − k − 2}, the argument proceeds as in part a).

Take i ∈ {p − k, . . . , k}. In this case, k + i + 1 > p, and then w<sup>l</sup> is not paired with wk+i+1. We therefore have

$$
\mathbb{P}(\overline{MSA}_{i} = w_{l} | X_{(k+1)} = w_{l}) \geq \mathbb{P}(\overline{MSA}_{i} = w_{l} \cap \{\tau_{i} = w_{k+i+1}\} | X_{(k+1)} = w_{l})
$$
  
=  $\mathbb{P}(\tau_{i} = w_{k+i+1} | X_{(k+1)} = w_{l}),$ 

and to obtain the result it is enough to show that

$$
\mathbb{P}(\tau_i = w_{k+i+1} | X_{(k+1)} = w_l) \ge 2^{-k-i+l}.
$$

Given that X(k+1) = w<sup>l</sup> , we know that w<sup>p</sup> is in S, since (wp, wl) is a pair. Then, in order to get τ<sup>i</sup> = wk+i+1, it is necessary and sufficient that all the elements in {wl+1, . . . , wk+i+1} \ {p} belong to S, which happens with probability at least 2−k−i+<sup>l</sup> , by a similar computation as in the proof of Lemma [3](#page-7-2) [a\).](#page-7-3) We deduce that

$$
\mathbb{P}(\overline{MSA}_i = w_l | X_{(k+1)} = w_l) \ge 2^{-k-i+l},
$$

which is what we wanted to show.

d) The first two cases can be proved as in a) and c). Let i = m<sup>l</sup> − k − 2; that is k + i + 2 = m<sup>l</sup> . We call wm′ the pair of wm<sup>l</sup> .

First, we can replicate the computations of Case c), and obtain:

$$
\mathbb{P}(\{\overline{MSA}_i = w_l\} \cap \{\tau_i = w_{k+i+1}\} | X_{(k+1)} = w_l) \ge 2^{-k-i+l}.
$$

As in Case b), in order to obtain the claimed bound of the lemma, we need to consider the event {τ<sup>i</sup> = wk+i+2}. We have

$$
\mathbb{P}(\overline{MSA}_{i} = w_{l}|X_{(k+1)} = w_{l}) \geq \mathbb{P}(\{\overline{MSA}_{i} = w_{l}\} \cap \{\tau_{i} = w_{k+i+2}\} | X_{(k+1)} = w_{l})
$$
  
=  $\mathbb{P}(\tau_{i} = w_{k+i+2} | X_{(k+1)} = w_{l}),$ 

where the equality stems from the fact that, when X(k+1) = w<sup>l</sup> and τ<sup>i</sup> = wk+i+2, all the elements in {wl+1, . . . , wk+i+1} \ {wm′} must be in S and then MSA<sup>i</sup> picks w<sup>l</sup> , because w<sup>l</sup> is not paired with wk+i+2. Since w<sup>l</sup> and w<sup>p</sup> are paired, given that X(k+1) = w<sup>l</sup> , we have that w<sup>p</sup> lies in S. Moreover, since wk+i+2 and wm′ are paired, if wk+i+2 lies in S, then wm′ lies in X. Hence, in order to get τ<sup>i</sup> = wk+i+2, it is necessary and sufficient that all the elements in {wl+1, . . . , wk+i+2} \ {wm′ , wp} belong to S, which happens with probability at least 2<sup>−</sup>k−i+<sup>l</sup> , by a similar computation as in the proof of Lemma [3](#page-7-2) [a\).](#page-7-3) We deduce that

$$
\mathbb{P}(\{\overline{MSA}_i = w_l\} \cap \{\tau_i = w_{k+i+2}\} | X_{(k+1)} = w_l) \geq 2^{-k-i+l}.
$$

We conclude that

$$
\mathbb{P}(\overline{MSA}_{i} = w_{l}|X_{(k+1)} = w_{l}) \geq \mathbb{P}(\{\overline{MSA}_{i} = w_{l}\} \cap \{\tau_{i} = w_{k+i+2}\}|X_{(k+1)} = w_{l}) + \mathbb{P}(\{\overline{MSA}_{i} = w_{l}\} \cap \{\tau_{i} = w_{k+i+1}\}|X_{(k+1)} = w_{l})
$$
  
\n
$$
\geq 2^{-k-i+l+1}.
$$

Proof of Proposition [2.](#page-6-1) Note that

$$
\sum_{i=1}^{k} \mathbb{E}(\overline{MSA}_{i}) \geq \sum_{i=1}^{k} \sum_{l=1}^{2k+1} \mathbb{P}(\overline{MSA}_{i} = w_{l} | X_{(k+1)} = w_{l}) w_{l} \mathbb{P}(X_{(k+1)} = w_{l})
$$
$$
= \sum_{l=1}^{2k+1} w_{l} \mathbb{P}(X_{(k+1)} = w_{l}) \sum_{i=1}^{k} \mathbb{P}(\overline{MSA}_{i} = w_{l} | X_{(k+1)} = w_{l}).
$$

In the remainder of the proof we show that for each l ∈ {1, . . . , 2k + 1},

$$
\sum_{i=1}^{k} \mathbb{P}(\overline{MSA}_{i} = w_{l} | X_{(k+1)} = w_{l}) = 1 - \delta_{l},
$$

where we recall that

$$
\delta_l = \begin{cases} 2^{-2k+l-1} & \text{if } w_l \text{ is not blocked and not ill-paired} \\ 2^{-2k+l} & \text{if } w_l \text{ is not blocked and ill-paired} \\ 0 & \text{otherwise.} \end{cases}
$$

Case 1. w<sup>l</sup> is not blocked and not ill-paired.

In this case, by Lemma [6](#page-12-0)

$$
\sum_{i=1}^{k} \mathbb{P}(\overline{MSA_i} = w_l | X_{(k+1)} = w_l) = \sum_{i=l-k}^{k} 2^{-k-i+l-1} = 1 - 2^{-2k+l-1}.
$$

Case 2. w<sup>l</sup> is not blocked and ill-paired.

$$
\sum_{i=1}^{k} \mathbb{P}(\overline{MSA_i} = w_l | X_{(k+1)} = w_l) = \sum_{i=l-k}^{p-k-2} 2^{-k-i+l-1} + \sum_{i=p-k}^{k} 2^{-k-i+l} = 1 - 2^{-2k+l}.
$$

Case 3. w<sup>l</sup> is blocked, and that either w<sup>l</sup> is not ill-paired, or it is ill-paired and m<sup>l</sup> < p. In this case,

$$
\sum_{i=1}^{k} \mathbb{P}(\overline{MSA}_{i} = w_{l} | X_{(k+1)} = w_{l}) = \sum_{i=l-k}^{m_{l}-k-3} 2^{-k-i+l-1} + 2^{l-m_{l}+2} = 1.
$$

Case 4. w<sup>l</sup> is blocked and ill-paired, and m<sup>l</sup> > p. In this case,

$$
\sum_{i=1}^{k} \mathbb{P}(\overline{MSA_i} = w_l | X_{(k+1)} = w_l) = \sum_{i=l-k}^{p-k-2} 2^{-k-i+l-1} + \sum_{i=p-k}^{m_l-k-3} 2^{-k-i+l} + 2^{-m_l+l+3} = 1.
$$

Putting everything together, we obtain the desired result.

### <span id="page-15-0"></span>3.2 Proof of Theorem [2](#page-4-3)

In order to prove Theorem [2,](#page-4-3) we use algorithms MSA1, . . . , MSAk+1 defined in Section [3.](#page-4-0) Then, we define the randomized strategy MSARAND as follows: (1) before the game starts, select a number I in {1, . . . , k + 1} uniformly at random, that is, I = i with probability 1/(k + 1). (2) Play MSA<sup>I</sup> .

Note that, unlike the randomized algorithm we used to prove Theorem 1, here MSARAND does not need access to the identity of the arriving variables. In the next proposition, we prove that MSARAND has a competitive ratio of at least <sup>1</sup> 2(k+1) . This directly implies Theorem [2.](#page-4-3) Indeed, MSARAND is a randomization over single-threshold algorithms. By linearity of expectation, there exists a single-threshold strategy in the support of MSARAND that performs as well as MSARAND. Proposition 3. The strategy MSARAND has a competitive ratio <sup>1</sup> 2(k+1) .

Proof. We want to prove that E(MSARAND) ≥ 1 2(k+1)E(X(k+1)). First, note that

$$
(k+1)\mathbb{E}(MSA_{RAND}) = \sum_{i=1}^{k+1} \mathbb{E}(MSA_i)
$$
  
= 
$$
\sum_{i=1}^{k} \mathbb{E}(MSA_i|X_{(k+1)} = w_{k+i})\mathbb{P}(X_{(k+1)} = w_{k+i}) + \mathbb{E}(MSA_{k+1}).
$$

Now, using that for each i ∈ {1, . . . , k}

$$
\mathbb{E}(MSA_i|X_{(k+1)} = w_{k+i}) \ge w_{k+i} \mathbb{P}(MSA_i = w_{k+i}|X_{(k+1)} = w_{k+i}),
$$

we have that (k + 1)E(MSARAND) is at least

$$
\sum_{i=1}^{k} w_{k+i} \mathbb{P}(MSA_i = w_{k+i} | X_{(k+1)} = w_{k+i}) \mathbb{P}(X_{(k+1)} = w_{k+i}) + \mathbb{E}(MSA_{k+1}).
$$

In the remainder of the proof we bound P(MSA<sup>i</sup> = wk+<sup>i</sup> |X(k+1) = wk+i) for each i ∈ {1, . . . , k}, and E(MSAk+1).

Step 1. For each i ∈ {1, . . . , k},

$$
\mathbb{P}(MSA_i = w_{k+i} | X_{(k+1)} = w_{k+i}) \ge 1/2.
$$

On one hand,

$$
\mathbb{P}(MSA_i = w_{k+i}|X_{(k+1)} = w_{k+i}) \geq \mathbb{P}(\{MSA_i = w_{k+i}\} \cap {\tau_i = w_{k+i+1}\} | X_{(k+1)} = w_{k+i})
$$
  
=  $\mathbb{P}(\tau_i = w_{k+i+1} | X_{(k+1)} = w_{k+i}),$ 

where the equality stems from the fact that, when X(k+1) = wk+<sup>i</sup> and τ<sup>i</sup> = wk+i+1, MSA<sup>i</sup> picks wk+<sup>i</sup> .

On the other hand, it is easy to see that P(τ<sup>i</sup> = wk+i+1|X(k+1) = wl) is 1 if wk+<sup>i</sup> and wk+i+1 are paired, and 1/2, otherwise. Therefore, we obtain P(τ<sup>i</sup> = wk+i+1|X(k+1) = wl) ≥ 2 −1 . The first step is completed.

Step 2. E(MSAk+1) ≥ 1/2 P<sup>ξ</sup>k+1 <sup>l</sup>=2k+1 <sup>w</sup>lP(X(k+1) <sup>=</sup> <sup>w</sup>l).

If 2k + 2 = ξk+1, the elements of {w1, . . . , w2k+2} form k + 1 pairs. Hence, if w2k+2 ∈ S, which happens with probability 1/2, there are exactly k + 1 elements larger than w2k+2 that are in X. In that case, MSAk+1 picks X(k+1). It follows that

$$
\mathbb{E}(MSA_{k+1}) \geq \frac{1}{2}\mathbb{E}(X_{(k+1)}) \geq \frac{1}{2}\sum_{l=2k+1}^{\xi_{k+1}} w_l \mathbb{P}(X_{(k+1)} = w_l).
$$

If 2k + 2 ̸= ξk+1,

$$
\mathbb{E}(MSA_{k+1}) = \mathbb{E}(MSA_{k+1}1_{\tau_{k+1} = w_{2k+2}}) + \mathbb{E}(MSA_{k+1}1_{\tau_{k+1} \le w_{2k+3}}).
$$

By Lemma [1,](#page-7-0)

$$
\mathbb{E}(MSA_{k+1}1_{\tau_{k+1}=w_{2k+2}}) \geq \mathbb{P}(X_{(k+1)}=w_{2k+1})w_{2k+1}\frac{\delta_{2k+1}}{2},
$$

where δ2k+1 is equal to 1 since by definition w2k+1 cannot be blocked nor ill-paired. On the other hand, by Lemma [2,](#page-7-1) the second term is lower bounded by

$$
\frac{1}{2} \sum_{l=2(k+1)}^{\xi_{k+1}} \mathbb{P}(X_{(k+1)} = w_l) w_l.
$$

Putting all together, we obtain Step 2.

Combining Step 1 and Step 2, we conclude that

$$
(k+1)\mathbb{E}(MSA_{RAND}) \geq \frac{1}{2} \sum_{l=k+1}^{\xi_{k+1}} w_l \mathbb{P}(X_{(k+1)} = w_l) = \mathbb{E}(X_{(k+1)}),
$$

and the proof is completed.

## <span id="page-17-0"></span>4 Upper bound on competitive ratio

In this section, we provide two tightness results. The first one, Theorem [3,](#page-17-1) establishes the tightness of the competitive ratio result for FI k-RPI presented in Section [3,](#page-4-0) by providing a parameterized hard distribution for FI k-RPI showing that no algorithm can have a competitive ratio of γ, with γ > 1/(k + 2) + β for any β > 0. This leads to a similar result for the lower-information model NI. The second result, Proposition [4,](#page-18-0) shows that the strategy MSARAND cannot guarantee a competitive ratio better than 1/(2k + 2) in the NI model.

<span id="page-17-1"></span>Theorem 3. For each β > 0, there exists an instance with 2(k + 1) variables such that no algorithm has a competitive ratio larger than 1/(k + 2) + β, regardless of the information model in k-RPI.

Proof of Theorem [3.](#page-17-1) Let 0 < ε < 1/(k + 1). Consider the following 2(k + 1) random variables: X<sup>i</sup> = 1 ε <sup>i</sup>−1( k+1 <sup>i</sup>−1) for i ∈ {1, . . . , k + 1} and for i ∈ {k + 2, . . . , 2(k + 1)}

$$
X_i = \begin{cases} 0 & \text{w.p. } 1 - \varepsilon, \\ 1/\varepsilon^{k+1} & \text{w.p. } \varepsilon. \end{cases}
$$

We now prove that for the instance with these 2(k + 1) random variables, no strategy of the gambler can attain a competitive ratio larger than 1/(k + 2) + O(ε).

Note that, as ε < (k + 1)−<sup>1</sup> , it holds X<sup>i</sup> < Xi+1 for 1 ≤ i ≤ k. That is, the deterministic variables arrive in increasing order. Indeed, for i ∈ {1, . . . , k}, X<sup>i</sup> > Xi+1 if and only if ε < <sup>i</sup> <sup>k</sup>−i+2 . As <sup>i</sup> <sup>k</sup>−i+2 is increasing in i, it is enough to have ε < (k + 1)<sup>−</sup><sup>1</sup> .

Let us compute E(X(k+1)). To this end, note that the (k + 1)-th largest variable corresponds to X<sup>i</sup> with i ≤ k + 1 if and only if exactly i − 1 variables take the value 1/εk+1 (because the deterministic variables arrive in increasing order with respect to their value), and it corresponds to a variable X<sup>i</sup> with i ≥ k + 2 if and only if all variables j ≥ k + 2 take the value 1/εk+1 .

We define the random variable Y as the number of variables among Xk+2, . . . , X2(k+1) that take the value 1/εk+1. Then, conditioning on the value of Y , we have:

$$
\mathbb{E}(X_{(k+1)}) = \sum_{j=0}^{k+1} \mathbb{E}(X_{k+1}|Y=j)\mathbb{P}(Y=j)
$$
  
=  $1 \cdot (1 - \varepsilon)^{k+1} + \sum_{j=1}^{k} \mathbb{E}(X_{k+1}|Y=j) \cdot \mathbb{P}(Y=j) + \frac{1}{\varepsilon^{k+1}} \cdot \varepsilon^{k+1}$   
=  $(1 - \varepsilon)^{k+1} + \sum_{j=1}^{k} \frac{1}{\varepsilon^{j} {k+1 \choose j}} \cdot {k+1 \choose j} \cdot \varepsilon^{j} \cdot (1 - \varepsilon)^{k+1-j} + 1$   
=  $1 + \sum_{j=0}^{k} (1 - \varepsilon)^{k+1-j}$ 

Let us now compute the optimal guarantee of the gambler. First, observe that the gambler should always accept the value 1/ε<sup>k</sup>+1, as it is the highest possible one. Furthermore, since the deterministic values are strictly increasing, if the gambler sees that a deterministic value has been removed, then all remaining variables—both deterministic and non-deterministic—must have either been removed or are equal to zero. In this case, the gambler receives 0. As a result, the gambler does not gain any useful information from observing past values, allowing us to restrict to strategies of the following form: (1) stop at time i, for some i ≤ k + 1 (2) stop at the first positive value appearing after stage k + 2.

Call ALG<sup>i</sup> the payoff of a strategy of the form (1). Under such a strategy, the gambler picks X<sup>i</sup> if and only if there are at least i − 1 variables taking a value 1/ε<sup>k</sup>+1. That is, if Y is greater than or equal to i − 1. Then, under this strategy, the gambler obtains in expectation

$$
\mathbb{E}(ALG_i) = \sum_{j=i-1}^{k+1} \mathbb{E}(ALG_i|Y=j)\mathbb{P}(Y=j)
$$
  
= 
$$
\frac{1}{\varepsilon^{i-1} {k+1 \choose i-1}} \sum_{j=i-1}^{k+1} {k+1 \choose j} \varepsilon^j (1-\varepsilon)^{k+1-j}
$$
  
= 
$$
(1-\varepsilon)^{k-i+2} + \sum_{j=i}^{k+1} \frac{{k+1 \choose j}}{{k+1 \choose i-1}} \varepsilon^{j-i+1} (1-\varepsilon)^{k+1-j}.
$$

Last, consider strategy (2). This strategy gets a positive payoff if and only if all variables Xk+2, . . . , X2(k+2) are positive, which happens with probability ε <sup>k</sup>+1. When this is the case, it gets payoff ε <sup>−</sup>(k+1). Consequently, (2) guarantees ε k+1 · ε <sup>−</sup>(k+1) = 1.

It follows that the optimal payoff of the gambler goes to 1 as ε → 0. Moreover, we have E(X(k+1)) → k + 2 as ε → 0. Consequently, for each β > 0, one can find ε > 0 such that no algorithm achieves a competitive ratio larger than 1/(k + 2) + β in the corresponding instance. This proves the theorem.

In what follows, we formally establish that the competitive ratio of 1/(2k + 2) is tight for our proposed algorithm, MSARAND.

<span id="page-18-0"></span>Proposition 4. For each β > 0, there exists an instance with k + 2 variables where MSARAND does not achieve a better competitive ratio than 1/(2k + 2) + β.

Proof. Let X<sup>1</sup> := 1 and for i ∈ {2, . . . , k + 2}

$$
X_i = \begin{cases} 0 & \text{w.p. } 1 - \varepsilon, \\ 1/\varepsilon^{k+2} & \text{w.p. } \varepsilon, \end{cases}
$$

with ε ≤ 1/2. We start by computing and estimating the (k + 1)-max.

$$
\mathbb{E}(X_{(k+1)}) = 1 \cdot (1 - \varepsilon^{k+1}) + \varepsilon^{-k-2} \cdot \varepsilon^{k+1},
$$

hence ε <sup>−</sup><sup>1</sup> ≤ E(X(k+1)) ≤ ε <sup>−</sup><sup>1</sup> + 1.

Let us now analyze MSARAND. First, we show that for i ≥ 2, E(MSAi) is O(1) as ε tends to 0.

Let i ∈ {2, . . . , k + 1}, and A be the event "the i-th largest sample is 0". The probability of A is at least (1 − ε) <sup>k</sup>+1, since the latter corresponds to the probability that all samples from F2, . . . , Fk+2 are 0. Then, P(A) tends to 1 as ε goes to 0.

Assume that A holds, meaning that the threshold for MSA<sup>i</sup> is 0. In this case, either X<sup>1</sup> is available and then MSA<sup>i</sup> picks it; or X<sup>1</sup> is not available and then the gambler is presented only with 0, getting a value 0. We deduce that

$$
\mathbb{E}(MSA_i|A) \le 1 \le \varepsilon \mathbb{E}(X_{(k+1)}).
$$

When A is not realized, we use the rough upper bound E(MSA<sup>i</sup> |A<sup>c</sup> ) ≤ E(X(k+1)). Therefore, we have

$$
\mathbb{E}(MSA_i) \leq \varepsilon \mathbb{E}(X_{(k+1)}) \mathbb{P}(A) + \mathbb{E}(X_{(k+1)}) (1 - \mathbb{P}(A)).
$$

Since P(A) converges to 1 as ε tends to 0, we deduce that E(MSAi)/E(X(k+1)) converges to 0 as ε goes to 0, as we wanted to show.

It remains to evaluate the performance of MSA1. To this end, let us define B the event "the maximum sample is 1". Note that this event occurs with probability (1 − ε) k+1 .

Assume that B holds, meaning that the threshold for MSA<sup>1</sup> is 1. Then, either X<sup>1</sup> is available, and thus MSA<sup>1</sup> picks X<sup>1</sup> = 1 with probability 1/2, and picks either ε <sup>−</sup>k−<sup>2</sup> or 0 with probability 1/2; or, the gambler is presented only with 0.

Hence,

$$
\mathbb{E}(MSA_1|B) \le 1 + \varepsilon^{k+1} \left[\frac{1}{2} + \frac{1}{2}\varepsilon^{-k-2}\right] \le 2 + \frac{1}{2}\varepsilon^{-1} \le (2\varepsilon + 1/2)\mathbb{E}(X_{(k+1)}).
$$

If B is not realized, we use again the inequality E(MSA1|B<sup>c</sup> ) ≤ E(X(k+1)), obtaining that

$$
\mathbb{E}(MSA_1) \le (2\varepsilon + 1/2)\mathbb{E}(X_{(k+1)})\mathbb{P}(B) + \mathbb{E}(X_{(k+1)})(1 - \mathbb{P}(B)).
$$

Since P(B) converges to 1 as ε tends to 0, we conclude that

$$
\limsup_{\varepsilon \to 0} \mathbb{E}(MSA_{RAND})/\mathbb{E}(X_{(k+1)}) \le \frac{1}{2k+2},
$$

and the result is proved.

## <span id="page-19-0"></span>5 I.I.D. Case for k = 1

In this section, we focus on i.i.d. instances where F<sup>1</sup> = · · · = F<sup>n</sup> in the case of k-RPI with k = 1. That is, in the sequence X1, . . . , Xn, the maximum value has been removed. The main result of this section is the following:

<span id="page-19-1"></span>Theorem 4. For any information model, there is an algorithm for 1-RPI with a competitive ratio of at least 0.4901.

For the rest of the section, we assume than X<sup>i</sup> are continuous with cdf F(·). Furthermore, following [\(Perez-Salazar and Verdugo,](#page-30-8) [2024\)](#page-30-8), we can also assume that F is strictly increasing and infinitely differentiable. Since the gambler observes the sequence of n − 1 values, we can assume that the maximum of the n values occurs in the last position n. Hence, the gambler faces the problem under the event E = {X1, . . . , Xn−<sup>1</sup> < Xn} = {maxi<n X<sup>i</sup> < Xn}. Note that P(E) = 1/n by the continuity of F.

To prove Theorem [4,](#page-19-1) we provide a fixed-threshold strategy that computes a threshold based on quantiles q ∈ [0, 1]. That is, given q ∈ [0, 1], the algorithm computes u ≥ 0 such that q = P(X ≥ τ ) = 1−F(τ ), and accepts the first value at least u in the observed sequence. We denote such an algorithm ALGq. The following lemma provides a lower bound for a particular choice of quantiles q.

<span id="page-19-2"></span>Lemma 7. Let n ≥ 3. For NI 1-RPI, if ALG<sup>q</sup> is run with q = α/(n − 1) for α ∈ [0, 2], then,

$$
\frac{\mathbb{E}(ALG_q)}{\mathbb{E}(X_{(2)})}\geq \min\left\{\frac{1-e^{-\alpha}}{\alpha},1-e^{-\alpha}(1+\alpha)\right\},\,
$$

for any continuous cdf F.

Using this lemma, and by equating (1 − e <sup>−</sup>α)/α = 1 − e <sup>−</sup>α(1 + α), we obtain that α ≈ 1.64718 and the competitive ratio of fixed-threshold solutions is ≥ 0.4901.

In Subsection [5.2,](#page-24-0) we show that no fixed-threshold solution can obtain a competitive ratio better than 0.5463.

### 5.1 Proof of Lemma [7](#page-19-2)

In this subsection, we provide the lower bound on the competitive ratio of ALG<sup>q</sup> for q = α/(n − 1). For notational convenience, we will avoid writing the subscript in ALG. The algorithm ALG computes the threshold u in advance and accept the first observed value that surpassed u. Then, the reward of ALG as a function of u is

$$
\mathbb{E}(ALG) = \sum_{i=0}^{n-2} \mathbb{P}(X_1, \dots, X_i < u \mid E) \mathbb{E}\left[X_{i+1} \mathbf{1}_{\{X_{i+1} \ge u\}} \mid X_1, \dots, X_i < u, E\right]
$$

By analyzing the different involved probabilities, we can find the following characterization of E(ALG) as a function of the quantile q:

<span id="page-20-1"></span>Proposition 5. If q ∈ [0, 1], then

$$
\mathbb{E}(ALG) = \int_0^1 r(v) \sum_{i=0}^{n-2} \frac{n}{n-i-1} (1-q)^i \left( \min\{q, v\} - \left( \frac{1 - (1 - \min\{q, v\})^{n-i}}{n-i} \right) \right) dv,
$$

here r(v) ≥ 0 is such that F −1 (1 − u) = R <sup>1</sup> u r(v) dv which exists due to the assumptions over the cdf F.

The proof of this proposition is technical appears at the of the section. Likewise, we can find an expression for E(X(2)) in terms of r(v) from the Proposition:

$$
\mathbb{E}(X_{(2)}) = \int_0^1 r(v)\mathbb{P}(\text{Binom}(n, v) \ge 2) \, \mathrm{d}v.
$$

Then,

$$
\frac{\mathbb{E}(ALG)}{\mathbb{E}(X_{(2)})} \ge \inf_{v \in [0,1]} \left\{ \frac{\sum_{i=0}^{n-2} \frac{n}{n-i-1} (1-q)^i \left( \min\{q, v\} - \left( \frac{1-(1-\min\{q, v\})^{n-i}}{n-i} \right) \right)}{\mathbb{P}(\text{Binom}(n, v) \ge 2)} \right\}
$$

This last bound is instance-independent and only depends on n and q. Let An,q(v) be the function in the infimum. We study the the infimum of An,q(v) for the regime v > q and v < q separately. The following proposition characterizes the behavior of An,q(v) in both regimes for q = α/(n − 1) and α ≤ 2. We defer the proof to the end of the section.

<span id="page-20-0"></span>Proposition 6. For q = α/(n − 1) and α ≤ 2, we have

1. If v > q, then, An,q(v) is decreasing in v;

2. If v ≤ q, then, An,q(v) is increasing in v.

With this proposition, we obtain

$$
\frac{\mathbb{E}(ALG)}{\mathbb{E}(X_{(2)})} \ge \min \left\{ \inf_{v \in [0,q]} \{A_{n,q}(v)\}, \inf_{v \in [q,1]} \{A_{n,q}(v)\} \right\}
$$
\n
$$
= \min \left\{ \lim_{v \to 0} A_{n,q}(v), A_{n,q}(1) \right\}
$$
\n
$$
= \min \left\{ (n-1) \frac{1 - (1-q)^{n-1}}{q}, 1 - (1-q)^{n-1} (1 + (n+1)q) \right\}
$$
\n
$$
\ge \min \left\{ \frac{1 - e^{-\alpha}}{\alpha}, 1 - e^{-\alpha} (1 + \alpha) \right\}
$$

where in the first equality we use Proposition [6,](#page-20-0) the second equality follows by a simple calculation, and in the last inequality we use the standard inequality 1 − x ≤ e −x . This finishes the proof of Lemma [7](#page-19-2) and by setting q = α/(n − 1). This finishes the proof of the lemma.

We now provide the missing proofs of Proposition [5](#page-20-1) and [6.](#page-20-0)

Proof of Proposition [5.](#page-20-1) The probability of reaching i+1 is the same as the probability of failing to observe a value at least larger than u among the first i observations, which is given by P(X1, . . . , X<sup>i</sup> < u | E), while the reward at i + 1 is the expected value when Xi+1 ≥ u. For the sake of notation, we define Ei,u = {X1, . . . , X<sup>i</sup> < u} for i = 0, . . . , n−2. We need to compute P(Ei,u | E) and P(Xi+1 < u′ | Ei,u, E) for u ′ ≥ u.

Clearly P(X<sup>1</sup> < u, . . . , X<sup>i</sup> < u | E) = 1 for i = 0 so let's assume that i > 0. Then,

$$
\mathbb{P}(E_{i,u} \mid E) = n \int_0^\infty \mathbb{P}(X_1, \dots, X_i < u, X_1, \dots, X_{n-1} < x) \, \mathrm{d}F(x) \\
= n \int_0^u F(x)^{n-1} \, \mathrm{d}F(x) + n \int_u^\infty F(u)^i F(x)^{n-i-1} \, \mathrm{d}F(x)
$$

$$
= F(u)^n + nF(u)^i \left( \frac{1 - F(u)^{n-i}}{n-i} \right)
$$

Note that if we assume that 0<sup>0</sup> = 1, then the formula above also work in the case i = 0. Now, for u ′ ≥ u,

$$
\mathbb{P}(X_{i+1} < u' \mid E_{i,u}, E) = n \frac{\mathbb{P}(X_{i+1} < u', E_{i,u}, E)}{\mathbb{P}(E_{i,u} \mid E)}
$$

we already computed the denominator; hence, we focus on computing the numerator.

$$
n\mathbb{P}(X_{i+1} < u', E_{i,u}, E) = n \int_0^\infty \mathbb{P}(X_{i+1} < u', X_1, \dots, X_i < u, X_1, \dots, X_{n-1} < x) \, \mathrm{d}F(x)
$$
\n
$$
= n \int_0^u F(x)^{n-1} \, \mathrm{d}F(x) + n \int_u^u F(u)^i F(x)^{n-i-1} \, \mathrm{d}F(x)
$$
\n
$$
+ nF(u)^i F(u') \int_{u'}^\infty F(x)^{n-i-2} \, \mathrm{d}F(x)
$$
\n
$$
= F(u)^n + nF(u)^i \left( \frac{F(u')^{n-i} - F(u)^{n-i}}{n-i} \right)
$$
\n
$$
+ nF(u)^i F(u)' \left( \frac{1 - F(u')^{n-i-1}}{n-i-1} \right)
$$
\n
$$
= F(u)^n - \frac{n}{n-i} F(u)^n + \frac{n}{n-i} F(u)^i F(u')^{n-i}
$$
\n
$$
+ \frac{n}{n-i-1} F(u)^i F(u') - \frac{n}{n-i-1} F(u)^i F(u')^{n-i}
$$

Then,

$$
\frac{\mathrm{d}}{\mathrm{d}x} \mathbb{P}(X_{i+1} < x \mid E_{i,u}, E) = F(u)^i \left(\frac{n}{n-i-1}\right) \frac{1 - F(x)^{n-i-1}}{\mathbb{P}(E_{i,u} \mid E)} \frac{\mathrm{d}F}{\mathrm{d}x}(x)
$$

and

$$
\mathbf{E}\left[X_{i+1}\mathbf{1}_{\{X_{i+1}\geq u\}} \mid E_{i,u}, E\right] = \frac{1}{\mathbb{P}(E_{i,u} \mid E)} F(u)^{i} \frac{n}{n-i-1} \int_{u}^{\infty} x\left(1 - F(x)^{n-i-1}\right) dF(x).
$$

Then,

$$
\mathbb{E}(ALG) = \sum_{i=0}^{n-2} \frac{n}{n-i-1} F(u)^i \int_u^{\infty} x (1 - F(x)^{n-i-1}) dF(x)
$$
  
= 
$$
\sum_{i=0}^{n-2} \frac{n}{n-i-1} (1 - q)^i \int_0^q F^{-1}(1 - w)(1 - (1 - w)^{n-i-1}) dw
$$
  
(Change of variable  $1 - q = F(u)$ )

$$
= \sum_{i=0}^{n-2} \frac{n}{n-i-1} (1-q)^i \int_0^q \int_w^1 r(v) dv (1-(1-w)^{n-i-1}) dw
$$
  
(Using that  $F^{-1}(1-w)$  is strictly decreasing and differentiable)  
$$
= \int_0^1 r(v) \sum_{i=0}^{n-2} \frac{n}{n-i-1} (1-q)^i \left( \min\{q, v\} - \left( \frac{1-(1-\min\{q, v\})^{n-i}}{n-i} \right) \right) dv
$$

Proof of Proposition [6.](#page-20-0) For v > q, we have

$$
A_{n,q}(v) = \frac{\sum_{i=0}^{n-2} \frac{n}{n-i-1} (1-q)^i \left( q - \left( \frac{1-(1-q)^{n-i}}{n-i} \right) \right)}{\mathbb{P}(\text{Binom}(n,v) \ge 2)} = \frac{1 - (1-q)^{n-1} (1 + (n-1)q)}{\mathbb{P}(\text{Binom}(n,v) \ge 2)}
$$

This last function is decreasing in v attaining its minimum at v = 1.

For v ≤ q, we have

$$
A_{n,q}(v) = \frac{\sum_{i=0}^{n-2} \frac{n}{n-i-1} (1-q)^i \left( v - \left( \frac{1-(1-v)^{n-i}}{n-i} \right) \right)}{\mathbb{P}(\text{Binom}(n, v) \ge 2)}
$$
  
= 
$$
\sum_{i=0}^{n-2} \frac{n}{(n-i-1)(n-i)} (1-q)^i \left( \frac{(1-v)^{n-i} - (1-(n-i)v)}{\mathbb{P}(\text{Binom}(n, v) \ge 2)} \right)
$$
  
= 
$$
\sum_{i=2}^{n} \frac{n}{i(i-1)} (1-q)^i G_{n,n-i}(v),
$$

where Gn,i(v) = (1−v) <sup>i</sup>−(1−iv) <sup>P</sup>(Binom(n,v)≥2) for i ∈ {2, . . . , n}. To conclude that An,q(v) is increasing in v, it is enough to show that Gn,i(v) is increasing in v for all i ∈ {2, . . . , n}. Then,

$$
G'_{n,i}(v) = \frac{g_{n,i}(v)}{\mathbb{P}(\text{Binom}(n,v) \ge 2)^2},
$$

where gn,i(v) = (−i(1 − v) <sup>i</sup>−<sup>1</sup> + i)P(Binom(n, v) ≥ 2) − ((1 − v) <sup>i</sup> − (1 − iv))n(n − 1)v(1 − v) n−2 . To conclude the proof, it is enough to show that gn,i(v) ≥ 0. We note that gn,i(0) = 0, so we only need to prove that g ′ n,i(v) ≥ 0. Now,

$$
g'_{n,i}(v) = i(i-1)(1-v)^{i-2} \mathbb{P}(\text{Binom}(n, v) \ge 2) - ((1-v)^{i} - (1-iv))n(n-1)(1-v)^{n-2} + ((1-v)^{i} - (1-iv))n(n-1)(n-2)v(1-v)^{n-3} = i(i-1)(1-v)^{i-2} \mathbb{P}(\text{Binom}(n, v) \ge 2) - n(n-1)(1-(n-1)v)(1-v)^{n-3}((1-v)^{i} - (1-iv))
$$

Using the second equality it is easy to verify that g ′ n,i(v) ≥ 0 for v ≥ 1/(n − 1). Hence, from now, we assume that v < 1/(n − 1). Furthermore, by inspection, we can verify that g ′ n,i(v) ≥ 0 for n ∈ {3, 4} and i ∈ {2, . . . , n}; hence, from now on, we assume that n ≥ 5. The following claim allows us to focus only on lower bounding g ′ n,2 (v).

<span id="page-22-0"></span>Claim 1. For n ≥ 5, v ≤ 1/(n − 1) and for all i ∈ {2, . . . , n − 1}, we have g ′ n,i(v) ≤ g ′ n,i+1(v).

This proof requires lower bounding several polynomials and various case analysis; hence, we defer it to the end. Now, note that

$$
g'_{n,2}(v) = 2\mathbb{P}(\text{Binom}(n, v) \ge 2) - n(n-1)(1 - (n-1)v)(1 - v)^{n-3}v^2
$$
  
\n
$$
\ge n(n-1)v^2(1 - v)^{n-2} - n(n-1)(1 - (n-1)v)v^2(1 - v)^{n-3}
$$
  
\n
$$
= n(n-1)v^2(1 - v)^{n-3}(1 - (1 - v)(1 - (n-1)v))
$$
  
\n
$$
= n(n-1)v^2(1 - v)^{n-3}(v + (n-1)v(1 - v))
$$

where in the first inequality we use the lower bound P(Binom(n, v) ≥ 2) ≥ P(Binom(n, v) = 2). From here, we obtain that g ′ n,2 (v) ≥ 0 with equality at v = 0. Using Claim [1,](#page-22-0) we conclude that G′ n,i(v) ≥ 0 for all i ∈ {2, . . . , n}.

Proof of Claim [1.](#page-22-0) Indeed,

$$
g'_{n,i}(v) - g'_{n,i+1}(v) = -\mathbb{P}(\text{Binom}(n, v) \ge 2) \left(2 - (i+1)v\right) i(1-v)^{i-2} \tag{6}
$$

<span id="page-23-0"></span>+ v(1 – (1 – v)<sup>i</sup>)(1 – (n – 1)v)n(n – 1)(1 – v)<sup>n-3</sup>  
\n
$$
\leq (1 - v)^{i-3} \Big( -i (2 - (i + 1)v) (1 – v) \mathbb{P}(\text{Binom}(n, v) \geq 2) + v(1 - (1 - v)^{i})(1 – (n - 1)v)n(n - 1) \Big) \tag{7}
$$

where in the inequality we use that (1 − v) <sup>n</sup>−<sup>3</sup> ≤ (1 − v) i−3 . Now, let ϕn,i(v) be the term in the big parenthesis in [\(7\)](#page-23-0), so g ′ n,i(v) − g ′ n,i+1(v) ≤ (1 − v) <sup>i</sup>−3ϕn,i(v). We now focus on proving that ϕn,i(v) ≤ 0. For this, we will reduce the problem to bounding only ϕn,2(v) ≤ 0 by proving that ϕn,2(v) ≥ ϕn,i(v) for all i = 2, . . . , n − 1. To prove this last inequality, we analyze the difference ϕn,i(v) − ϕn,i+1(v) for i ∈ {2, . . . , n − 2}:

$$
\phi_{n,i}(v) - \phi_{n,i+1}(v)
$$
  
= 2(1 - v)(1 - (i + 1)v) \mathbb{P}(\text{Binom}(n, v) \ge 2) - v^2 (1 - v)^i (1 - (n - 1)v) n(n - 1)  
$$
\ge v^2 (1 - v)^i (n - 1) n \left( (1 - (i + 1)v)(1 - v)^{n - i - 2} \left( 1 + \frac{n - 5}{3} v \right) - (1 - v(n - 1)) \right)
$$
  
= v^2 (1 - v)^i (n - 1) n \cdot \theta\_{n,i}(v).

Note that the function (1 − (i + 1)v)(1 − v) n−i−2 is decreasing in i; hence, for n ≥ 5, we have θn,i(v) ≥ θn,n−2(v). From here, we obtain

$$
\phi_{n,i}(v) - \phi_{n,i+1}(v) \ge v^2 (1-v)^i (n-1) n \cdot \theta_{n,n-2}(v).
$$

On the other hand, we have θn,n−2(v) = (1 − (n − 1)v) n−5 3 v ≥ 0. From here, we obtain that ϕn,i(v) ≥ ϕn,i+1(v) for all i ∈ {2, . . . , n − 2} and so ϕn,2(v) ≥ ϕn,i(v) for all i ∈ {2, . . . , n − 2}. Now,

$$
\phi_{n,2}(v) = -2(2-3v)(1-v)\mathbb{P}(\text{Binom}(n,v) \ge 2) + v^2(2-v)(1-(n-1)v)n(n-1)
$$
  
\n
$$
\le -2(2-3v)(1-v)\left(\binom{n}{2}v^2(1-v)^{n-2} + \binom{n}{3}v^3(1-v)^{n-3}\right)
$$
  
\n
$$
+v^2(2-v)(1-(n-1)v)n(n-1)
$$
  
\n
$$
= n(n-1)v^2\left(-(2-3v)\left((1-v)^{n-1} + \frac{n-2}{3}v(1-v)^{n-2}\right) + (2-v)(1-(n-1)v)\right)
$$
  
\n
$$
= n(n-1)v^2\left((2-v)(1-(n-1)v) - (2-3v)(1-v)^{n-2}\left(1 + \frac{n-5}{3}v\right)\right)
$$

We analyze this last bound for the case n = 5 and case n ≥ 6 separately. For n = 5, we have

$$
\phi_{5,2}(v) \le 20v^2 \left( (2-v)(1-4v) - (2-3v)(1-v)^3 \right).
$$

The polynomial (2 − v)(1 − 4v) − (2 − 3v)(1 − v) <sup>3</sup> has roots v ∈ {0,(11 − i √ 11)/6,(11 + i √ 11)/6} with 0 having multiplicity 2. Since, the polynomial tends to −∞ when v → ∞ and its only 0 when v = 0, we deduce that ϕ5,2(v) ≤ 0.

Now, assume that n ≥ 6, then

$$
\phi_{n,2}(v) \le n(n-1)v^2 \left( (2-v)(1-v)^{n-1} - (2-3v)(1-v)^{n-2} \left( 1 + \frac{n-5}{3}v \right) \right)
$$
  
=  $n(n-1)v^2 \left( (1-v)^{n-2} \cdot v \cdot \left( -\frac{2(n-5)}{3} + (n-4)v \right) \right)$ 

where in the first inequality we use Bernoulli's inequality on 1−(n−1)v ≤ (1−v) <sup>n</sup>−<sup>1</sup> and in the equality we simply reorder the big parenthesis from the previous line. Now, the polynomial v(−2(n−5)/3+ (n−4)v) has 2 roots at v ∈ {0, 2(n − 5)/(3(n − 4)). Hence, for v ≤ 1/(n − 1), we must have that ϕn,2(v) ≤ 0.

Going back to the function g ′ n,i, all our calculations give us

$$
g'_{n,i}(v) - g'_{n,i+1}(v) \le (1 - v)^{i-3} \phi_{n,2}(v) \le 0,
$$

which finishes the proof of Claim [1.](#page-22-0)

### <span id="page-24-0"></span>5.2 An Upper Bound for Single-Threshold Solutions

We present an instance that shows that no strategy in the class of single-threshold (including randomization) can obtain a competitive ratio larger than 0.5463. We use a counterexample motivated by [Perez-Salazar et al.](#page-30-9) [\(2025\)](#page-30-9). For n ≥ 1, we consider the following function from (0, 1] to R<sup>+</sup>

$$
f(u)=\frac{a\cdot c_n}{u} {\bf 1}_{(0,1/n^{10})}(u)+b\cdot {\bf 1}_{[1/n^{10},\beta/n]}(u)
$$

where 1<sup>X</sup> is the indicator function that is 1 for u ∈ X and 0 for u /∈ X, a, b > 0 and β > 1/n are positive constants to be optimized and c<sup>n</sup> = n · 1 − 1 − 1/n<sup>10</sup><sup>n</sup>−<sup>1</sup> <sup>−</sup><sup>1</sup> . We are going to assume that a + b ≤ 1. For n large enough, we have that f is nonincreasing.

Now, we can construct a random variable from f as follows. First, we add a small perturbation to f so f is smooth and strictly decreasing. This can be done by taking a convolution with a smooth function. Let's call f<sup>ε</sup> the resulting function, with small error ε > 0; hence, when ε → 0, we have fε(u) → f(u), except for a set of measure 0. Note that f<sup>ε</sup> is surjective in R+. Now, for x ≥ 0, let Fε(x) = 1 − f − ε (x). Note that F is increasing, Fε(0) = 0 and Fε(+∞) = 1; hence, F<sup>ε</sup> is a valid CDF. We define the random variable X<sup>ε</sup> to be the random variable following Fε. By construction, F −1 ε (1 − u) = fε(u). For ε → 0, we have F −1 ε (1 − u) → f(u), except for a set of measure 0 in [0, 1]. To avoid notational clutter, from now on, we simply work with f(u) instead of fε. By abusing notation, we will write F −1 (1 − u) = f(u), but it has to be understood that this equality occurs except for the points 1/n<sup>10</sup> and β/n.

We now consider a sequence of n independent random variables following F (the limit of F<sup>ε</sup> when ε → 0). We assume that n is large. The result now follows from the following two Lemmas.

<span id="page-24-1"></span>Lemma 8. We have E(X(2)) → a + b(1 − e −β (1 + β)) when n → ∞.

<span id="page-24-2"></span>Lemma 9. There is n<sup>0</sup> ≥ 0 such that for any algorithm ALG, if the input is of length n ≥ n0, the value collected by the algorithm is bounded as

$$
\mathbb{E}(\mathbf{ALG}) \le p(a, b, \beta) + 5\frac{\beta(1+\beta)^2}{n-\beta},
$$

where p(a, b, β) = maxλ∈[0,β] a(1 − e −λ )/λ + b 1 − e −λ (1 + λ) .

We first provide the tight upper bound and then we prove the lemmas. Using these two lemma, for any algorithm and n ≥ n0, we have

$$
\frac{E(\mathrm{ALG})}{\mathbb{E}(X_{(2)})} \le \frac{p(a, b, \beta) + 5\beta(1+\beta)^2/(n-\beta)}{\mathbb{E}(X_{(2)})}
$$

Using numerical optimization to minimize p(a, b, β), we found a ≈ 0.5463, b ≈ 0.4537 and β ≈ 109.131, we obtain p(a, b, β) ≈ 0.5463. Hence, for n large, we obtain that E(ALG)/E(X(2)) ≤ 0.5463 + o(n). This shows that with one threshold, we cannot recover the approximation of 1 − 1/e ≈ 0.6321 in the standard prophet inequality.

In the remainder of the subsection, we present the proof of Lemma [8](#page-24-1) and [9.](#page-24-2)

Proof of Lemma [8.](#page-24-1) We have

$$
\mathbb{E}(X_{(2)}) = n(n-1) \int_0^1 F^{-1}(1-u)q(1-q)^{n-2} dq
$$

$$
= n(n-1) \int_0^{1/n^{10}} a \cdot c_n (1-q)^{n-2} dq + n(n-1) \int_{1/n^{10}}^{\beta/n} bq (1-q)^{n-2} dq
$$
  
=  $ac_n n \left( 1 - \left( 1 - \frac{1}{n^{10}} \right) \right)$   
+  $b \left( \frac{1}{n^9} \left( 1 - \frac{1}{n^{10}} \right)^{n-1} - \beta \left( 1 - \frac{\beta}{n} \right)^{n-1} + \left( 1 - \frac{1}{n^{10}} \right)^n - \left( 1 - \frac{\beta}{n} \right)^n \right)$ 

The conclusion now follows by taking limit in the last equality.

Proof of Lemma [9.](#page-24-2) We can parametrize every single-threshold algorithm via the quantile chosen by it. If ALG<sup>q</sup> denotes the value obtained by a single-threshold algorithm that always uses quantile q, we have ALG ≤ maxq∈[0,1] ALGq. We analyze this last maximum for q ≤ 1/n<sup>10</sup> , q ∈ [1/n10, β/n] and q ≥ β/n.

For q ≤ 1/n10, we have

$$
\mathbb{E}(\text{ALG}_q) = \sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \int_0^q \frac{ac_n}{w} (1-(1-w)^k) dw
$$
  
\n
$$
\leq a \cdot c_n \sum_{k=1}^{n-1} \frac{n}{k} \sum_{\ell=0}^{k-1} \int_0^{1/n^{10}} (1-w)^{\ell} dw
$$
  
\n
$$
= ac_n \sum_{k=1}^{n-1} \sum_{\ell=0}^{k-1} \frac{1 - (1-1/n^{10})^{\ell+1}}{\ell+1}
$$
  
\n
$$
\leq ac_n \sum_{k=1}^{n-1} \frac{n}{k} \frac{k}{n^{10}}
$$
  
\n
$$
= a \cdot \frac{1/n^9}{1 - (1-1/n^{10})^{n-1}} \leq a \left(1 + \frac{3}{n-1}\right)
$$

where in the first inequality we upper bounded the integral for q = 1/n<sup>10</sup> and we also upper bounded (1 − q) <sup>n</sup>−k−<sup>1</sup> ≤ 1; in the second equality we performed the integration; in the second inequality we used Bernoulli's inequality: (1 − 1/n10) <sup>ℓ</sup>+1 ≥ 1 − (ℓ + 1)/n10, and in the last inequality we used the following claim.

Claim 2. We have cn/n<sup>8</sup> ≤ (1 + 3/(n − 1)).

Proof. First, note that

$$
\left(1 - \frac{1}{n^{10}}\right)^{n-1} \le e^{-(n-1)/n^{10}} \le 1 - \frac{n-1}{n^{10}} + \frac{(n-1)^2}{2n^{20}}\tag{8}
$$

Then,

<span id="page-25-0"></span>
$$
\frac{c_n}{n^8} = \frac{1/n^9}{1 - (1 - 1/n^{10})^{n-1}}
$$
  
\n
$$
\leq \frac{n}{(n-1)(1 - (n-1)/(2n^{10}))}
$$
  
\n
$$
\leq \left(1 + \frac{1}{n-1}\right)\left(1 + \frac{1}{2n}\right)
$$
  
\n
$$
\leq 1 + \frac{3}{n-1}
$$

where in the first inequality we used inequality [\(8\)](#page-25-0), the second inequality follows by 1 ≤ (1 − (n − 1)/2n <sup>10</sup>)(1 + 1/2n) and the last inequality follows simply by expanding the multiplication and bounding 1/2n, 1/(2n(n − 1)) ≤ 1/(n − 1).

Therefore, for any q ≤ 1/n10, we have that the value of the algorithm is upper bounded by a · (1 + 3/(n − 1)).

For q = λ/n ∈ [1/n10, β], we have

$$
\mathbb{E}(\text{ALG}_q) = \sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \int_0^{1/n^{10}} \frac{ac_n}{w} (1-(1-w)^k) dw
$$
  
+ 
$$
\sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \int_{1/n^{10}}^q b (1-(1-w)^k) dw
$$
  
$$
\leq ac_n \sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \frac{k}{n^{10}}
$$
  
+ 
$$
b \sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \left( \frac{(1-q)^{k+1} - ((1-1/n^{10})^{k+1} - (q-1/n^{10})(k+1))}{k+1} \right)
$$

where in the first inequality we upper bounded the first term in the first line in a manner similar to the case q ≤ 1/n<sup>10</sup> and we integrated the second term. The following two claims allow us to upper bound this last inequality by controlling the error.

<span id="page-26-0"></span>Claim 3. We have c<sup>n</sup> 1 − (1 − q) n−1 /(qn<sup>9</sup> ) ≤ (1 − e −λ )/λ + β(1 + β)/(n − β)

Proof. In the proof, we use that λ ≤ β ≤ 2. First, we note that

$$
1 - \left(1 - \frac{\lambda}{n}\right)^{n-1} = 1 - e^{-\lambda} + e^{-\lambda} - \left(1 - \frac{\lambda}{n}\right)^{n-1}
$$
  
\n
$$
\leq 1 - e^{-\lambda} + e^{-\lambda} - e^{-\lambda\left(\frac{n-1}{n-\lambda}\right)}
$$
  
\n
$$
= 1 - e^{-\lambda} + e^{-\lambda}\left(1 - e^{\lambda\left(\frac{1-\lambda}{n-\lambda}\right)}\right)
$$
  
\n
$$
\leq 1 - e^{-\lambda} + \frac{|\lambda(1-\lambda)|}{n-\lambda} \leq 1 - e^{-\lambda} + \frac{\beta(1+\beta)}{n-\beta}
$$

where in the first inequality we used that 1/(1−λ/n) ≤ e λ/(n−λ) using the standard inequality 1+x ≤ e x , in the second inequality we used that e <sup>−</sup><sup>λ</sup> ≤ 1 and 1 − |x| ≤ e −x , and in the last inequality, we simply used that λ ≤ β.

Claim 4. We have

$$
\sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \left( \frac{(1-q)^{k+1} - ((1-1/n^{10})^{k+1} - (q-1/n^{10})(k+1))}{k+1} \right)
$$
  
 
$$
\leq 1 - e^{-\lambda} (1+\lambda) + 3 \frac{\beta(1+\beta)^2}{n}.
$$

Proof. First, we have

$$
\sum_{k=1}^{n-1} \frac{n}{k(k+1)} (1-q)^{n-k-1} ((1-q)^{k+1} - (1-q(k+1))) = \sum_{k=1}^{n-1} nq^2 (1-q)^{n-k-1}
$$
$$
- (1-q)^n + (1-qn)
$$
$$
= 1 - (1-q)^{n-1} (1-q+qn)
$$

where in the first equality we simply rearranged the sum and in the next line we computed the sum. Then,

$$
\sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \left( \frac{(1-q)^{k+1} - ((1-1/n^{10})^{k+1} - (q-1/n^{10})(k+1))}{k+1} \right)
$$

$$
\leq \sum_{k=1}^{n-1} \frac{n}{k(k+1)} (1-q)^{n-k-1} \left( (1-q)^{k+1} - (1-q(k+1)) \right)
$$
  
+ 
$$
\sum_{k=1}^{n-1} \frac{n}{k(k+1)} (1-q)^{n-k-1} \left( 1 - (1-1/n^{10})^{k+1} \right)
$$
  
$$
\leq 1 - (1-q)^{n-1} (1-q+qn) + \sum_{k=1}^{n-1} \frac{1}{k} \frac{1}{n^9}
$$
  
$$
\leq 1 - (1-q)^{n-1} (1-q+qn) + \frac{\ln(n)}{n^9}
$$

where in the second inequality, we added and subtracted 1 to the parenthesis to form the term studied at the beginning of the proof, in the second inequality, we used Bernoulli's inequality and in the last inequality, we used the standard Harmonic sum bound. Finally,

$$
-(1-q)^{n-1}(1-q+qn) = -(1+\lambda)\left(1-\frac{\lambda}{n}\right)^{n-1} + \frac{\lambda}{n}\left(1-\frac{\lambda}{n}\right)^{n-1}
$$
$$
\leq -(1+\lambda)e^{-\lambda\left(\frac{n-1}{n-\lambda}\right)} + \frac{\beta}{n}
$$
$$
= \left(e^{-\lambda} - e^{-\lambda\left(\frac{n-1}{n-\lambda}\right)}\right)(1+\lambda) - e^{-\lambda}(1+\lambda) + \frac{\beta}{n}
$$
$$
\leq -e^{-\lambda}(1+\lambda) + \frac{\beta(1+\beta)^2}{n-\beta} + \frac{\beta}{n}
$$

where in the second and last inequalities, we used the same bounds used in the analysis of Claim [3.](#page-26-0) From here, the inequality of the claim follows.

Therefore,

$$
\mathbb{E}(\mathbf{ALG}_q) \le a \cdot \left(\frac{1 - e^{-\lambda}}{\lambda}\right) + b(1 - e^{-\lambda}(1 + \lambda)) + 4\frac{\beta(1 + \beta)^2}{n - \beta}
$$

For q ≥ β/n, we have

$$
\mathbb{E}(\text{ALG}_q) = \sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \int_0^{1/n^{10}} \frac{ac_n}{w} (1-(1-w)^k) dw
$$
  
+ 
$$
\sum_{k=1}^{n-1} \frac{n}{k} (1-q)^{n-k-1} \int_{1/n^{10}}^{\beta/n} b (1-(1-w)^k) dw.
$$

This last term is decreasing in q; hence it attains its maximum at q = β/n.

The conclusion of the lemma follows by putting together the three bounds that we found. Additionally, the bound for λ ∈ [1/n<sup>9</sup> , β] supersedes the bound for q ≤ 1/n<sup>10</sup> and q ≥ β/n.

## 6 Conclusion and Final Remarks

In this work, we introduced the residual prophet inequality problem (k-RPI), a new variant of the classical prophet inequality model where the top k variables are removed before observation. Our formulation highlights the impact of correlation in sequential selection problems and demonstrates that classical single-threshold approaches are insufficient in this setting. We provided a randomized algorithm with a competitive ratio of 1/(k + 2) for the FI model and showed the tightness of this bound. For the NI model, we give a randomized threshold algorithm with a competitive ratio of 1/(2k+ 2). Additionally, we analyzed the i.i.d. case of 1-RPI and proposed an algorithm with a competitive ratio of at least 0.4901. Furthermore, we proved that no single-threshold strategy can achieve a competitive ratio greater than 0.5464.

Since this is the first time k-RPI is introduced, our work opens up several promising directions for future research. One such direction is to investigate whether the 1/(k + 2) competitive ratio can be achieved using a threshold-based strategy. Another natural avenue is to determine the tight competitive ratio for single-threshold strategies in the i.i.d. case of 1-RPI, and to explore how these results might extend to k-RPI for k ≥ 2. One of the limitations of our current analysis is that it relies heavily on being able to compute probabilities under the condition of the maximum value being removed; these probabilities become intractable to handle for larger values of k. Naturally, determining the optimal policy k-RPI or even analyzing multi-thresholds strategies are exciting future questions, even for k = 1.

A natural extension is to explore k-RPI under natural combinatorial constraints such as cardinality or matroid constraints, where the gambler can select multiple values while satisfying feasibility conditions, as it has been studied for the classical prophet inequality by [Kleinberg and Weinberg](#page-29-7) [\(2012a\)](#page-29-7).

The k-RPI problem is very pessimistic as the k largest random variables are removed from the observed sequence. A more relaxed model would consider probabilities of failure. For example, a possibility could be where the i-th largest variable is removed with probability p<sup>i</sup> . This is related to the model by [Perez-Salazar et al.](#page-29-6) [\(2024\)](#page-29-6); [Smith](#page-30-10) [\(1975\)](#page-30-10) and [Tamaki](#page-30-11) [\(1991\)](#page-30-11) where p<sup>i</sup> = p for all i.

Finally, an interesting extension is to study if better competitive ratios for k-RPI can be obtained when the removed variables are not necessary the largest, and the gambler has some offline information regarding the variables and/or the values removed.

### Funding

This work was partially supported by ANID Chile through grants FB210005 (CMM), 11240705 (FONDE-CYT), and AFB230002 (ISCI); by ANR France grants ANR-21-CE40-0020 (CONVERGENCE), and ANR-17-EURE-0010 (Investissements d'Avenir program).

## References

- <span id="page-28-3"></span>Arsenis, M., Kleinberg, R.: Individual fairness in prophet inequalities. In: Proceedings of the 23rd ACM Conference on Economics and Computation, pp. 245–245 (2022)
- <span id="page-28-6"></span>Alaei, S.: Bayesian combinatorial auctions: Expanding single buyer mechanisms to many buyers. SIAM Journal on Computing 43(2), 930–972 (2014)
- <span id="page-28-2"></span>Abels, A., Pitschmann, E., Schmand, D.: Prophet inequalities over time. In: Proceedings of the 24th ACM Conference on Economics and Computation, pp. 1–20 (2023)
- <span id="page-28-8"></span>Correa, J., Cristi, A., Epstein, B., Soto, J.: The two-sided game of googol. Journal of Machine Learning Research 23(113), 1–37 (2022)
- <span id="page-28-5"></span>Correa, J., Foncea, P., Hoeksma, R., Oosterwijk, T., Vredeveld, T.: Posted price mechanisms and optimal threshold strategies for random arrivals. Mathematics of operations research 46(4), 1452–1478 (2021)
- <span id="page-28-1"></span>Correa, J., Foncea, P., Pizarro, D., Verdugo, V.: From pricing to prophets, and back! Operations Research Letters 47(1), 25–29 (2019)
- <span id="page-28-0"></span>Chawla, S., Hartline, J.D., Malec, D.L., Sivan, B.: Multi-parameter mechanism design and sequential posted pricing. In: Proceedings of the Forty-second ACM Symposium on Theory of Computing, pp. 311–320 (2010)
- <span id="page-28-7"></span>Chawla, S., Hartline, J.D., Malec, D.L., Sivan, B.: Multi-parameter mechanism design and sequential posted pricing. In: STOC 2010, pp. 311–320 (2010)
- <span id="page-28-4"></span>Disser, Y., Fearnley, J., Gairing, M., G¨obel, O., Klimm, M., Schmand, D., Skopalik, A., T¨onnis, A.: Hiring secretaries over time: The benefit of concurrent employment. Mathematics of Operations Research 45(1), 323–352 (2020)

- <span id="page-29-12"></span>D¨utting, P., Feldman, M., Kesselheim, T., Lucier, B.: Prophet inequalities made easy: Stochastic optimization by pricing nonstochastic inputs. SIAM Journal on Computing 49(3), 540–582 (2020)
- <span id="page-29-9"></span>Dynkin, E.B.: The optimum choice of the instant for stopping a markov process. Soviet Math. Dokl. 4, 627–629 (1963)
- <span id="page-29-16"></span>Ezra, T., Feldman, M., Kupfer, R.: Prophet inequality with competing agents. In: Algorithmic Game Theory: 14th International Symposium, SAGT 2021, Aarhus, Denmark, September 21–24, 2021, Proceedings 14, pp. 112–123 (2021). Springer
- <span id="page-29-10"></span>Ferguson, T.S.: Who solved the secretary problem? Statist. Sci. 4, 282–296 (1989)
- <span id="page-29-11"></span>Gilbert, J.P., Mosteller, F.: Recognizing the maximum of a sequence. Journal of the American Statistical Association 61(313), 35–73 (1966)
- <span id="page-29-17"></span>Gensbittel, F., Pizarro, D., Renault, J.: Competition and recall in selection problems. Dynamic Games and Applications 14(4), 806–845 (2024)
- <span id="page-29-3"></span>Gallego, G., Segev, D.: A constructive prophet inequality approach to the adaptive probemax problem. arXiv preprint arXiv:2210.07556 (2022)
- <span id="page-29-4"></span>Goyal, V., Udwani, R.: Online matching with stochastic rewards: Optimal competitive ratio via pathbased formulation. Operations Research 71(2), 563–580 (2023)
- <span id="page-29-1"></span>Hill, T.P., Kertz, R.P.: Comparisons of stop rule and supremum expectations of iid random variables. The Annals of Probability, 336–345 (1982)
- <span id="page-29-2"></span>Hajiaghayi, M.T., Kleinberg, R., Sandholm, T.: Automated online mechanism design and prophet inequalities. In: AAAI, vol. 7, pp. 58–65 (2007)
- <span id="page-29-13"></span>Hajiaghayi, M.T., Kleinberg, R., Sandholm, T.: Automated online mechanism design and prophet inequalities. In: AAAI 2007, pp. 58–65 (2007)
- <span id="page-29-5"></span>Huang, Z., Zhang, Q.: Online primal dual meets online matching with stochastic rewards: configuration lp to the rescue. In: Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pp. 1153–1164 (2020)
- <span id="page-29-18"></span>Immorlica, N., Kleinberg, R., Mahdian, M.: Secretary problems with competing employers. In: International Workshop on Internet and Network Economics, pp. 389–400 (2006). Springer
- <span id="page-29-19"></span>Karlin, A., Lei, E.: On a competitive secretary problem. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 29 (2015)
- <span id="page-29-0"></span>Krengel, U., Sucheston, L.: Semiamarts and finite values. Bulletin of the American Mathematical Society 83(4), 745–747 (1977)
- <span id="page-29-7"></span>Kleinberg, R., Weinberg, S.M.: Matroid prophet inequalities. In: Proceedings of the Forty-fourth Annual ACM Symposium on Theory of Computing, pp. 123–136 (2012)

<span id="page-29-14"></span>Kleinberg, R., Weinberg, S.M.: Matroid prophet inequalities. In: STOC 2012, pp. 123–136 (2012)

- <span id="page-29-8"></span>Lindley, D.V.: Dynamic programming and decision theory. J. Roy. Statist. Soc. Ser. C Appl. Statist. 10, 39–52 (1961)
- <span id="page-29-15"></span>Nuti, P., Vondr´ak, J.: Secretary problems: The power of a single sample. In: Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 2015–2029 (2023). SIAM

<span id="page-29-6"></span>Perez-Salazar, S., Singh, M., Toriello, A.: Robust online selection with uncertain offer acceptance.

Mathematics of Operations Research (2024)

- <span id="page-30-9"></span>Perez-Salazar, S., Singh, M., Toriello, A.: The iid prophet inequality with limited flexibility. Mathematics of Operations Research (2025)
- <span id="page-30-8"></span>Perez-Salazar, S., Verdugo, V.: Optimal guarantees for online selection over time. arXiv preprint arXiv:2408.11224 (2024)
- <span id="page-30-7"></span>Ramsey, D.: A stackelberg game based on the secretary problem: Optimal response is history dependent. arXiv preprint arXiv:2409.04153 (2024)
- <span id="page-30-2"></span>Rose, J.S.: A problem of optimal choice and assignment. Operations Research 30(1), 172–181 (1982)
- <span id="page-30-3"></span>Rinott, Y., Samuel-Cahn, E.: Comparisons of optimal stopping values and prophet inequalities for negatively dependent random variables. The Annals of Statistics 15(4), 1482–1490 (1987)
- <span id="page-30-4"></span>Rinott, Y., Samuel-Cahn, E.: Orderings of optimal stopping values and prophet inequalities for certain multivariate distributions. Journal of multivariate analysis 37(1), 104–114 (1991)
- <span id="page-30-5"></span>Rinott, Y., Samuel-Cahn, E.: Optimal stopping values and prophet inequalities for some dependent random variables. Lecture Notes-Monograph Series, 343–358 (1992)
- <span id="page-30-6"></span>Rubinstein, A., Wang, J.Z., Weinberg, S.M.: Optimal single-choice prophet inequalities from samples. In: 11th Innovations in Theoretical Computer Science Conference (2020)
- <span id="page-30-0"></span>Samuel-Cahn, E.: Comparison of threshold stop rules and maximum for independent nonnegative random variables. the Annals of Probability, 1213–1216 (1984)
- <span id="page-30-10"></span>Smith, M.: A secretary problem with uncertain employment. Journal of applied probability 12(3), 620– 624 (1975)
- <span id="page-30-11"></span>Tamaki, M.: A secretary problem with uncertain employment and best choice of available candidates. Operations Research 39(2), 274–284 (1991)
- <span id="page-30-1"></span>Vanderbei, R.J.: The postdoc variant of the secretary problem. Technical report, Tech. Report (2012)