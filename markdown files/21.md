# Flow Matching Meets Biology and Life Science: A Survey

Zihao Li∗†, Zhichen Zeng∗†, Xiao Lin∗†, Feihao Fang† , Yanru Qu† , Zhe Xu†‡, Zhining Liu† ,

> Xuying Ning† , Tianxin Wei† , Ge Liu⋄†, Hanghang Tong⋄†, Jingrui He⋄†

† University of Illinois Urbana-Champaign ‡ Meta

*Abstract*—Over the past decade, advances in generative modeling, such as generative adversarial networks, masked autoencoders, and diffusion models, have significantly transformed biological research and discovery, enabling breakthroughs in molecule design, protein generation, drug discovery, and beyond. At the same time, biological applications have served as valuable testbeds for evaluating the capabilities of generative models. Recently, flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling, with growing interest in its application to problems in biology and life sciences. This paper presents the first comprehensive survey of recent developments in flow matching and its applications in biological domains. We begin by systematically reviewing the foundations and variants of flow matching, and then categorize its applications into three major areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. For each, we provide an in-depth review of recent progress. We also summarize commonly used datasets and software tools, and conclude with a discussion of potential future directions. The corresponding curated resources are available at [https:](https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology) [//github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.](https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology)

*Index Terms*—Flow Matching, Generative Modeling, Molecule Generation, Protein Generation, Computational Biology, Artificial Intelligence, Survey

#### I. INTRODUCTION

Flow Matching (FM) [\[1\]](#page-15-0) has recently emerged as a powerful paradigm for generative modeling, offering a flexible and scalable framework applicable across a wide range of domains, such as computer vision [\[1,](#page-15-0) [2\]](#page-15-1), and natural language processing [\[3,](#page-15-2) [4\]](#page-15-3). By constructing a continuous probability trajectory between simple and complex distributions, FM provides an efficient and principled method to model high-dimensional, structured data. While FM has demonstrated strong performance in conventional generative tasks such as image, video, and language synthesis, its potential extends far beyond these domains. In particular, its ability to model diverse modalities while preserving structural and geometric constraints makes it especially well-suited for applications in biology and life sciences.

At the same time, biological and life science applications present a natural testbed for FM (Figure [1\)](#page-0-0). These tasks, ranging from genomic sequence modeling [\[5,](#page-15-4) [6,](#page-15-5) [7\]](#page-15-6), molecular graph generation [\[8,](#page-15-7) [9,](#page-15-8) [10\]](#page-15-9), and protein structure prediction [\[11,](#page-15-10) [12,](#page-15-11) [13\]](#page-15-12), to biomedical image synthesis [\[14,](#page-15-13) [15,](#page-15-14) [16,](#page-15-15) [17\]](#page-16-0), are often high-dimensional, multimodal, and governed by strict structural, physical, or biochemical constraints. In fact, they have already served as benchmarks for validating the performance of various generative modeling paradigms, such

![](_page_0_Figure_14.jpeg)

<span id="page-0-0"></span>Fig. 1. Flow Matching Meets Biological and Life Sciences. Flow matching serves as a powerful generative modeling paradigm for a wide range of biological and life science applications. Conversely, these domains offer rich and diverse tasks for evaluating and advancing flow matching techniques. In this survey, we first present state-of-the-art flow matching models and their variants, then categorize their applications into four major areas: sequence modeling, molecule generation, protein design, and other emerging biological applications.

as Generative Adversarial Networks [\[18,](#page-16-1) [19,](#page-16-2) [20\]](#page-16-3), Masked Autoencoders [\[21,](#page-16-4) [22,](#page-16-5) [23,](#page-16-6) [24\]](#page-16-7), and Diffusion Models [\[25,](#page-16-8) [26,](#page-16-9) [27\]](#page-16-10). Compared to traditional rule-based simulations [\[28,](#page-16-11) [29,](#page-16-12) [30,](#page-16-13) [31\]](#page-16-14) and physics-driven models [\[32,](#page-16-15) [33,](#page-16-16) [34,](#page-16-17) [35\]](#page-16-18), which often suffer from limited scalability and reliance on expert-crafted rules, these machine-learning-based generative models offer a datadriven alternative that can scale to complex biological systems, adapt to diverse modalities, and generalize beyond handcrafted constraints [\[36,](#page-16-19) [37,](#page-16-20) [38,](#page-16-21) [39,](#page-16-22) [40,](#page-16-23) [41,](#page-16-24) [42,](#page-16-25) [43,](#page-16-26) [44\]](#page-16-27). By learning directly from empirical data, they enable the generation of biologically plausible outputs while significantly reducing the need for domain-specific assumptions. FM, as a newer yet promising alternative, inherits key advantages from these models such as expressiveness, scalability, and data efficiency, while introducing a more stable training objective based on continuous probability flows. Its ability to generate highquality samples with fewer inference steps makes it particularly appealing for biological applications, where modeling precision and computational efficiency are both critical.

Interest in applying FM to biological problems is growing rapidly. As illustrated in Figure [2,](#page-1-0) we have observed a steadily growing trend in the number of FM-related publications, with a visible rise in bio-related applications. The first biological applications appeared at NeurIPS 2023 [\[45,](#page-17-0) [46\]](#page-17-1), both focusing on molecule generation. This momentum continued with the introduction of FM-based protein generation models at ICLR 2024 [\[47\]](#page-17-2), followed by further progress in biological sequence and peptide generation at ICML 2024. Beyond these milestones, 2024 and 2025 have seen the emergence

<sup>∗</sup>Equal Contributions.

<sup>⋄</sup>Corresponding authors.

Preprint. Released July 2025.

![](_page_1_Figure_1.jpeg)

Trend of Flow Matching and its Applications in Biology and Life Science Research

<span id="page-1-0"></span>Fig. 2. Trend of published papers on flow matching (FM) and its applications in biology and life sciences across major ML conferences from 2023 to 2025. The blue line indicates the total number of FM papers, while the orange line shows the subset focused on biological applications. Annotations highlight key milestones in FM and its adoption for molecule, sequence, and protein generation, illustrating the rapid growth and expanding interest in this area.

of increasingly specialized FM variants, such as categorical FM [\[48\]](#page-17-3), rectified FM [\[49\]](#page-17-4), and non-Euclidean formulations including Riemannian [\[50\]](#page-17-5) and Dirichlet [\[51\]](#page-17-6) FM. Many of these have begun to find applications in structural biology, molecular conformation modeling, and biomedical imaging. This upward trajectory highlights not only the methodological innovation within FM, but also its growing relevance in life science domains that demand high-dimensional, structure-aware generative modeling.

As both FM and its biological applications evolve, the landscape has become increasingly fragmented, making it difficult to keep track of key developments and emerging trends. This survey addresses this gap by providing the first comprehensive review of FM in the context of biology and life sciences. We begin with a systematic overview of FM methods and variants, and then categorize their biological applications into three core areas: biological sequence modeling, molecule generation and design, and protein generation. We also review auxiliary topics such as bioimage modeling and spatial transcriptomics, summarize commonly used datasets and tools, and conclude with open challenges and future directions. Our goal is to offer an accessible entry point for newcomers, while equipping experienced researchers with a clear map of the field's current trajectory.

## *A. Challenges of Generative Modeling for Biology*

Biological systems are among the most intricate and multifaceted systems in the natural world [\[106,](#page-19-0) [107,](#page-19-1) [108\]](#page-19-2), shaped by billions of years of evolution and governed by deeply

intertwined physical, chemical, and informational processes. Modeling such systems has long been a grand challenge across scientific disciplines, demanding tools that can reconcile precision with flexibility [\[109,](#page-19-3) [110,](#page-19-4) [111,](#page-19-5) [112,](#page-19-6) [112,](#page-19-6) [113,](#page-19-7) [114\]](#page-19-8). The complexity of biological data and phenomena stems from a confluence of factors, with some of the most formidable challenges including: (i) the necessity to embed rich domain knowledge, ranging from physical laws to biochemical constraints, into generative models in a way that ensures structural and functional validity; (ii) the scarcity, incompleteness, and noise characteristic of real-world biological datasets, often resulting from expensive or error-prone experimental procedures; (iii) the inherently multi-scale and multi-modal nature of biological processes, which span atomic interactions to cellular behavior, and integrate diverse data types such as sequences, structures, and spatial-temporal signals; (iv) the increasing demand for controllable and condition-aware generation, where outputs must satisfy explicit biological properties or therapeutic objectives; and (v) the pressing need for models that are not only accurate but also computationally scalable and sample-efficient, especially in applications such as drug discovery or protein design where inference speed can be critical. Together, these challenges make it challenging for biology models.

FM, as a recently introduced generative modeling paradigm, holds strong potential for addressing the unique challenges of biological data. It learns a deterministic vector field to map a simple base distribution directly to complex target data via continuous probability trajectories. This yields several advantages particularly relevant to biological applications, such as faster and more stable sampling, easier conditioning on

![](_page_2_Figure_1.jpeg)

<span id="page-2-0"></span>Fig. 3. Overview of the survey taxonomy. We begin by introducing the foundations of flow matching, including its core models and variants. Our taxonomy then categorizes flow matching applications into major biological domains and tasks.

structured inputs, and the ability to incorporate geometric or physical priors into the modeling process. Since its introduction, a growing number of studies have explored the use of FM in tackling biological tasks. These early successes demonstrate not only the method's versatility but also its capacity to model the structured, multimodal, and constraint-rich nature of biological systems, positioning FM as a compelling alternative to conventional generative frameworks in the life sciences.

# *B. Our contributions*

This survey presents the first comprehensive review of FM and its applications in biology and life sciences. Our key contributions are summarized as follows:

- A Unified Taxonomy of Flow Matching Variants. We introduce a structured taxonomy of FM methodologies, spanning general FM, conditional and rectified FM, non-Euclidean and discrete FM, and hybrid variants.
- In-depth Survey of Biological Applications. We sys-

<span id="page-3-0"></span>Reference Generative Modeling Task Domain Jabbar et al. (2021) [\[115\]](#page-19-23) Generative Adversarial Network General Xia et al. (2022) [\[116\]](#page-19-24) Generative Adversarial Network Anomaly Detection Greener et al. (2022) [\[117\]](#page-19-25) Various ML and Generative Modeling Methods Biology, including protein design and DNA sequence Li et al. (2023) [\[118\]](#page-20-0) Autoencoder General, including Image Classification and NLP Tasks Yang et al. (2023) [\[27\]](#page-16-10) Diffusion Model General, including CV, NLP, Multimodal Tasks Croitoru et al. (2023) [\[119\]](#page-20-1) Diffusion Model Various Tasks in Computer Vision Liang et al. (2024) [\[120\]](#page-20-2) Variational Autoencoder Recommendation Cao et al. (2024) [\[121\]](#page-20-3) Diffusion Model General, including image, video and audio Generation Guo et al. (2024) [\[26\]](#page-16-9) Diffusion Model Biology, including protein, molecular, gene-expression tasks Saad et al. (2024) [\[122\]](#page-20-4) Generative Adversarial Network Biomedical Image Synthesis Tang et al. (2024) [\[123\]](#page-20-5) Various Generative Modeling Methods De Novo Drug Design Du et al. (2024) [\[124\]](#page-20-6) Various Generative Modeling Methods Molecular Design Zhang et al. (2025) [\[125\]](#page-20-7) Large Language Models Biology and Chemistry, e.g., molecular, protein, genomic tasks Ours Flow Matching Various Tasks in Biology and Life Science

TABLE I EXISTING SURVEYS RELATED TO THIS WORK. WE PRESENT THE FIRST COMPREHENSIVE SURVEY DEDICATED TO FLOW MATCHING AND ITS APPLICATIONS IN BIOLOGY AND LIFE SCIENCES.

tematically categorize and review the use of FM across three primary biological domains: *biological sequence modeling*, *molecule generation and design*, and *protein generation*. We further explore several other emerging applications beyond this scope.

- Comprehensive Benchmark and Dataset Survey. We compile and review widely used biological datasets, benchmarks, and software tools adopted in FM research.
- Trend, Challenges, and Emerging Directions. We contextualize the evolution of FM through bibliometric trends and identify key methodological innovations. We further analyze domain-specific modeling challenges which may motivate new FM research directions.
- Bridging Modeling and Biology Communities. By mapping methodological advances in FM to diverse biological challenges, we offer a cross-disciplinary bridge that connects the machine learning community developing FM algorithms with the biological sciences community seeking powerful generative tools.

#### *C. Connection to existing survey*

Existing related surveys can be broadly categorized into three groups. The first category focuses exclusively on generative modeling methodologies. These surveys either provide comprehensive overviews of specific classes of generative models [\[115,](#page-19-23) [118,](#page-20-0) [121\]](#page-20-3) or examine their applications within particular domains, such as computer vision [\[119\]](#page-20-1), recommendation systems [\[120\]](#page-20-2), and anomaly detection [\[116\]](#page-19-24). The second category surveys the use of generative models in biology prior to the advent of FM. For example, [\[124\]](#page-20-6) reviews generative models for molecular design, [\[123\]](#page-20-5) focuses on de novo drug design, and [\[117\]](#page-19-25) provides a broad overview of machine learning methods in both predictive and generative biological modeling. Table [I](#page-3-0) presents a comparison of existing surveys on generative modeling, highlighting their covered model classes and application domains. To the best of our knowledge, this work presents the first comprehensive survey dedicated to FM and its applications in biology and life sciences. By bridging recent developments in generative modeling with their emerging

applications in biological domains, this survey aims to fill a critical gap in the literature.

#### *D. Outline of the survey*

To provide a comprehensive understanding of FM in the context of biology and life sciences, this survey is organized into several key sections. We begin by introducing the fundamental concepts and methodologies underlying FM in Section [III,](#page-5-0) establishing a foundation for its application in biological contexts. Next, in Section [IV,](#page-7-0) we delve into specific areas of application, starting with biology sequence generation, followed by molecule generation and design in Section [V,](#page-7-1) and then peptide and protein generation in Section [VI,](#page-9-0) each highlighting recent advancements and representative studies. In section [VII,](#page-11-0) we also discuss other emerging applications of FM in biology. Finally, we conclude by outlining future research directions and potential challenges, aiming to inspire further exploration and innovation in this rapidly evolving field. Figure [3](#page-2-0) presents the overall structure of this survey, with each section divided into various subtopics for a more detailed exploration.

#### II. BACKGROUND

Generative modeling seeks to learn a probability distribution pdata(x) from a dataset of examples {xi} N <sup>i</sup>=1, such that we can generate new samples xˆ ∼ pθ(x) that resemble real data. These models underpin advances in biology tasks ranging from molecular generation to protein design and cellular imaging [\[123,](#page-20-5) [124,](#page-20-6) [126,](#page-20-8) [127,](#page-20-9) [128\]](#page-20-10), with AlphaFold [\[11,](#page-15-10) [12,](#page-15-11) [129\]](#page-20-11) standing out as one of the most prominent and transformative examples, recognized with the Nobel Prize in 2024. AlphaFold leverages deep generative principles to predict protein 3D structures directly from amino acid sequences, a task that had challenged the field for decades [\[13,](#page-15-12) [114,](#page-19-8) [130\]](#page-20-12). By effectively modeling the conditional distribution over protein conformations, AlphaFold not only revolutionized protein structure prediction but also highlighted the broader potential of generative models to capture complex, structured biological phenomena at scale. In biology domains, data is often highdimensional, multimodal, and governed by physical or biochemical constraints [\[131,](#page-20-13) [132,](#page-20-14) [133,](#page-20-15) [134\]](#page-20-16), requiring generative

<span id="page-4-0"></span>TABLE II NOTATION USED IN GENERATIVE MODELING PARADIGMS.

| Symbol   | Description                         |  |
|----------|-------------------------------------|--|
| x        | Data sample                         |  |
| z        | Latent variable                     |  |
| pdata(x) | True data distribution              |  |
| pθ(x)    | Model distribution                  |  |
| fθ       | Invertible function (flow)          |  |
| uθ(x, t) | Velocity field in FM                |  |
| pt(x)    | Intermediate distribution at time t |  |
| ϵ        | Noise in diffusion model            |  |
| LFM      | Flow Matching loss                  |  |
| LDM      | Diffusion model loss                |  |

models to strike a careful balance between validity, diversity, and interpretability. In this section, we provide a brief overview of the major paradigms in generative modeling, with the goal of establishing a conceptual and mathematical foundation for understanding more recent developments such as FM. For clarity and consistency, all symbols used throughout this paper are summarized in Table [II.](#page-4-0)

### *A. Variational Autoencoder (VAE)*

Variational Autoencoders (VAEs) [\[135,](#page-20-17) [136,](#page-20-18) [137,](#page-20-19) [138,](#page-20-20) [139\]](#page-20-21) are a class of latent-variable generative models that aim to model the data distribution pdata(x) through a learned probabilistic decoder pθ(x|z), where z is a latent variable drawn from a prior p(z), typically a standard Gaussian. Since the true posterior p(z|x) is often intractable, VAEs introduce an approximate posterior qϕ(z|x), known as the encoder, and optimize the model using variational inference. The training objective is to maximize a variational lower bound, known as the evidence lower bound (ELBO), on the marginal loglikelihood of the data:

$$
\log p_{\theta}(x) \geq \mathbb{E}_{q_{\phi}(z|x)} [\log p_{\theta}(x|z)] - \mathrm{KL}\left(q_{\phi}(z|x) \| p(z)\right)
$$

The first term encourages accurate reconstruction of the input data from the latent variable z, while the second term regularizes the approximate posterior to stay close to the prior distribution. During training, the reparameterization trick is used to allow gradients to backpropagate through the sampling process, typically by expressing z ∼ qϕ(z|x) as z = µ(x)+σ(x)⊙ϵ, where ϵ ∼ N (0, I). However, VAEs often suffer from over-regularization and produce blurred outputs, especially in high-dimensional domains such as images and molecular graphs [\[140,](#page-20-22) [141,](#page-20-23) [142\]](#page-20-24).

## *B. Generative Adversarial Network (GAN)*

Generative Adversarial Networks (GANs) [\[18\]](#page-16-1) are a class of implicit generative models that learn to generate realistic data by playing a two-player minimax game between two neural networks: a generator G<sup>θ</sup> and a discriminator Dϕ. The generator maps noise samples z ∼ p(z), typically drawn from a simple prior such as a Gaussian, into synthetic data samples Gθ(z). The discriminator attempts to distinguish between real samples x ∼ pdata and generated samples Gθ(z). The original GAN objective is formulated as:

$$
\min_{G_{\theta}} \max_{D_{\phi}} \mathbb{E}_{x \sim p_{data}}[\log D_{\phi}(x)] + \mathbb{E}_{z \sim p(z)}[\log(1 - D_{\phi}(G_{\theta}(z)))]
$$

GANs are known to suffer from several practical challenges, including training instability, sensitivity to hyperparameters, and mode collapse Numerous variants have been proposed to improve training dynamics and sample diversity, such as Wasserstein GANs [\[143\]](#page-20-25), Least-Squares GANs [\[144\]](#page-20-26), and conditional GANs [\[145\]](#page-20-27). In biological applications, GANs have been used for generating realistic cell images [\[146\]](#page-20-28), synthesizing gene expression profiles [\[20,](#page-16-3) [147\]](#page-20-29), and augmenting scarce datasets [\[148\]](#page-21-0). Despite their limitations, their ability to capture complex data distributions without explicit density estimation makes them a compelling choice for modeling highdimensional biological data [\[149\]](#page-21-1).

#### *C. Flow-Based Model*

Flow-based models (also known as normalizing flows) [\[150,](#page-21-2) [151\]](#page-21-3) are a family of generative models that construct complex data distributions by applying a sequence of invertible transformations to a simple base distribution, typically a standard Gaussian distribution. Given a base variable z ∼ pz(z), a flow model learns an invertible mapping x = fθ(z) such that the model distribution pθ(x) can be computed exactly via the change-of-variables formula:

$$
\log p_{\theta}(x) = \log p_{z}(f_{\theta}^{-1}(x)) + \log \left| \det \left( \frac{\partial f_{\theta}^{-1}(x)}{\partial x} \right) \right|
$$

The goal is to train the parameters θ to maximize the log-likelihood of the observed data under this model. The invertibility of f<sup>θ</sup> allows for exact and tractable likelihood computation, efficient sampling, and deterministic inference. To ensure both tractability and expressivity, flow models are often constructed as a composition of multiple simple bijective transformations:

<span id="page-4-1"></span>
$$
f_{\theta} = f_K \circ f_{K-1} \circ \cdots \circ f_1 \tag{1}
$$

Each component f<sup>k</sup> is designed to allow efficient computation of the Jacobian determinant and its inverse. Representative architectures include NICE [\[152\]](#page-21-4), RealNVP [\[153\]](#page-21-5), Glow [\[154\]](#page-21-6), and Masked Autoregressive Flows (MAF) [\[155\]](#page-21-7), which utilize affine coupling layers or autoregressive transforms to maintain invertibility.

However, the invertible constraint on f<sup>θ</sup> along with the need to compute the determinant of the Jacobian ∂fθ(x) ∂x imposes significant constraints on model expressiveness and design flexibility. Continuous normalizing flow (CNF) [\[156\]](#page-21-8) address these limitations by replacing the discrete sequence of transformations (Eq. [\(1\)](#page-4-1)) with a continuous-time dynamic system dx dt = f(x(t), t). This formulation leads to a more efficient computation of the log-density change:

$$
\frac{\partial \log p(x(t))}{\partial t} = -\text{Tr}\left(\frac{df}{dx(t)}\right)
$$

Notably, the vector field f is not required to be invertible.

CNFs serve as a foundational building block for FM. While CNFs allow for more expressive modeling, their training via maximum likelihood still demands computationally expensive ODE solvers. A core motivation behind flow matching is to simplify the training of ODE-based generative models, without sacrificing the benefits of continuous-time formulations.

#### *D. Diffusion Models (DM)*

Diffusion models [\[25,](#page-16-8) [157,](#page-21-9) [158,](#page-21-10) [159,](#page-21-11) [160\]](#page-21-12) are a family of likelihood-based generative models that generate data by reversing a gradual noising process. They define a forward process that incrementally transforms data into noise, and parameterize a neural network to fit the groundtruth reverse process, recovering data from noise step by step.

*a) Forward Process:* The forward process defines a sequence of latent variables {xt} T <sup>t</sup>=0, which are the gradually corrupted version of the clean data x<sup>0</sup> ∼ pdata. A typical forward process is formulated as a set of Gaussian distributions conditioned on the previous step:

$$
q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)
$$

where {βt} is called noise schedule. Usually, the distribution of the corrupted data at any time t has a closed form:

$$
q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t)I),
$$
$$
\bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s)
$$

*b) Training:* Similar to many likelihood-based models, negative log-likelihood is a canonical choice of the loss function [\[25,](#page-16-8) [157,](#page-21-9) [161\]](#page-21-13). Beyond that, cross-entropy or square error are also widely used [\[25,](#page-16-8) [162\]](#page-21-14). Based on that, neural networks (NNs) are used to parameterize various components of the diffusion process, such as to predict the data [\[163\]](#page-21-15), predict the noise [\[25\]](#page-16-8), and predict the score [\[160\]](#page-21-12). The following unweighted regression loss for predicting the noise is a popular example:

$$
\mathcal{L}_{DM} = \mathbb{E}_{x_0, t, \epsilon} \left[ \left\| \epsilon - \epsilon_{\theta}(x_t, t) \right\|^2 \right]
$$
$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \ \epsilon \sim \mathcal{N}(0, I)
$$

*c) Generation:* Equipped with the NN-parameterized component, the reverse process of the diffusion process is used for generation. For example, the reverse process with the NN-predicted noise ϵ<sup>θ</sup> can denoise the Gaussian noise x<sup>T</sup> ∼ N (0, I) gradually:

$$
x_{t-1} = \frac{1}{\sqrt{1 - \beta_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_{\theta}(x_t, t) \right) + noise
$$

A well-known limitation of diffusion models is their slow sampling process, which often requires hundreds of iterative steps. To address this inefficiency, several acceleration techniques have been proposed, including the adoption of tailored numerical solvers [\[164\]](#page-22-0), model distillation [\[163\]](#page-21-15), and continuous-time formulations [\[160,](#page-21-12) [161\]](#page-21-13). Notably, Probability flow ODE [\[159\]](#page-21-11) and DDIM [\[160\]](#page-21-12) demonstrate that there exists a deterministic ODE whose solution shares the same marginal distributions as the reverse-time stochastic differential equation (SDE) used in diffusion models. This observation is conceptually aligned with the idea behind flow matching (FM), although both probability flow ODE and DDIM remain trained using the standard loss functions of diffusion models, such as the evidence lower bound (ELBO).

### III. FLOW-MATCHING BASICS

<span id="page-5-0"></span>In this section, we provide background knowledge on flowmatching (FM) models, including general FM and discrete FM.

## *A. General Flow-Matching*

Flow-matching is a continuous-time generative framework that generalizes diffusion models by *regressing a vector field that transports one distribution into another* [\[54\]](#page-17-9). In general, FM aims to construct a velocity field uθ(x, t) to transport a source p<sup>0</sup> to a target p<sup>1</sup> via the continuity equation:

$$
\frac{\partial p_t}{\partial t} + \nabla \cdot (p_t u_\theta(x, t)) = 0.
$$

An FM can be trained by minimizing the squared loss between the neural velocity field uθ(x, t) and a reference velocity field u ∗ t (x, t) as follows

<span id="page-5-1"></span>
$$
\mathcal{L}_{FM} = \mathbb{E}_{t \sim [0,1], x_t \sim p_t(x)} \|u^*(x_t, t) - u_\theta(x_t, t)\|^2.
$$
 (2)

Promising as it might be, directly optimizing the objective in Eq. [\(2\)](#page-5-1) is impractical: the optimal velocity field u ∗ (x, t) encodes a highly complex joint transformation between two highdimensional distributions [\[165\]](#page-22-1). To overcome this challenge, conditional FM variants have been introduced to enable more tractable training (Paragraph [III-A0a\)](#page-5-2). Concurrently, rectified FM methods propose improved noise couplings along the straight-line probability path (Paragraph [III-A0b\)](#page-6-0). Finally, non-Euclidean FM extensions generalize the framework from flat Euclidean space to curved manifolds, accommodating data with intrinsic geometric structure (Paragraph [III-A0c\)](#page-6-1).

<span id="page-5-2"></span>*a) Conditional FM [\[52,](#page-17-7) [53,](#page-17-8) [54,](#page-17-9) [55\]](#page-17-10):* To resolve the intractable u ∗ (x, t), conditional FM introduces a conditioning variable z, e.g., class label, and define a conditional path p(x|t, z) such that the induced global path p(x|t) = R z p(x|t, z)p(z)dz transforms p<sup>0</sup> to pdata and the corresponding conditional velocity field has analytical form. A conditional FM can be trained by minimizing the quadratic loss between the neural velocity field uθ(x, t) and the conditional velocity field u ∗ t (x, t, z) as follows

<span id="page-5-3"></span>
$$
\mathbb{E}_{t \sim [0,1], x_t \sim p_t(x|z), z \sim p_z} ||u^*(x_t, t, z) - u_\theta(x_t, t)||^2.
$$
 (3)

The training procedure involves sampling a conditioning variable z, e.g., via linear interpolation [\[52,](#page-17-7) [166\]](#page-22-2) or Gaussian path [\[54\]](#page-17-9), and random time t, constructing x<sup>t</sup> along the prescribed path, and minimizing the corresponding loss. Once the model is trained, the sampling/generation process is done by solving the learned ODE dx/dt = uθ(x, t) using an ODE solver from t = 0 (noise) to t = 1 (data). The key theoretical foundation of conditional FM is that the gradient of the FM objective in Eq. [\(2\)](#page-5-1) is equivalent to gradient of the CFM objective in Eq. [\(3\)](#page-5-3). Building upon the conditioning variable z, one can define velocity field in analytical forms with tractable training.

TABLE III COMPREHENSIVE COMPARISON OF MAJOR GENERATIVE MODELING PARADIGMS.

| Model Type    | Training Objective | Number of Function<br>Evaluations | Structured Data Support              |
|---------------|--------------------|-----------------------------------|--------------------------------------|
| VAE           | Likelihood         | Low                               | Moderate (via extensions)            |
| GAN           | Adversarial loss   | Low                               | Weak (limited geometry)              |
| Diffusion     | Likelihood         | SDE solver-dependent              | Strong (SE(3), graph diffusion)      |
| Flow-Based    | Likelihood         | Low                               | Moderate (design-dependent)          |
| Flow Matching | Velocity matching  | ODE solver-dependent              | Strong (geometry-aware, equivariant) |

<span id="page-6-0"></span>*b) Rectified FM [\[49,](#page-17-4) [55,](#page-17-10) [56,](#page-17-11) [57,](#page-17-12) [166\]](#page-22-2):* Infinite probability path exist between source and target distributions that can be leveraged by conditional FM, rectified FM prefers the linear transport trajectory that best connect two distributions. [\[166\]](#page-22-2) proposes to train a velocity field carrying each sample x<sup>0</sup> to its paired target x<sup>1</sup> along nearly-straight lines via

$$
\mathbb{E}_{(x_0,x_1)\sim\pi}\int_0^1\|u_\theta(x_t,t)-(x_1-x_0)\|^2dt
$$

where pi is a coupling of p<sup>0</sup> and p1. It is shown that the optimal transport (OT) coupling provides a straight coupling for p<sup>0</sup> and p1, simplifying the flow and reducing curliness [\[55,](#page-17-10) [57\]](#page-17-12).

<span id="page-6-1"></span>*c) Non-Euclidean FM [\[50,](#page-17-5) [58,](#page-17-13) [60,](#page-17-15) [61,](#page-17-16) [62\]](#page-17-17):* Non-Euclidean flows extend continuous flows to curved data spaces. For example, [\[62\]](#page-17-17) introduce Riemannian Continuous Normalizing Flows, defining the generative flow by an ODE on the manifold to model flexible densities on spheres, tori, hyperbolic spaces, etc.. [\[61\]](#page-17-16) propose Neural Manifold ODEs, integrating dynamics chart-by-chart (e.g. via local coordinate charts) so that the learned velocity field stays tangent to the manifold. More recently, [\[58\]](#page-17-13) propose Riemannian FM by using geodesic distances as a "premetric" they derive a closedform target vector field pushing a base distribution to the data without any stochastic diffusion or divergence term. On simple manifolds (e.g. spheres or hyperbolic space where geodesics are known) Riemannian FM is completely simulation-free, and even on general geometries it only requires solving a single ODE without calculating expensive score or density estimates. [\[60\]](#page-17-15) introduce Fisher FM, treating categorical distributions as points on the probability simplex with the Fisher–Rao metric and transporting them along spherical geodesics. In general, Riemannian flows replace straight-line interpolations with intrinsic geodesics and explicitly account for the manifold's metric (e.g. via the Riemannian divergence in the change-ofdensity). These works tackle the challenges of defining tangent vector fields and volume corrections on curved spaces via chart-based integration, metric-adjusted log-density formulas, or flow-matching losses that avoid divergence estimates. Overall, they enable scalable generative modeling on curved domains (spheres, Lie groups, statistical manifolds, etc.), respecting curvature in ways standard Euclidean FM cannot.

### *B. Discrete Flow-Matching*

Discrete FM has emerged as a powerful paradigm for generative modeling over discrete data domains, such as sequences, graphs, and categorical structures, covering a wide range of biological objects [\[4,](#page-15-3) [162\]](#page-21-14). By extending the principles of continuous FM to discrete spaces, DFM enables the design of efficient, non-autoregressive generative models. This section delves into two principal frameworks: Continuous-Time Markov Chain (CTMC)-based methods (Paragraph [III-B0a\)](#page-6-2) and simplex-based methods (Paragraph [III-B0b\)](#page-6-3).

<span id="page-6-2"></span>*a) Continuous-Time Markov Chain (CTMC):* CTMCbased approaches model the generative process as a continuoustime stochastic evolution over discrete states, leveraging the mathematical framework of continuous-time Markov chains to define and learn probability flows. [\[63\]](#page-17-18) utilizes CTMCs to model flows over discrete state spaces. This approach allows for the integration of discrete and continuous data, facilitating applications like protein co-design by enabling multimodal generative modeling. Fisher Flow [\[60\]](#page-17-15) adopts a geometric perspective by considering categorical distributions as points on a statistical manifold endowed with the Fisher-Rao metric. This approach leads to optimal gradient flows that minimize the forward Kullback-Leibler divergence, improving the quality of generated discrete data. [\[65\]](#page-17-20) expanded the design space of discrete generative models by allowing arbitrary discrete probability paths within the CTMC framework. This holistic approach enables the use of diverse corruption processes, providing greater flexibility in modeling complex discrete data distributions. DeFog [\[64\]](#page-17-19) is a discrete FM framework tailored for graph generation. By employing a CTMC-based approach, DeFoG achieves efficient training and sampling, outperforming existing diffusion models in generating realistic graph.

<span id="page-6-3"></span>*b) Simplex-based discrete FM:* Simplex-based methods operate within the probability simplex, modeling flows over continuous relaxations of discrete distributions. These approaches often employ differentiable approximations to handle the challenges posed by discrete data. SimplexFlow [\[167\]](#page-22-3) combines continuous and categorical flow matching for 3D de novo molecule generation, where intermediate states are guaranteed to reside on the simplex. Dirichlet FM [\[51\]](#page-17-6) utilizes mixtures of Dirichlet distributions to define probability paths over the simplex, addressing discontinuities in training targets and enables efficient. α-flow [\[59\]](#page-17-14) unifies various continuousstate discrete FM models under the lens of information geometry. By operating on different α-representations of probabilities, this framework optimizes the generalized kinetic energy, enhancing performance in tasks such as image and protein sequence generation. STGFlow [\[67\]](#page-17-22) employs a Gumbel-Softmax interpolant with a time-dependent temperature for controllable biological sequence generation, which includes a classifier-based guidance mechanism that enhances the quality and controllability of generated sequences.

#### IV. SEQUENCE MODELING

<span id="page-7-0"></span>FM has emerged as a powerful framework for biological sequence generation, offering deterministic and controllable modeling of discrete structures such as DNA, RNA, and wholegenome data. In this section, we survey different FM models designed for biological sequence generation, including DNA sequence, RNA sequence, whole-genome modeling, and antibody design. By leveraging continuous transformations, flow matching enables efficient generation of sequences conditioned on various biological constraints and properties.

#### *A. DNA Sequence Generation*

Early deep generative models, e.g. GANs or autoregressive models, struggled to satisfy the complex constraints of functional genomics sequences. FM models provide natural solutions to bridge this gap by mapping discrete nucleotide sequences into continuous probabilistic spaces for training [\[51\]](#page-17-6). Instead of simulating a stochastic diffusion [\[51\]](#page-17-6), FM models directly train a continuous vector field that transports a simple base distribution, e.g., uniform distribution over nucleotides, into the empirical DNA data distribution.

Fisher-Flow [\[60\]](#page-17-15) introduces a geometry-based flow matching approach, which treats discrete DNA sequences as points on a statistical manifold endowed with the Fisher–Rao metric. By allowing for continuous reparameterization of discrete data, probability mass is transported along optimal geometric paths on the positive orthant of a hypersphere, achieving state-ofthe-art performance on DNA promoter and enhancer sequence generation benchmarks compared to earlier diffusion-based and flow-based models.

Besides categorical distribution, Dirichlet distribution is adopted to handle discrete sequences. Dirichlet Flow [\[51\]](#page-17-6) utilizes mixtures of Dirichlet distributions to define probability paths on the simplex, addressing discontinuities and pathologies in naive linear flow matching. Dirichlet Flow enables one-step DNA sequence generation and achieves superior distributional metrics and target-specific design performance compared to prior models on complex DNA design tasks.

In addition, STGFlow [\[67\]](#page-17-22) proposes straight-through guidance, combining Gumbel-Softmax flows with classifier-based guidance to steer the generation process toward desired sequence properties, facilitating controllable de novo DNA sequence generation.

#### *B. RNA Sequence Generation*

Flow matching has recently been applied to RNA sequence and structure design. Rather than focusing solely on sequence generation, existing FM methods prioritize structural fidelity, enabling advanced applications in inverse folding, protein-conditioned design, and ensemble backbone sampling. RNACG [\[68\]](#page-17-23) introduces a versatile flow-matching framework for conditional RNA generation that supports tasks ranging from 3D inverse folding to translation efficiency prediction. RNAFlow [\[69\]](#page-17-24) couples an RNA inverse-folding module with a pretrained structure predictor to co-generate RNA sequences and their folded structures in the context

of bound proteins. RiboGen [\[70\]](#page-17-25) develops the first deep

network to jointly synthesize RNA sequences and all-atom 3D conformations via equivariant multi-flow architectures. Most recently, RNAbpFlow [\[168\]](#page-22-4) presents a SE(3)-equivariant flowmatching model that conditions on both sequence and base-pair information to sample diverse RNA backbone ensembles.

## *C. Whole-Genome Modeling*

At the whole-genome level, flow matching has been applied to model single-cell genomics data. GENOT [\[71\]](#page-18-0) employs entropic Gromov-Wasserstein flow matching to learn mappings between cellular states in single-cell transcriptomics, facilitating studies of cell development and drug response. cellFlow [\[72\]](#page-18-1) proposes a generative flow-based model for single-cell count data that operates directly in raw transcription count space, preserving the discrete nature of the data. CFGen [\[73\]](#page-18-2) introduces a flow-based conditional generative model capable of generating multi-modal and multi-attribute single-cell data, addressing tasks such as rare cell type augmentation and batch correction.

# *D. Antibody Sequence Generation*

FM has also been utilized for antibody sequence generation. IgFlow [\[74\]](#page-18-3) proposes a SE(3)-equivariant FM model for de novo antibody variable region generation (heavy/light chains and CDR loops). IgFlow supports unconditional antibody sequence-structure generation and conditional CDR loop inpainting, producing structures comparable to those from a diffusion-based model while achieving higher self-consistency in conditional designs; it also offers efficiency benefits like faster inference and better sample efficiency than the diffusion counterpart. dyAb [\[75\]](#page-18-4) proposes a flexible antibody design FM, which integrates coarse-grained antigen–antibody interface alignment with fine-grained flow matching on both sequences and structures. By explicitly modeling antigen conformational changes (via AlphaFold2 predictions) before binding, dyAb significantly improves the design of high-affinity antibodies in cases where target antigens undergo dynamic structural shifts.

These advancements demonstrate the versatility of flow matching in modeling complex biological sequences and structures, providing a unified framework for deterministic and controllable generation across various biological domains.

## V. MOLECULE GENERATION

<span id="page-7-1"></span>Molecule generation is a fundamental task in biological modeling, playing a crucial role in drug discovery, material design, and understanding molecular interactions [\[169,](#page-22-5) [170,](#page-22-6) [171\]](#page-22-7). The ability to generate novel molecules with desired properties has significant implications for both theoretical and applied research in life sciences [\[172,](#page-22-8) [173\]](#page-22-9). Traditional approaches, such as rule-based simulations and heuristic algorithms, often face challenges in scalability and diversity [\[174,](#page-22-10) [175\]](#page-22-11). In contrast, generative models, including flow matching, offer a data-driven approach to efficiently explore the vast chemical space [\[26,](#page-16-9) [176,](#page-22-12) [177\]](#page-22-13).

In this section, we review recent advancements in molecule generation using flow matching techniques. We focus on methods that leverage continuous probability flow trajectories to generate novel molecular structures and properties, highlighting how flow matching has enhanced molecule generation.

#### *A. 2D Molecule Generation*

![](_page_8_Figure_3.jpeg)

Fig. 4. 2D graph representations of example molecules generated from the GEOM-Drugs [\[178\]](#page-22-14) (left two) and QM9 [\[179\]](#page-22-15) (right two) datasets. Each molecule is visualized as a 2D graph, where atoms are nodes and chemical bonds are edges, capturing both structural and topological properties.

Although real-world molecules are inherently threedimensional objects, researchers often simplify the problem by using 2D graph-based molecular modeling when the 3D structure is not the primary focus [\[180,](#page-22-16) [181,](#page-22-17) [182\]](#page-22-18). This approach offers several advantages, including increased computational efficiency and reduced information requirements during inference.

Flow matching on graph data remains relatively unexplored, as the concept of flow matching itself is still under development. Nevertheless, existing studies often use 2D molecule generation as a preliminary test case to evaluate newly proposed flow matching variants. For instance, Eijkelboom et al. [\[77\]](#page-18-6) combine flow matching with variational inference to introduce Variational Flow Matching for graph generation and CatFlow for handling categorical data. Additionally, GGFlow [\[76\]](#page-18-5) presents a discrete flow matching generative model that integrates optimal transport for molecular graphs. This model features an edgeaugmented graph transformer, enabling direct communication among chemical bonds, thereby improving the representation of molecular structures. DeFoG [\[78\]](#page-18-7) introduces a discrete formulation of flow matching tailored to the graph domain, explicitly decoupling the training and sampling phases to overcome inefficiencies in traditional diffusion-based models. By leveraging permutation-invariant graph matching objectives and exploring a broader sampling design space, DeFoG achieves strong empirical results on molecular graph generation with significantly fewer refinement steps.

#### *B. 3D Molecule Generation*

![](_page_8_Figure_8.jpeg)

Fig. 5. 3D graph representations of example molecules generated from the GEOM-Drugs [\[178\]](#page-22-14) (left two) and QM9 [\[179\]](#page-22-15) (right two) datasets. Atoms are shown as nodes positioned in 3D Euclidean space, and bonds are represented as edges connecting them. These visualizations capture spatial geometry and stereochemistry important for molecular property prediction.

Generating accurate 3D molecular structures is a critical task in drug discovery and structural biology [\[183\]](#page-22-19). Unlike 2D graphbased approaches, which primarily capture atomic connectivity, 3D molecular representations inherently encode spatial information, including bond angles, torsions, and stereochemistry. This spatial fidelity is essential for modeling interactions such as molecular docking, binding affinity, and conformational stability. While 2D representations cannot distinguish between stereoisomers or capture geometric nuances, 3D methods accurately model spatial conformation, enabling a more precise understanding of molecular properties [\[169,](#page-22-5) [184,](#page-22-20) [185\]](#page-22-21).

*a) SE(3)-equivariant:* To ensure physically meaningful and symmetry-consistent outputs, recent advancements have incorporated SE(3)-equivariant neural architectures into flow matching models. These models leverage the inherent symmetries of molecular systems, modeling graph generation as a continuous normalizing flow over node and edge features. For instance, Megalodon [\[80\]](#page-18-9) introduces scalable transformer models with basic equivariant layers, trained using a hybrid denoising objective to generate 3D molecules efficiently, achieving state-of-the-art results in both structure generation and energy benchmarks. EquiFM [\[45\]](#page-17-0) further improves the generation of 3D molecules by combining hybrid probability transport with optimal transport regularization, significantly speeding up sampling while maintaining stability. EquiFlow [\[79\]](#page-18-8) addresses the challenge of conformation prediction using conditional flow matching and an ODE solver for fast and accurate inference. By leveraging equivariant modeling, these methods improve the generation of valid and physically consistent molecular conformations, advancing the field of 3D molecule generation. Equivariant Variational Flow Matching [\[81\]](#page-18-10) frames flow matching as a variational inference problem and enables both end-to-end conditional generation and posthoc controlled sampling without retraining. The model further provides a principled equivariant formulation of VFM, ensuring invariance to rotations, translations, and atom permutations, which are essential for molecular applications.

*b) Efficiency:* Generating high-quality 3D molecular structures efficiently is a major challenge in drug discovery and structural biology. While generative models have shown promise in modeling complex molecular structures, many existing approaches suffer from slow sampling speeds and computational inefficiency. Flow matching-based methods leverage optimal transport and equivariant architectures to achieve faster and more reliable generation. For instance, GOAT [\[82\]](#page-18-11) formulates a geometric optimal transport objective to map multi-modal molecular features efficiently, using an equivariant representation space to achieve a double speedup compared to previous methods. MolFlow [\[83\]](#page-18-12) introduces scale optimal transport, significantly reducing sampling steps while maintaining high chemical validity. SemlaFlow [\[84\]](#page-18-13) combines latent attention with equivariant flow matching, achieving an order-of-magnitude speedup with as few as 20 sampling steps. A recent work introduces SO(3)-Averaged Flow Matching with Reflow [\[85\]](#page-18-14), targeting both training and inference efficiency for 3D molecular conformer generation. The proposed SO(3)-averaged training objective leads to faster convergence and improved generalization compared to Kabschaligned or optimal transport baselines. ET-Flow [\[86\]](#page-18-15) leverages equivariant flow matching to generate low-energy molecular conformations efficiently, bypassing the need for complex geometric calculations.

*c) Guided Generation:* Guided and conditional enables the creation of structures that align with specific biological properties or conditions. In the context of flow matching, guided generation incorporates domain-specific knowledge to steer the generative process, while conditional generation aims to produce diverse outputs based on given inputs or contexts. These approaches are especially valuable in applications where accurate constraints are available. Recent advancements in flow matching have introduced several methods to enhance guided and conditional generation. FlowDPO [\[87\]](#page-18-16) addresses the challenge of 3D structure prediction by combining flow matching with Direct Preference Optimization (DPO), minimizing hallucinations while producing high-fidelity atomic structures. In conditional generation, Extended Flow Matching (EFM) [\[88\]](#page-18-17) generalizes the continuity equation, enabling more flexible modeling by incorporating inductive biases. For mixedtype molecular data, FlowMol [\[89\]](#page-18-18) extends flow matching to handle both continuous and categorical variables, achieving robust performance in 3D de novo molecule generation. 3D energy-based flow matching [\[90\]](#page-18-19) further enhances conditional generation by explicitly incorporating energy signals into both training and inference, improving structural plausibility and convergence. Together, these advances highlight the growing adaptability of flow-based approaches in generating biologically meaningful 3D molecular structures under domain constraints. Additionally, OC-Flow [\[91\]](#page-18-20) leverages optimal control theory to guide flow matching without retraining, showing superior efficiency on complex geometric data, including protein design.

#### *C. Conditional Molecule Design and Applications*

Recent advancements in flow matching for property-driven molecule design focus on not only generating the molecules themselves, but also predicting potential functionalities of the generated molecules. In scenarios requiring precise geometric control, GeoRCG [\[93\]](#page-19-10) enhances molecule generation by integrating geometric representation conditions, achieving significant quality improvements on challenging benchmarks. Additionally, conditional generation with improved structural plausibility has been addressed by integrating distorted molecules into training datasets, as demonstrated in Improving Structural Plausibility in 3D Molecule Generation [\[94\]](#page-19-11). This method leverages property-conditioned training to selectively generate high-quality conformations. Stiefel Flow Matching [\[95\]](#page-19-12) tackles the problem of structure elucidation under moment constraints by embedding molecular point clouds within the Stiefel manifold, allowing for efficient and accurate generation of 3D structures with precise physical properties. Finally, IDFlow [\[186\]](#page-23-0) adopts an energy-based perspective on flow matching for molecular docking, where the generative process learns a deep mapping function to transform random molecular conformations into physically plausible protein-ligand binding structures.

Structure-Based Drug Design (SBDD) is a key task in AIassisted drug discovery, aiming to design small-molecule drugs

that can bind to a given protein pocket structure. The main challenges in this domain lie in modeling the target protein structure, capturing protein–ligand interactions, enabling multimodal generation, and ensuring the chemical validity of generated molecules. In recent years, generative models have shown great potential in addressing these challenges, with Flow Matching (FM) models demonstrating unique advantages in multimodal modeling and generation efficiency. MolFORM [\[187\]](#page-23-1) applies multimodal FM to the SBDD setting and employs DPO to optimize molecular binding affinity. FlexSBDD [\[92\]](#page-19-9) further introduces protein pocket flexibility into the model, making it more reflective of real-world binding scenarios. In addition, MolCRAFT [\[188\]](#page-23-2) adopts a Bayesian Flow Network (BFN) to model multimodal distributions in continuous parameter space, where BFN similarly defines a flow distribution. Moreover, [\[189\]](#page-23-3) reveals the equivalence between BFN, diffusion models, and stochastic differential equations (SDEs). PocketXMol [\[190\]](#page-23-4) provides a unified generative model for handling a variety of protein–ligand tasks.

#### VI. PROTEIN GENERATION

### <span id="page-9-0"></span>*A. Unconditional Generation*

*a) Backbone Generation:* Protein backbone generation aims to rapidly synthesize physically realizable 3D scaffolds that are diverse, designable, and functionally conditionable, while adhering to SE(3)-equivariance, local bond constraints, and global topological consistency. Recent efforts approach this challenge from two directions: enhancing the flow matching framework and improving protein feature representation learning. From the flow matching perspective, FrameFlow [\[191\]](#page-23-5) accelerates diffusion by reframing it as deterministic SE(3) flow matching, cutting sampling steps five-fold and doubling designability over FrameDiff. FoldFlow-SFM [\[192\]](#page-23-6) further extends this by introducing stochastic flows on SE(3) manifolds using Riemannian optimal transport, enabling the rapid generation of long backbones (up to 300 residues) with high novelty and diversity. Complementarily, recent work also advances architectural designs for protein representation learning. Yang et al. [\[193\]](#page-23-7) combine global Invariant Point Attention (IPA) with local neighborhood aggregation to extract meaningful features, and further use ESMFold and AlphaFold3 to filter the invalid generated backbones. Wagner, Simon, et al. [\[194\]](#page-23-8) proposes Clifford frame attention (CFA), an extension of IPA by exploiting projective geometric algebra and higherorder message passing to capture residue-frame interactions, yielding highly designable proteins with richer fold topologies. FoldFlow-2 [\[195\]](#page-23-9) augments SE(3) flows with PLM embeddings and a multi-modal fusion trunk, enabling sequence-conditioned generation with reinforced reward alignment and state-ofthe-art diversity, novelty, and designability on million-scale synthetic–real datasets. Proteina [\[196\]](#page-23-10) scales unconditional FM to a 400 M-parameter non-equivariant transformer trained on 21 M synthetic backbones, using hierarchical CATH conditioning to transport isotropic noise to native-like C<sup>α</sup> traces. ProtComposer [\[197\]](#page-23-11) augments a Multiflow [\[63\]](#page-17-18) backbone with SE(3)-invariant cross-attention to user-sketched 3-D ellipsoid tokens, steering the FM vector field toward compositional spatial layouts while preserving unconditional diversity.

TABLE IV

COMPARISON OF MAJOR PROTEIN MODELING TASKS. WE HIGHLIGHT THE DISTINCTIONS IN INPUT, OUTPUT, OBJECTIVE, AND REPRESENTATIVE METHODS.

| Task                         | Input                                     | Output                                     | Objective                                                                    |
|------------------------------|-------------------------------------------|--------------------------------------------|------------------------------------------------------------------------------|
| Protein Structure Prediction | Amino acid sequence                       | Full 3D structure (backbone + side chains) | Predict natural folded conformation                                          |
| Protein Design               | Target structure or functional constraint | Amino acid sequence (or full structure)    | Design a sequence that folds into a desired structure or achieves a function |
| Protein Backbone Generation  | Partial structure, constraints, or motifs | Backbone atomic coordinates (N, Cα, C)     | Generate realistic backbone conformations as design templates                |

*b) Co-design Generation:* Recent work reframes sequence–structure co-design as learning a unified vector field that jointly models discrete amino acid identities and continuous 3D coordinates, bypassing the traditional two-stage pipeline that separately samples a backbone before fitting a compatible sequence. This co-generative setting is especially challenging due to the need to reconcile fundamentally different data manifolds, enforce SE(3) symmetry, and ensure bidirectional invertibility, all while scaling to the vast combinatorial space of long proteins. CoFlow [\[198\]](#page-23-12) proposes a joint discrete flow that models residue identities and inter-residue distances as CTMC states, augmented with a multimodal masked language module that allows structural flows and sequence tokens to condition each other. Discrete Flow Models (DFM) [\[63\]](#page-17-18) formalize flow matching on arbitrary discrete spaces by interpreting scorebased guidance as CTMC generator reversal. Instantiated as MultiFlow, this framework enables sequence-only, structureonly, or joint generation within a single architecture-agnostic model, achieving state-of-the-art perplexity and TM-scores while being orders of magnitude faster than diffusion-based baselines. Finally, APM [\[199\]](#page-23-13) introduces a Seq&BB module that jointly learns continuous SE(3) flows for backbone frames and discrete token flows for sequences, leveraging protein language models, Invariant Point Attention, and Transformer encoders to capture residue-level and pairwise interactions. APM supports precise interchain modeling and de novo design of protein complexes with specified binding properties.

## *B. Conditional Generation*

*a) Motif-scaffolding Generation:* Motif-Scaffolding Generation. Conditional SE(3) flow-matching models embed fixed functional motifs into de-novo backbones by learning equivariant vector fields that respect both local motif geometry and global fold constraints, overcoming the diversity and fidelity limits of earlier diffusion approaches. FrameFlow-Motif [\[200\]](#page-23-14) augments FrameFlow [\[191\]](#page-23-5) with motif amortization and inference-time motif guidance, enabling scaffold generation around functional motifs with special-designed data augmentation and estimated conditional scores. EVA [\[201\]](#page-23-15) casts scaffolding as geometric inverse design, steering a pretrained flow along motif-aligned probability paths to accelerate convergence and boost structural fidelity.

*b) Pocket & binder Design:* Conditional pocket and binder design tackles the dual challenge of sculpting a protein interface that both accommodates a specific ligand conformation and retains global fold stability, all while respecting SE(3) symmetry and the rich geometric-chemical priors that govern non-covalent recognition. Flow-matching models address these hurdles by learning equivariant vector fields that map an easy base distribution to the manifold of ligand-compatible

protein–ligand complexes in a single, differentiable pass, avoiding the slow guidance loops and hand-crafted potentials of earlier diffusion or docking pipelines. AtomFlow [\[202\]](#page-23-16) unifies protein and ligand atoms into "biotokens" and applies atomic-resolution SE(3) flow matching to co-generate ligand conformations and binding backbones directly from a 2-D molecular graph. FlowSite [\[203\]](#page-23-17) introduces a self-conditioned harmonic flow objective that first aligns apo proteins to a harmonic potential and then co-generates discrete residue types and 3-D ligand poses, supporting multi-ligand docking and outperforming prior generative and physics-based baselines on pocket-level benchmarks. PocketFlow [\[204\]](#page-23-18) incorporates protein–ligand interaction priors (e.g., hydrogen-bond geometry) directly into the flow, then applies multi-granularity guidance to produce high-affinity pockets that significantly improve Vina scores and generalize across small molecules, peptides, and RNA ligands. To efficiently recover all-atom structures from coarse-grained simulations, FlowBack [\[205\]](#page-23-19) utilizes flow matching to map coarse-grained representations to all-atom configurations, achieving high fidelity in protein and DNA structure reconstruction.

## *C. Structure Prediction*

*a) Conformer Prediction:* Accurately sampling the conformational ensembles underlying protein function remains challenging due to the cost of exhaustive molecular dynamics. Recent work leverages sequence-conditioned, SE(3)-equivariant flow matching to efficiently generate diverse, physically consistent states aligned with experimental observables. AlphaFold Meets Flow Matching [\[206\]](#page-23-20) repurposes single-state predictors (AlphaFold, ESMFold) as generative engines by fine-tuning them under a harmonic flow-matching objective, yielding AlphaFlow/ESMFlow ensembles that surpass MSA-subsampled AlphaFold on the precision-diversity trade-off and reach equilibrium observables faster than replicate MD trajectories. P2DFlow [\[207\]](#page-23-21) augments SE(3) flow matching with a latent "ensemble" dimension and a physics-motivated prior, enabling it to reproduce crystallographic B-factor fluctuations and ATLAS MD distributions more faithfully than earlier baselines.

*b) Side-chain Packing:* Predicting rotameric states for each residue requires joint compliance with steric constraints, energetic preferences, and SE(3)-equivariance. Recent work has explored constrained side-chain prediction through flow matching. FlowPacker [\[208\]](#page-23-22) formulates side-chain placement as torsional flow matching, coupling the learned vector field to EquiformerV2 [\[209\]](#page-23-23), an SE(3)-equivariant graph attention backbone. PepFlow [\[210\]](#page-23-24) generalizes this approach to fullatom peptides using a multi-modal flow that captures joint distributions over backbone frames, side-chain torsions, and residue identities. Partial sampling from this flow achieves

state-of-the-art results in fixed-backbone packing and receptorbound refinement, while maintaining full differentiability for downstream design applications.

*c) Docking Prediction:* Recent work reframes proteinligand docking as a flow-matching (FM) generative problem, replacing diffusion with a simulation-free objective that learns a bijective map from unbound receptors (apo) to bound complexes (holo). FlowSite [\[203\]](#page-23-17) introduces a self-conditioned FM objective that harmonically couples translational, rotational and torsional degrees of freedom. By leveraging GAT and TFN layers for ligand–protein interaction modeling, it further extends to jointly generate contact residues and ligand coordinates, substantially improving sample quality, simplicity, and generality in pocket-level docking. Meanwhile, FlowDock [\[211\]](#page-23-25) learns a geometric flow mapping unbound to bound structures, while predicting per-complex confidence and binding affinity estimates. It achieves a 51% blind docking success rate on the PoseBusters benchmark, outperforming single-sequence AlphaFold3 without MSA inputs, and ranks in the top-5 for affinity prediction in CASP16 across 140 complexes.

#### *D. Peptide and Antibody Generation*

Recent work [\[207,](#page-23-21) [210,](#page-23-24) [212,](#page-23-26) [213,](#page-24-0) [214\]](#page-24-1) formulates peptide design as conditional flow matching over multiple geometric and categorical manifolds, explicitly modeling residue type, spatial position, orientation, and angles in a unified generative framework. PepFlow [\[210\]](#page-23-24) introduces the first multi-modal flow matching framework for protein structure design, jointly modeling residue positions via Euclidean CFM, orientations via Spherical CFM, angles via Toric CFM, and types via Simplex CFM. This unified approach achieves excellent performance on sequence recovery and side-chain packing in receptorconditioned design tasks. D-Flow [\[207\]](#page-23-21) extends this paradigm to D-peptides by augmenting limited training data through a chirality-aware mirror transformation and incorporating a lightweight structural adapter into a pretrained protein language model. PPFlow [\[212\]](#page-23-26) formulates peptide torsion generation as flow matching on a (3n − 3)-torus with n being the number of amino acids, while modeling global transitions and residue types via Euclidean flows and employing SO(3)-CFM for rotations. This formulation enables effective conditional sampling for diverse tasks such as peptide optimization and docking. Finally, NLFlow [\[213\]](#page-24-0) pioneers non-linear conditional vector fields by employing polynomial interpolation over the position manifold, enabling faster convergence toward binding pockets and effectively addressing temporal inconsistencies across modalities. This approach leads to improvements in structural stability and binding affinity compared to prior linear flow models. Collectively, these studies underscore the importance of manifold-specific flows, conditioning strategies, and geometric priors for scalable, high-fidelity peptide generation. In contrast to these geometry-intensive approaches, ProtFlow [\[214\]](#page-24-1) treats peptides as amino acid sequences and bypasses non-Euclidean representations by embedding each residue using a pretrained protein language model (PLM). In the embedding space of PLMs, ProtFlow trains a reflow-enabled sequence flow model that supports both single-step generation and multi-chain codesign. Collectively, these studies highlight the critical role of

manifold-specific flows, conditioning strategies, and geometric priors in enabling scalable and high-fidelity peptide generation.

The study of antibody structure design with flow matching is emerging as well. For instance, FlowAB [\[215\]](#page-24-2) utilizes energy-guided SE(3) flow matching to improve antibody structure refinement, integrating physical priors to enhance CDR accuracy with minimal computational overhead.

#### VII. OTHER BIO APPLICATIONS

#### <span id="page-11-0"></span>*A. Dynamic Cell Trajectory Prediction*

Dynamic Cell Trajectory. Generative trajectory models seek to reconstruct the continuously branching, stochastic evolution of cells from high-dimensional, sparsely sampled single-cell readouts, which is an endeavor hampered by severe noise, irregular time points, and the risk that straight Euclidean interpolants stray outside the biological manifold. CellFlow [\[96\]](#page-19-13) tackles this by framing morphology evolution under perturbations as an image-level flow-matching problem on cellular masks, enabling realistic, perturbation-conditioned movies of shape change that outperform diffusion and GAN baselines in both faithfulness and diversity. GENOT-L [\[71\]](#page-18-0) introduces an entropic Gromov-Wasserstein flow that couples gene-expression geometry across time points, producing probabilistic lineage trajectories that capture heterogeneity and branching better than optimal-transport predecessors while remaining simulation-free. Metric Flow Matching [\[97\]](#page-19-14) instead learns geodesic vector fields under a data-induced Riemannian metric, yielding smoother interpolations that respect the manifold's curvature and achieving state-of-the-art accuracy on single-cell trajectory benchmarks with fewer artifacts than Euclidean flows. Diversified Flow Matching [\[98\]](#page-19-15) extends this line of work by ensuring translation identifiability across diverse conditional distributions, a key challenge in modeling heterogeneous cellular states. Unlike prior GAN-based solutions, this work formulates the problem within an ODE-based flow matching framework, offering stable training and explicit transport trajectories. Collectively, these works highlight the importance of geometry-aware objectives and probabilistic conditioning for faithful dynamic cell-state generation.

#### *B. Bio-image Generation and Enhancement*

Leveraging continuous probability flow to efficiently model biological structures, flow matching has shown great potential for bio-image generation and enhancement, enabling faster and more accurate modeling of complex biological data. One notable application is FlowSDF [\[99\]](#page-19-16), which introduces image-guided conditional flow matching for medical image segmentation. By modeling signed distance functions (SDF) instead of binary masks, FlowSDF achieves smoother and more accurate segmentation. This method also generates uncertainty maps, enhancing robustness in prediction tasks. For medical image synthesis, an optimal transport flow matching approach [\[100\]](#page-19-17) addresses the challenge of balancing generation speed and image quality. By creating a more direct mapping between distributions, this method reduces inference time while maintaining high-quality outputs, and supports diverse imaging modalities,

<span id="page-12-0"></span>

| Task                                     | Dataset                                                                                                                                                                                                                                                                                                 | Scale / Number of Samples                                                                                                                                                                                                          | Links                                                                                                                                                                                                                                                     | Used By                                                                                          |
|------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
| DNA Sequence<br>Generation               | Promoter DNA Sequence<br>Enhancer DNA Sequence                                                                                                                                                                                                                                                          | 100, 000<br>104,665 (fly brain); 88,870 (human melanoma)                                                                                                                                                                           | Paper1; Paper2 Paper3 Code1; Code2; Code3;<br>Paper1 Paper2; Code1; Code2;                                                                                                                                                                                | [51] [60] [67]<br>[51] [60]                                                                      |
| RNA Sequence<br>Generation               | Rfam Database [216]<br>Muscle/PC3/HEK 5' UTR libraries [217]<br>RNAsolo [218]                                                                                                                                                                                                                           | Over 20M sequences<br>41,446<br>18,808 RNA 3D structures                                                                                                                                                                           | Paper; Homepage; Huggingface;<br>Paper; Code<br>Paper; Homepage                                                                                                                                                                                           | [68]<br>[68]<br>[69] [70]                                                                        |
| Single-cell<br>Trajectory                | Pancreas single-cell data [219]<br>Drug perturbation single-cell data [220]<br>Multi-modal single-cell analysis [221]<br>PBMC [222]<br>Dentate gyrus dataset [223]<br>Human Lung cells Atlas [224]<br>Tabula Muris [226]<br>Embryoid Body (EB) [227]<br>CITE-seq (Cite) [228]<br>Multiome (Multi) [228] | 36,351 cells<br>650K single-cell transcriptomes<br>120K single cells (human bone marrow)<br>30K cells<br>18,213 cells<br>584,944. [72] uses a subset of 32,272 [225]<br>245,389 cells<br>5 marginals<br>4 marginals<br>4 marginals | Paper; Download Link<br>Paper; Download Instruction<br>Paper; Homepage; Dataset List<br>Paper; Download Link<br>Paper; scVelo Documentation<br>Paper; Homepage; Dataset List<br>Paper; Homepage; Code<br>Paper; Code<br>Paper; Homepage<br>Paper Homepage | [71]<br>[71]<br>[71] [73]<br>[72] [73]<br>[72] [73]<br>[72] [73]<br>[73]<br>[97]<br>[97]<br>[97] |
| Molecule<br>Generation                   | Quantum Machine (QM) [179]                                                                                                                                                                                                                                                                              | Various sizes. QM9: 133,885                                                                                                                                                                                                        | Paper; Homepage; Paper With Code; Kaggle                                                                                                                                                                                                                  | [53] [64] [66] [76] [77] [78]<br>[45] [79] [80] [45] [81] [83]<br>[82] [91] [89] [93] [94] [95]  |
|                                          | ZINC [229]                                                                                                                                                                                                                                                                                              | Various sizes. ZINC250K: 249,456                                                                                                                                                                                                   | Paper; Paper With Code; Huggingface; Kaggle                                                                                                                                                                                                               | [53] [64] [76] [77] [78]<br>[45] [88] [81] [94]                                                  |
|                                          | Guacamol [230]<br>MOSES [231]                                                                                                                                                                                                                                                                           | 1,591,378<br>1,936,963                                                                                                                                                                                                             | Paper; Code; Paper With Code<br>Paper; Code; Paper With Code                                                                                                                                                                                              | [64] [78] [45]<br>[64] [78] [45]                                                                 |
|                                          | GEOM-Drugs [178]                                                                                                                                                                                                                                                                                        | 430,000                                                                                                                                                                                                                            | Paper; Code; Paper With Code                                                                                                                                                                                                                              | [66] [80] [45] [81] [82] [83]<br>[85] [89] [86] [93] [94] [95]                                   |
|                                          | PoseBusters benchmark [232]<br>GEOM-QM9 [178]<br>SAbDab [233]                                                                                                                                                                                                                                           | 308 curated protein–ligand complexes<br>133,885<br>9,680                                                                                                                                                                           | Paper; code; Paper with code<br>Paper; Code; Paper With Code<br>Paper; Homepage;                                                                                                                                                                          | [74]<br>[66] [79] [85] [86]<br>[87] [215] [234]                                                  |
| Molecular Binder<br>Generation           | Binding MOAD [235]<br>CrossDocked [236]                                                                                                                                                                                                                                                                 | 41K complexes<br>22.5M protein-molecule pairs                                                                                                                                                                                      | Paper; Homepage<br>Paper; Code                                                                                                                                                                                                                            | [92] [203] [204]<br>[92] [204]                                                                   |
| Molecular Docking                        | PDBBind [237]<br>PPDBench [238]                                                                                                                                                                                                                                                                         | 33,653 biomolecular complexes<br>133 protein-peptide complexes                                                                                                                                                                     | Paper; Homepage<br>Paper; Homepage                                                                                                                                                                                                                        | [90] [203] [204]<br>[204]                                                                        |
| Protein Sequence<br>Design               | UniRef [239]<br>Protein Data Bank (PDB) [240]<br>Open Metagenomic Corpus (OMG) [243]<br>SAbDab [233]<br>OAS-paired antibody sequences [244]<br>RAbD Benchmark [245]<br>UniProt [246]<br>UniProtKB/SwissProt [247]                                                                                       | Various sizes. UniRef50: 70,198,728<br>Over 200K. 18,684 for curated version [241]<br>3.3B in total. OMG_prot50: 207M<br>9,680<br>1.86M pairs<br>60<br>Over 60 million sequences<br>18364 sequence entries, 5,986,949 amino acids  | Paper; Homepage; Huggingface<br>Paper; Homepage; Wikipedia<br>Paper ; Code; HuggingFace; Genomic LM<br>Paper; Homepage;<br>Paper; Homepage<br>Paper; Manual;<br>Paper; Homepage<br>Paper; Homepage                                                        | [59] [67]<br>[63] [205] [242]<br>[67]<br>[74] [75]<br>[74]<br>[75]<br>[214]<br>[214]             |
| Protein Backbone<br>Generation           | Protein Data Bank (PDB) [240]<br>SCOPe [248]<br>Huguet et al. [195]                                                                                                                                                                                                                                     | Over 200K. 18,684 for curated version [241]<br>108,069<br>160K structures                                                                                                                                                          | Paper; Homepage; Wikipedia<br>Paper; Homepage<br>Paper; Code                                                                                                                                                                                              | [90] [191] [192] [195] [202] [200]<br>[194] [202]<br>[195]                                       |
| De Novo Protein<br>Generation            | PepBDB [249]<br>PepMerge [210]                                                                                                                                                                                                                                                                          | 13,299 peptide-protein complex<br>8,365                                                                                                                                                                                            | Paper; Homepage; PepBDB-ML<br>Paper; Code                                                                                                                                                                                                                 | [212]<br>[250] [210] [213]                                                                       |
| Protein Ensemble<br>Dynamics             | Protein Data Bank (PDB) [240]<br>ATLAS [251]                                                                                                                                                                                                                                                            | Over 200K proteins<br>1390 protein chains [241]                                                                                                                                                                                    | Paper; Homepage; Wikipedia<br>Paper; Homepage                                                                                                                                                                                                             | [206]<br>[206] [207]                                                                             |
| Protein Docking or<br>Side-chain Packing | CASP [252]                                                                                                                                                                                                                                                                                              | Various sizes                                                                                                                                                                                                                      | Paper; Homepage                                                                                                                                                                                                                                           | [208] [211]                                                                                      |
| Peptide Binder<br>Design                 | PDBBind [237]<br>PepNN (peptide binding sites) [254]<br>BioLip2 [255]<br>Binder discrimination dataset [74]                                                                                                                                                                                             | 33,653 biomolecular complexes<br>Varoius sizes ranging from 251 to 2,517<br>385,160 protein chains; 781,684 interactions<br>4,883 antibody–antigen complexes                                                                       | Paper; Homepage<br>Paper; Code; Wikipedia<br>Paper; Homepage; Web Code<br>Paper;                                                                                                                                                                          | [69] [193] [253]<br>[67]<br>[67]<br>[74]                                                         |
| Peptide Design                           | PepBDB [249]<br>PepMerge [210]                                                                                                                                                                                                                                                                          | 13,299 peptide-protein complex<br>8,365                                                                                                                                                                                            | Paper; Homepage; PepBDB-ML<br>Paper; Code                                                                                                                                                                                                                 | [212]<br>[250] [210] [213]                                                                       |

including 2D and 3D. In MR image reconstruction, Multi-Modal Straight Flow Matching (MMSFlow) [\[101\]](#page-19-18) significantly reduces the number of inference steps by forming a linear path between undersampled and reconstructed images. Leveraging multi-modal information with low- and high-frequency fusion layers, MMSFlow achieves state-of-the-art performance in fastMRI and Brats-2020 benchmarks.

## *C. Cellular Microenvironments from Spatial Transcriptomics*

Flow matching has also emerged as a powerful framework for modeling spatial transcriptomics (ST) data, which captures gene expression levels across spatial locations within a tissue. The core task in ST involves reconstructing or generating spatiallyresolved gene expression maps that reflect underlying cellular microenvironments and tissue organization. One such method is STFlow [\[102\]](#page-19-19) which introduces a scalable flow matching framework for generating spatial transcriptomics data from

whole-slide histology images. It models the joint distribution of gene expression across all spatial spots in a slide, thereby explicitly capturing cell-cell interactions and tissue organization. Complementarily, Wasserstein Flow Matching (WFM) [\[103\]](#page-19-20) generalizes flow-based generative modeling to families of distributions. It introduces a principled way to model both 2D and 3D spatial structures of cellular microenvironments, and leverages the geometry of Wasserstein space to better match distributional characteristics across biological contexts. Together, these methods highlight the utility of flow matching in capturing the spatially-aware, high-dimensional distributions characteristic of modern transcriptomics datasets.

# *D. Neural Activities*

Flow matching has recently shown promise in modeling and aligning neural activity, particularly for time series and braincomputer interface (BCI) applications, where neural signals are

<span id="page-13-0"></span>

| Task                          | Dataset                                                                 | Scale / Number of Samples                                                                                       | Links                                                                                 | Used By                        |
|-------------------------------|-------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|--------------------------------|
| Cell Morphology<br>Profiling  | BBBC021 [256]<br>RxRx1 [257]<br>JUMP Cell Painting [258]                | 39,600 images<br>125,510 images<br>1.6 billion profiles                                                         | Paper; Homepage;<br>Paper; Homepage; Code; Paper With Code<br>Paper; Code; AWS        | [96]<br>[96]<br>[96]           |
| Medical Image<br>Segmentation | MoNuSeg [259]<br>GlaS [260]<br>CAMUS [261]<br>MSD Brain MRI [262]       | 30 train + 14 test images<br>85 train + 80 test images<br>450 patients; 1,600 images<br>750 scans (T1-weighted) | Paper; Homepage;<br>Paper; Homepage;<br>Paper; Homepage<br>Paper; AWS; PapersWithCode | [99]<br>[99]<br>[100]<br>[100] |
| MRI<br>Reconstruction         | fastMRI [263]<br>BraTS-2020 (Brain Tumor Segmentation) [264]            | Knee: 1,398 scans; Brain: 7,002 scans<br>494 subjects , 240×240                                                 | Homepage; Paper; Code<br>Homepage; Paper; Kaggle                                      | [101]<br>[101]                 |
| Spatial<br>Transcriptomics    | HEST-1k [265]<br>STImage-1K4M [266]                                     | 1,229 profiles<br>1,149 slides (4 293 195 spots)                                                                | Paper; Code<br>Paper; Code; HuggingFace                                               | [102]<br>[102]                 |
| Single-cell<br>Omics          | seqFISH [267]<br>scRNA-seq [268]                                        | 351 genes; 29 cells per niche<br>32 PCs per meta-cell                                                           | Paper; Code<br>Paper; Code                                                            | [103]<br>[103]                 |
| Neural<br>Time Series         | Mouse brain LFP [269]                                                   | 50 marginals (50–500 ms)                                                                                        | Paper; Code                                                                           | [104]                          |
| Neural Population<br>Dynamics | CO-C (Monkey C) [270]<br>CO-M (Monkey M) [271]<br>RT-M (Monkey M) [272] | 5 sessions; 957 units<br>9 sessions; 1,728 units<br>1 session; 130 units                                        | DREAM; Paper<br>pmd-1; Paper<br>NLB-RTT; Paper                                        | [105]<br>[105]<br>[105]        |

TABLE VI DATASETS AND SOFTWARE IN BIOLOGY AND LIFE SCIENCE TO TEST FLOW MATCHING METHODS (PART II)

often stochastic and nonstationary. Stream-level Flow Matching with Gaussian Processes [\[104\]](#page-19-21) extends conditional flow matching by introducing streams, which arelatent stochastic paths modeled with Gaussian processes. This reduces variance in vector field estimation, enabling more accurate modeling of correlated time series such as neural recordings. Flow-Based Distribution Alignment [\[105\]](#page-19-22) tackles inter-day neural signal shifts in BCIs through source-free domain adaptation. By learning stable latent dynamics via flow matching and ensuring stability through Lyapunov analysis, it enables reliable fewtrial neural adaptation across days. These approaches highlight the versatility of flow matching for neural data, supporting both high-fidelity generation and robust adaptation with limited supervision.

#### VIII. EVALUATION TASKS AND DATASETS

In this section, we summarize evaluation tasks and datasets used for assessing flow matching methods in biology and life sciences. As listed in Table [V](#page-12-0) and Table [VI,](#page-13-0) these tasks span a wide spectrum of domains, including genomics, transcriptomics, molecular chemistry, and structural biology. For each dataset, we also report its data scale or number of samples. Flow matching has been applied to a diverse set of generation and modeling problems, such as biological sequence generation, cell trajectory inference, molecule design, and protein structure modeling.

*Sequence-Level Generation.* Flow matching models have been evaluated on tasks like DNA [\[51,](#page-17-6) [60,](#page-17-15) [67\]](#page-17-22), RNA [\[216,](#page-24-3) [217,](#page-24-4) [218\]](#page-24-5), and protein [\[239,](#page-25-4) [240,](#page-25-5) [243\]](#page-25-8) sequence generation. These datasets range from promoter and enhancer sequences to large-scale protein and metagenomic corpora, covering both canonical and noncoding regions of the genome.

*Single-Cell Modeling and Trajectory Inference.* Flow matching has been used to model temporal or conditional transitions in high-dimensional single-cell gene expression data, including developmental trajectories [\[219\]](#page-24-6), perturbation responses [\[220\]](#page-24-7), and modality prediction [\[221\]](#page-24-8). Datasets such as PBMC [\[222\]](#page-24-9), dentate gyrus [\[223\]](#page-24-10), and Tabula Muris [\[226\]](#page-24-13) provide diverse experimental contexts for evaluating these tasks.

*Molecular Generation and Conformation Modeling.* Datasets such as QM9 [\[179\]](#page-22-15), ZINC [\[229\]](#page-24-16), GEOM-Drugs [\[178\]](#page-22-14), and MOSES [\[231\]](#page-24-18) provide chemically diverse molecular structures, enabling evaluation of molecular validity, novelty, and 3D geometry. Flow matching models are tested on their ability to generate, edit, or align molecular graphs and conformers.

*Protein and Complex Design.* Structural datasets like SCOPe [\[248\]](#page-25-13), ATLAS [\[251\]](#page-25-16), and curated PDB subsets support evaluation of flow-based models on protein backbone generation, folding, and structural refinement. Complementary datasets such as Binding MOAD [\[235\]](#page-25-0), CrossDocked [\[236\]](#page-25-1), BioLip2 [\[255\]](#page-25-20), and PepBDB [\[249\]](#page-25-14) enable studies on molecular docking, peptide-protein interactions, and binder generation.

Notably, many datasets are reused across different tasks due to their structural richness and biological relevance. For instance, the Protein Data Bank (PDB) [\[240\]](#page-25-5) is used in tasks ranging from protein sequence design and backbone generation to modeling conformational dynamics and performing docking. Similarly, SAbDab [\[233\]](#page-24-20) supports antibody sequence generation, structural modeling, and binder discrimination.

Despite the growing adoption of flow matching in biology, the field still lacks unified benchmarks for many tasks. This is likely due to the inherent heterogeneity of biological problems, ranging from sequence to structure, from single-cell to population scale, which makes standardized evaluation more challenging. This stands in contrast to fields like computer vision or NLP, where well-defined benchmarks are more prevalent [\[273,](#page-26-13) [274,](#page-26-14) [275,](#page-26-15) [276\]](#page-26-16). Continued efforts in dataset curation and task formulation are needed to support consistent and reproducible assessment of generative models in the life sciences.

#### IX. FUTRUE DIRECTION

# *A. Flow Matching for Discrete Sequence Generation*

Flow Matching has recently emerged as a promising generative modeling paradigm, offering a compelling balance

between generation quality and training stability. While its success in continuous domains like image and molecule generation has been widely documented, applying FM to discrete sequence generation—especially in domains such as natural language, genomics, and code—remains a vibrant and largely underexplored frontier.

One of the most intriguing directions lies in understanding the representational advantages of discrete Flow Matching compared to traditional paradigms such as Masked Language Modeling (MLM). Unlike MLM, which relies on partial observation and token masking, FM provides a direct mapping from a base distribution to the target sequence via a continuous probability flow. This raises the question: Can discrete FM yield more semantically coherent representations and facilitate better downstream performance in tasks such as classification? Recent advances, such as Fisher Flow[\[60\]](#page-17-15) and Dirichlet FM[\[51\]](#page-17-6), demonstrate that geometry-aware formulations over the probability simplex can encode meaningful geometric constraints and structure-aware trajectories, enabling more faithful modeling of discrete data distributions.

Another fundamental question concerns the generation capabilities of discrete FM relative to autoregressive (AR) models. While AR models remain the gold standard in natural language generation due to their strong likelihood modeling and contextual fluency, they suffer from slow sampling and exposure bias. In contrast, discrete FM supports parallel generation through ODE integration or sampling over learned Markov trajectories, offering substantial efficiency gains. However, its generation quality still lags behind state-of-the-art AR transformers in language generation [\[60\]](#page-17-15), prompting future research into architectural refinements and better training objectives.

Furthermore, the integration of FM with Transformer architectures remains an open challenge. Existing Transformerbased FM models either operate in latent embedding space or use discrete-continuous relaxations (e.g., Gumbel-Softmax) to approximate gradient flows. Yet, the Transformer's causal attention structure may be suboptimal for non-autoregressive FM-based sequence generation, especially in domains where left-to-right order is arbitrary or non-existent (e.g., protein sequences, biological pathways). This invites research into order-agnostic architectures or the use of permutation-invariant encoders to better align with FM-based modeling.

Finally, flow matching may offer unique advantages in nonlanguage sequence modeling tasks, such as biomolecular design and genome modeling, where biological constraints (e.g., basepairing, structural motifs) must be enforced. Unlike language, these sequences often lack natural generation order and exhibit rich multi-modal dependencies. FM's ability to incorporate conditioning, geometry-aware constraints, and structure-guided generation (e.g., via SE(3)-equivariant or manifold-aware flows) makes it a particularly attractive candidate. Future work may focus on developing discrete FM formulations that are not only domain-adaptive, but also biologically interpretable and sample-efficient.

#### *B. Small Molecule Generation and Modeling*

Small molecule generation is a core task in cheminformatics and drug discovery, where FM has recently shown promising capabilities in both unconditional and conditional generation settings. By modeling continuous probability flows between simple priors and molecular distributions, FM offers an appealing alternative to diffusion models, with improved sample efficiency and the potential to integrate domain knowledge. However, due to the scarcity of molecular structure data and the complexity of structural constraints, several key challenges remain before FM can fully realize its potential for small molecule generation.

One fundamental limitation lies in the data scarcity and structural heterogeneity of small molecule datasets. Unlike macromolecules such as proteins, which benefit from largescale structural repositories (e.g., PDB), small molecule datasets are often limited in size and diversity, especially for annotated 3D conformers. As a result, FM models trained on these datasets may struggle to generalize across different chemical scaffolds, limiting their utility in low-resource or out-ofdistribution scenarios. Addressing this issue may require more effective data augmentation strategies (e.g., using force field simulations or generative conformer expansion), transfer learning pipelines, or semi-supervised flow matching objectives that make better use of unlabeled data.

To improve the physical plausibility and functional relevance of generated small molecules, a key direction lies in incorporating domain-specific inductive priors into both the training and sampling stages of flow matching. Small molecules are governed by well-defined chemical and physical constraints—such as bond lengths and angles, valence rules, charge distributions, and conformational energetics—which can be explicitly modeled to constrain the learned probability flow. Embedding such priors into the vector field design or generation trajectories (e.g., via energy-guided loss functions or structure-aware conditioning) can substantially improve the realism and synthesizability of generated compounds.

At the same time, enhancing the conditional generation capabilities of FM is essential for tasks that demand goaldirected molecular design, such as generating molecules with desired pharmacological properties, satisfying functional group templates, or fitting into predefined binding pockets. Conditional flow matching offers a natural framework for structure- and property-guided generation, enabling fine-grained control over outputs via learned trajectories that satisfy specific constraints. Future work may explore more expressive conditioning schemes, multi-property guidance, or interaction-aware control mechanisms, paving the way for FM-based models to support precision molecular design in high-stakes domains such as drug discovery and materials engineering.

A further challenge lies in modeling molecular interactions and dynamic processes. Molecular docking and binding affinity prediction remain critical tasks in early-stage drug design, requiring models to account for conformational flexibility in small molecules and the adaptive nature of protein binding pockets, particularly with respect to side-chain rearrangements. Even more challenging tasks, such as enzyme design, involve

not just molecular recognition but also modeling of specific reaction mechanisms. Thus, leveraging the FM framework to capture inter-molecular interactions and reaction dynamics represents a crucial and promising direction for future research.

#### *C. Protein*

In the field of protein modeling, Flow Matching (FM) has emerged as an efficient approach for sequence and structure modeling, demonstrating complementary advantages to traditional methods. Proteins, as highly complex biological macromolecules, exhibit a unique combination of discrete primary sequences and continuous three-dimensional structures, which poses distinct challenges for the design and training of FM-based models.

One important future direction is to establish effective matching mechanisms across different protein modalities. For example, in mapping from amino acid sequences to 3D structures, FM could serve as a bridge between discrete and continuous spaces, enhancing the model's expressiveness in structure prediction and generation tasks. Furthermore, in applications such as protein-protein docking and complex assembly modeling, FM offers a promising framework for capturing transformation paths in high-dimensional, complex spaces.

In addition, modeling protein dynamics—such as conformational changes or ligand-induced fit—remains a core challenge in structural biology. Future work may explore integrating FM with physical simulations (e.g., molecular dynamics) or diffusion-based processes, enabling the learning of natural transition paths between protein states and improving interpretability of their functional mechanisms.

#### X. CONCLUSION

Flow matching has become a compelling alternative to diffusion-based generative modeling, offering advantages in stability, efficiency, and control. In this survey, we provide a structured overview of its growing use in biology and life sciences, covering a diverse range of tasks from sequence generation and molecular design to protein modeling. We also compile a comprehensive list of datasets used for evaluation, including their scale and cross-task applicability. Despite promising progress, we also summarize the challenges that the field faces. We hope this survey could clarify current trends and motivate future research at the intersection of generative modeling and the life sciences.

#### REFERENCES

- <span id="page-15-0"></span>[1] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, "Flow matching for generative modeling," in *The Eleventh International Conference on Learning Representations*, 2023. [Online]. Available: [https:](https://openreview.net/forum?id=PqvMRDCJT9t) [//openreview.net/forum?id=PqvMRDCJT9t](https://openreview.net/forum?id=PqvMRDCJT9t)
- <span id="page-15-1"></span>[2] Y. Jin, Z. Sun, N. Li, K. Xu, K. Xu, H. Jiang, N. Zhuang, Q. Huang, Y. Song, Y. Mu, and Z. Lin, "Pyramidal flow matching for efficient video generative modeling," in *The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28,*

*2025*. OpenReview.net, 2025. [Online]. Available: <https://openreview.net/forum?id=66NzcRQuOq>

- <span id="page-15-2"></span>[3] V. T. Hu, D. Wu, Y. M. Asano, P. Mettes, B. Fernando, B. Ommer, and C. Snoek, "Flow matching for conditional text generation in a few sampling steps," in *Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 2: Short Papers, St. Julian's, Malta, March 17-22, 2024*, Y. Graham and M. Purver, Eds. Association for Computational Linguistics, 2024, pp. 380–392. [Online]. Available: <https://aclanthology.org/2024.eacl-short.33>
- <span id="page-15-3"></span>[4] I. Gat, T. Remez, N. Shaul, F. Kreuk, R. T. Chen, G. Synnaeve, Y. Adi, and Y. Lipman, "Discrete flow matching," *Advances in Neural Information Processing Systems*, vol. 37, pp. 133 345–133 385, 2024.
- <span id="page-15-4"></span>[5] G. M. Church and W. Gilbert, "Genomic sequencing." *Proceedings of the National Academy of Sciences*, vol. 81, no. 7, pp. 1991–1995, 1984.
- <span id="page-15-5"></span>[6] J. C. Venter, M. D. Adams, E. W. Myers, P. W. Li, R. J. Mural, G. G. Sutton, H. O. Smith, M. Yandell, C. A. Evans, R. A. Holt *et al.*, "The sequence of the human genome," *science*, vol. 291, no. 5507, pp. 1304–1351, 2001.
- <span id="page-15-6"></span>[7] C. S. Pareek, R. Smoczynski, and A. Tretyn, "Sequencing technologies and genome sequencing," *Journal of applied genetics*, vol. 52, pp. 413–435, 2011.
- <span id="page-15-7"></span>[8] S. Luo, J. Guan, J. Ma, and J. Peng, "A 3d generative model for structure-based drug design," *Advances in Neural Information Processing Systems*, vol. 34, pp. 6229–6239, 2021.
- <span id="page-15-8"></span>[9] A. V. Sadybekov and V. Katritch, "Computational approaches streamlining drug discovery," *Nature*, vol. 616, no. 7958, pp. 673–685, 2023.
- <span id="page-15-9"></span>[10] S. Mathur and C. Hoskins, "Drug development: Lessons from nature," *Biomedical reports*, vol. 6, no. 6, pp. 612– 614, 2017.
- <span id="page-15-10"></span>[11] J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel, O. Ronneberger, L. Willmore, A. J. Ballard, J. Bambrick *et al.*, "Accurate structure prediction of biomolecular interactions with alphafold 3," *Nature*, vol. 630, no. 8016, pp. 493–500, 2024.
- <span id="page-15-11"></span>[12] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko *et al.*, "Highly accurate protein structure prediction with alphafold," *nature*, vol. 596, no. 7873, pp. 583–589, 2021.
- <span id="page-15-12"></span>[13] M. Baek, I. Anishchenko, I. R. Humphreys, Q. Cong, D. Baker, and F. DiMaio, "Efficient and accurate prediction of protein structure using rosettafold2," *BioRxiv*, pp. 2023–05, 2023.
- <span id="page-15-13"></span>[14] R. A. Robb, *Biomedical imaging, visualization, and analysis*. John Wiley & Sons, Inc., 1999.
- <span id="page-15-14"></span>[15] C. M. Tempany and B. J. McNeil, "Advances in biomedical imaging," *Jama*, vol. 285, no. 5, pp. 562–567, 2001.
- <span id="page-15-15"></span>[16] A. Webb, *Introduction to biomedical imaging*. John Wiley & Sons, 2022.

- <span id="page-16-0"></span>[17] R. M. Rangayyan, *Biomedical image analysis*. CRC press, 2004.
- <span id="page-16-1"></span>[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial networks," *Communications of the ACM*, vol. 63, no. 11, pp. 139–144, 2020.
- <span id="page-16-2"></span>[19] L. Lan, L. You, Z. Zhang, Z. Fan, W. Zhao, N. Zeng, Y. Chen, and X. Zhou, "Generative adversarial networks and its applications in biomedical informatics," *Frontiers in public health*, vol. 8, p. 164, 2020.
- <span id="page-16-3"></span>[20] M. Lee, "Recent advances in generative adversarial networks for gene expression data: a comprehensive review," *Mathematics*, vol. 11, no. 14, p. 3055, 2023.
- <span id="page-16-4"></span>[21] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, "Masked autoencoders are scalable vision learners," in *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, 2022, pp. 16 000–16 009.
- <span id="page-16-5"></span>[22] O. Kraus, K. Kenyon-Dean, S. Saberian, M. Fallah, P. McLean, J. Leung, V. Sharma, A. Khan, J. Balakrishnan, S. Celik *et al.*, "Masked autoencoders are scalable learners of cellular morphology," *arXiv preprint arXiv:2309.16064*, 2023.
- <span id="page-16-6"></span>[23] M. Yuan, A. Shen, K. Fu, J. Guan, Y. Ma, Q. Qiao, and M. Wang, "Proteinmae: masked autoencoder for protein surface self-supervised learning," *Bioinformatics*, vol. 39, no. 12, p. btad724, 2023.
- <span id="page-16-7"></span>[24] H.-Y. S. Chien, H. Goh, C. M. Sandino, and J. Y. Cheng, "Maeeg: Masked auto-encoder for eeg representation learning," *arXiv preprint arXiv:2211.02625*, 2022.
- <span id="page-16-8"></span>[25] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," *Advances in neural information processing systems*, vol. 33, pp. 6840–6851, 2020.
- <span id="page-16-9"></span>[26] Z. Guo, J. Liu, Y. Wang, M. Chen, D. Wang, D. Xu, and J. Cheng, "Diffusion models in bioinformatics and computational biology," *Nature reviews bioengineering*, vol. 2, no. 2, pp. 136–154, 2024.
- <span id="page-16-10"></span>[27] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang, "Diffusion models: A comprehensive survey of methods and applications," *ACM Computing Surveys*, vol. 56, no. 4, pp. 1–39, 2023.
- <span id="page-16-11"></span>[28] J. R. Faeder, M. L. Blinov, B. Goldstein, and W. S. Hlavacek, "Rule-based modeling of biochemical networks," *Complexity*, vol. 10, no. 4, pp. 22–41, 2005.
- <span id="page-16-12"></span>[29] M. Hwang, M. Garbey, S. A. Berceli, and R. Tran-Son-Tay, "Rule-based simulation of multi-cellular biological systems—a review of modeling techniques," *Cellular and molecular bioengineering*, vol. 2, pp. 285–294, 2009.
- <span id="page-16-13"></span>[30] J. R. Faeder, M. L. Blinov, and W. S. Hlavacek, "Rulebased modeling of biochemical systems with bionetgen," *Systems biology*, pp. 113–167, 2009.
- <span id="page-16-14"></span>[31] L. A. Chylek, L. A. Harris, J. R. Faeder, and W. S. Hlavacek, "Modeling for (physical) biologists: an introduction to the rule-based approach," *Physical biology*, vol. 12, no. 4, p. 045007, 2015.
- <span id="page-16-15"></span>[32] J. Willard, X. Jia, S. Xu, M. Steinbach, and V. Kumar, "Integrating physics-based modeling with machine learning: A survey," *arXiv preprint arXiv:2003.04919*, vol. 1,

no. 1, pp. 1–34, 2020.

- <span id="page-16-16"></span>[33] J. Newman, *Physics of the life sciences*. Springer Science & Business Media, 2008.
- <span id="page-16-17"></span>[34] K. Franklin, P. Muir, T. Scott, and P. Yates, *Introduction to biological physics for the health and life sciences*. John Wiley & Sons, 2019.
- <span id="page-16-18"></span>[35] K. Baverstock, "Life as physics and chemistry: A system view of biology," *Progress in Biophysics and Molecular Biology*, vol. 111, no. 2-3, pp. 108–115, 2013.
- <span id="page-16-19"></span>[36] B. Yelmen and F. Jay, "An overview of deep generative models in functional and evolutionary genomics," *Annual Review of Biomedical Data Science*, vol. 6, no. 1, pp. 173–189, 2023.
- <span id="page-16-20"></span>[37] D. M. Anstine and O. Isayev, "Generative models as an emerging paradigm in the chemical sciences," *Journal of the American Chemical Society*, vol. 145, no. 16, pp. 8736–8750, 2023.
- <span id="page-16-21"></span>[38] C. Bilodeau, W. Jin, T. Jaakkola, R. Barzilay, and K. F. Jensen, "Generative models for molecular discovery: Recent advances and challenges," *Wiley Interdisciplinary Reviews: Computational Molecular Science*, vol. 12, no. 5, p. e1608, 2022.
- <span id="page-16-22"></span>[39] D. Xue, Y. Gong, Z. Yang, G. Chuai, S. Qu, A. Shen, J. Yu, and Q. Liu, "Advances and challenges in deep generative models for de novo molecule generation," *Wiley Interdisciplinary Reviews: Computational Molecular Science*, vol. 9, no. 3, p. e1395, 2019.
- <span id="page-16-23"></span>[40] D. Fu and J. He, "DPPIN: A biological repository of dynamic protein-protein interaction network data," in *IEEE International Conference on Big Data, Big Data 2022, Osaka, Japan, December 17-20, 2022*, S. Tsumoto, Y. Ohsawa, L. Chen, D. V. den Poel, X. Hu, Y. Motomura, T. Takagi, L. Wu, Y. Xie, A. Abe, and V. Raghavan, Eds. IEEE, 2022, pp. 5269–5277. [Online]. Available: <https://doi.org/10.1109/BigData55660.2022.10020904>
- <span id="page-16-24"></span>[41] L. Zheng, B. Jing, Z. Li, Z. Zeng, T. Wei, M. Ai, X. He, L. Liu, D. Fu, J. You, H. Tong, and J. He, "Pyg-ssl: A graph self-supervised learning toolkit," *CoRR*, vol. abs/2412.21151, 2024. [Online]. Available: <https://doi.org/10.48550/arXiv.2412.21151>
- <span id="page-16-25"></span>[42] D. Fu, Y. Zhu, Z. Liu, L. Zheng, X. Lin, Z. Li, L. Fang, K. Tieu, O. Bhardwaj, K. Weldemariam, H. Tong, H. F. Hamann, and J. He, "Climatebench-m: A multi-modal climate data benchmark with a simple generative method," *CoRR*, vol. abs/2504.07394, 2025. [Online]. Available: <https://doi.org/10.48550/arXiv.2504.07394>
- <span id="page-16-26"></span>[43] L. Zheng, B. Jing, Z. Li, H. Tong, and J. He, "Heterogeneous contrastive learning for foundation models and beyond," in *Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024*, R. Baeza-Yates and F. Bonchi, Eds. ACM, 2024, pp. 6666–6676. [Online]. Available: <https://doi.org/10.1145/3637528.3671454>
- <span id="page-16-27"></span>[44] D. Fu, L. Fang, Z. Li, H. Tong, V. I. Torvik, and J. He, "Parametric graph representations in the era of foundation models: A survey and position," *CoRR*, vol. abs/2410.12126, 2024. [Online]. Available:

<https://doi.org/10.48550/arXiv.2410.12126>

- <span id="page-17-0"></span>[45] Y. Song, J. Gong, M. Xu, Z. Cao, Y. Lan, S. Ermon, H. Zhou, and W. Ma, "Equivariant flow matching with hybrid probability transport for 3d molecule generation," in *Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023. [Online]. Available: [http://papers.nips.cc/paper\\_files/paper/2023/hash/](http://papers.nips.cc/paper_files/paper/2023/hash/01d64478381c33e29ed611f1719f5a37-Abstract-Conference.html) [01d64478381c33e29ed611f1719f5a37-Abstract-Conferen](http://papers.nips.cc/paper_files/paper/2023/hash/01d64478381c33e29ed611f1719f5a37-Abstract-Conference.html)ce. [html](http://papers.nips.cc/paper_files/paper/2023/hash/01d64478381c33e29ed611f1719f5a37-Abstract-Conference.html)
- <span id="page-17-1"></span>[46] L. Klein, A. Krämer, and F. Noé, "Equivariant flow matching," *Advances in Neural Information Processing Systems*, vol. 36, pp. 59 886–59 910, 2023.
- <span id="page-17-2"></span>[47] A. J. Bose, T. Akhound-Sadegh, G. Huguet, K. Fatras, J. Rector-Brooks, C. Liu, A. C. Nica, M. Korablyov, M. M. Bronstein, and A. Tong, "Se(3)-stochastic flow matching for protein backbone generation," in *The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net, 2024. [Online]. Available: <https://openreview.net/forum?id=kJFIH23hXb>
- <span id="page-17-3"></span>[48] C. Cheng, J. Li, J. Peng, and G. Liu, "Categorical flow matching on statistical manifolds," in *Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024*, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: [http://papers.nips.cc/paper\\_files/paper/2024/hash/](http://papers.nips.cc/paper_files/paper/2024/hash/62a58f2130894e44e8a272c563a2c6f1-Abstract-Conference.html) [62a58f2130894e44e8a272c563a2c6f1-Abstract-Conferen](http://papers.nips.cc/paper_files/paper/2024/hash/62a58f2130894e44e8a272c563a2c6f1-Abstract-Conference.html)ce. [html](http://papers.nips.cc/paper_files/paper/2024/hash/62a58f2130894e44e8a272c563a2c6f1-Abstract-Conference.html)
- <span id="page-17-4"></span>[49] N. Kornilov, P. Mokrov, A. Gasnikov, and A. Korotin, "Optimal flow matching: Learning straight trajectories in just one step," *Advances in Neural Information Processing Systems*, vol. 37, pp. 104 180–104 204, 2024.
- <span id="page-17-5"></span>[50] R. T. Chen and Y. Lipman, "Flow matching on general geometries," *arXiv preprint arXiv:2302.03660*, 2023.
- <span id="page-17-6"></span>[51] H. Stark, B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola, "Dirichlet flow matching with applications to dna sequence design," *arXiv preprint arXiv:2402.05841*, 2024.
- <span id="page-17-7"></span>[52] M. S. Albergo and E. Vanden-Eijnden, "Building normalizing flows with stochastic interpolants," *arXiv preprint arXiv:2209.15571*, 2022.
- <span id="page-17-8"></span>[53] F. Eijkelboom, G. Bartosh, C. Andersson Naesseth, M. Welling, and J.-W. van de Meent, "Variational flow matching for graph generation," *Advances in Neural Information Processing Systems*, vol. 37, pp. 11 735– 11 764, 2024.
- <span id="page-17-9"></span>[54] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, "Flow matching for generative modeling," *arXiv preprint arXiv:2210.02747*, 2022.
- <span id="page-17-10"></span>[55] A. Tong, K. Fatras, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, G. Wolf, and Y. Bengio, "Im-

proving and generalizing flow-based generative models with minibatch optimal transport," *arXiv preprint arXiv:2302.00482*, 2023.

- <span id="page-17-11"></span>[56] S. Lee, Z. Lin, and G. Fanti, "Improving the training of rectified flows," *Advances in Neural Information Processing Systems*, vol. 37, pp. 63 082–63 109, 2024.
- <span id="page-17-12"></span>[57] X. Liu, C. Gong, and qiang liu, "Flow straight and fast: Learning to generate and transfer data with rectified flow," in *The Eleventh International Conference on Learning Representations*, 2023. [Online]. Available: <https://openreview.net/forum?id=XVjTT1nw5z>
- <span id="page-17-13"></span>[58] R. T. Chen and Y. Lipman, "Riemannian flow matching on general geometries," *arXiv e-prints*, pp. arXiv–2302, 2023.
- <span id="page-17-14"></span>[59] C. Cheng, J. Li, J. Fan, and G. Liu, "α-flow: A unified framework for continuous-state discrete flow matching models," *arXiv preprint arXiv:2504.10283*, 2025.
- <span id="page-17-15"></span>[60] O. Davis, S. Kessler, M. Petrache, I. Ceylan, M. Bronstein, and J. Bose, "Fisher flow matching for generative modeling over discrete data," *Advances in Neural Information Processing Systems*, vol. 37, pp. 139 054–139 084, 2024.
- <span id="page-17-16"></span>[61] A. Lou, D. Lim, I. Katsman, L. Huang, Q. Jiang, S. N. Lim, and C. M. De Sa, "Neural manifold ordinary differential equations," *Advances in Neural Information Processing Systems*, vol. 33, pp. 17 548–17 558, 2020.
- <span id="page-17-17"></span>[62] E. Mathieu and M. Nickel, "Riemannian continuous normalizing flows," *Advances in Neural Information Processing Systems*, vol. 33, pp. 2503–2515, 2020.
- <span id="page-17-18"></span>[63] A. Campbell, J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola, "Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design," *arXiv preprint arXiv:2402.04997*, 2024.
- <span id="page-17-19"></span>[64] Y. Qin, M. Madeira, D. Thanou, and P. Frossard, "Defog: Discrete flow matching for graph generation," *arXiv preprint arXiv:2410.04263*, 2024.
- <span id="page-17-20"></span>[65] N. Shaul, I. Gat, M. Havasi, D. Severo, A. Sriram, P. Holderrieth, B. Karrer, Y. Lipman, and R. T. Chen, "Flow matching with general discrete paths: A kineticoptimal perspective," *arXiv preprint arXiv:2412.03487*, 2024.
- <span id="page-17-21"></span>[66] I. Dunn and D. R. Koes, "Mixed continuous and categorical flow matching for 3d de novo molecule generation," *ArXiv*, pp. arXiv–2404, 2024.
- <span id="page-17-22"></span>[67] S. Tang, Y. Zhang, A. Tong, and P. Chatterjee, "Gumbelsoftmax flow matching with straight-through guidance for controllable biological sequence generation," *arXiv preprint arXiv:2503.17361*, 2025.
- <span id="page-17-23"></span>[68] L. Gao and Z. J. Lu, "Rnacg: A universal rna sequence conditional generation model based on flow-matching," *arXiv preprint arXiv:2407.19838*, 2024.
- <span id="page-17-24"></span>[69] D. Nori and W. Jin, "Rnaflow: Rna structure & sequence design via inverse folding-based flow matching," *arXiv preprint arXiv:2405.18768*, 2024.
- <span id="page-17-25"></span>[70] D. Rubin, A. d. S. Costa, M. Ponnapati, and J. Jacobson, "Ribogen: Rna sequence and structure co-generation with equivariant multiflow," *arXiv preprint arXiv:2503.02058*, 2025.

- <span id="page-18-0"></span>[71] D. Klein, T. Uscidda, F. Theis, and M. Cuturi, "Genot: Entropic (gromov) wasserstein flow matching with applications to single-cell genomics," *Advances in Neural Information Processing Systems*, vol. 37, pp. 103 897– 103 944, 2024.
- <span id="page-18-1"></span>[72] A. Palma, T. Richter, H. Zhang, A. Dittadi, and F. J. Theis, "cellflow: a generative flow-based model for single-cell count data," in *ICLR 2024 Workshop on Machine Learning for Genomics Explorations*.
- <span id="page-18-2"></span>[73] A. Palma, T. Richter, H. Zhang, M. Lubetzki, A. Tong, A. Dittadi, and F. J. Theis, "Multi-modal and multiattribute generation of single cells with cfgen," in *The Thirteenth International Conference on Learning Representations*.
- <span id="page-18-3"></span>[74] S. Nagaraj, A. Shanehsazzadeh, H. Park, J. King, and S. Levine, "Igflow: Flow matching for de novo antibody design," *Advances in Neural Information Processing Systems*, 2024.
- <span id="page-18-4"></span>[75] C. Tan, Y. Zhang, Z. Gao, Y. Huang, H. Lin, L. Wu, F. Wu, M. Blanchette, and S. Z. Li, "dyab: Flow matching for flexible antibody design with alphafolddriven pre-binding antigen," in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 39, no. 1, 2025, pp. 782–790.
- <span id="page-18-5"></span>[76] X. Hou, T. Zhu, M. Ren, D. Bu, X. Gao, C. Zhang, and S. Sun, "Improving molecular graph generation with flow matching and optimal transport," *CoRR*, vol. abs/2411.05676, 2024. [Online]. Available: <https://doi.org/10.48550/arXiv.2411.05676>
- <span id="page-18-6"></span>[77] F. Eijkelboom, G. Bartosh, C. A. Naesseth, M. Welling, and J. van de Meent, "Variational flow matching for graph generation," in *Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024*, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: [http://papers.nips.cc/paper\\_files/paper/2024/hash/](http://papers.nips.cc/paper_files/paper/2024/hash/15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conference.html) [15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conferen](http://papers.nips.cc/paper_files/paper/2024/hash/15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conference.html)ce. [html](http://papers.nips.cc/paper_files/paper/2024/hash/15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conference.html)
- <span id="page-18-7"></span>[78] Y. Qin, M. Madeira, D. Thanou, and P. Frossard, "Defog: Discrete flow matching for graph generation," *CoRR*, vol. abs/2410.04263, 2024. [Online]. Available: <https://doi.org/10.48550/arXiv.2410.04263>
- <span id="page-18-8"></span>[79] Q. Tian, Y. Xu, Y. Yang, Z. Wang, Z. Liu, P. Yan, and X. Li, "Equiflow: Equivariant conditional flow matching with optimal transport for 3d molecular conformation prediction," *CoRR*, vol. abs/2412.11082, 2024. [Online]. Available: <https://doi.org/10.48550/arXiv.2412.11082>
- <span id="page-18-9"></span>[80] D. Reidenbach, F. Nikitin, O. Isayev, and S. G. Paliwal, "Applications of modular co-design for de novo 3d molecule generation," in *NeurIPS 2024 Workshop on AI for New Drug Modalities*.
- <span id="page-18-10"></span>[81] F. Eijkelboom, H. Zimmermann, S. Vadgama, E. Bekkers, M. Welling, C. A. Naesseth, and J.-W. van de Meent, "Controlled generation with equivariant variational flow matching," in *Forty-second International Conference on*

*Machine Learning, ICML 2025*. OpenReview.net, 2025.

- <span id="page-18-11"></span>[82] H. Hong, W. Lin, and K. C. Tan, "Accelerating 3d molecule generation via jointly geometric optimal transport," in *The Thirteenth International Conference on Learning Representations*.
- <span id="page-18-12"></span>[83] R. Irwin, A. Tibo, J. P. Janet, and S. Olsson, "Efficient 3d molecular generation with flow matching and scale optimal transport," *CoRR*, vol. abs/2406.07266, 2024. [Online]. Available: [https://doi.org/10.48550/arXiv.2406.](https://doi.org/10.48550/arXiv.2406.07266) [07266](https://doi.org/10.48550/arXiv.2406.07266)
- <span id="page-18-13"></span>[84] ——, "Semlaflow–efficient 3d molecular generation with latent attention and equivariant flow matching," in *The 28th International Conference on Artificial Intelligence and Statistics*, 2025.
- <span id="page-18-14"></span>[85] Z. Cao, M. Geiger, A. D. S. Costa, D. Reidenbach, K. Kreis, T. Geffner, F. Pellegrini, G. Zhou, and E. Kucukbenli, "Efficient molecular conformer generation with so (3) averaged flow-matching and reflow."
- <span id="page-18-15"></span>[86] M. Hassan, N. Shenoy, J. Lee, H. Stärk, S. Thaler, and D. Beaini, "Et-flow: Equivariant flow-matching for molecular conformer generation," in *Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024*, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: [http://papers.nips.cc/paper\\_files/paper/2024/hash/](http://papers.nips.cc/paper_files/paper/2024/hash/e8bd617e7dd0394ceadf37b4a7773179-Abstract-Conference.html) [e8bd617e7dd0394ceadf37b4a7773179-Abstract-Conferen](http://papers.nips.cc/paper_files/paper/2024/hash/e8bd617e7dd0394ceadf37b4a7773179-Abstract-Conference.html)ce. [html](http://papers.nips.cc/paper_files/paper/2024/hash/e8bd617e7dd0394ceadf37b4a7773179-Abstract-Conference.html)
- <span id="page-18-16"></span>[87] R. Jiao, X. Kong, W. Huang, and Y. Liu, "3d structure prediction of atomic systems with flow-based direct preference optimization," in *Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024*, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: [http://papers.nips.cc/paper\\_files/paper/2024/hash/](http://papers.nips.cc/paper_files/paper/2024/hash/c6fdc94aeb2cb3a426d510d970045dab-Abstract-Conference.html) [c6fdc94aeb2cb3a426d510d970045dab-Abstract-Conferen](http://papers.nips.cc/paper_files/paper/2024/hash/c6fdc94aeb2cb3a426d510d970045dab-Abstract-Conference.html)ce. [html](http://papers.nips.cc/paper_files/paper/2024/hash/c6fdc94aeb2cb3a426d510d970045dab-Abstract-Conference.html)
- <span id="page-18-17"></span>[88] N. Isobe, M. Koyama, K. Hayashi, and K. Fukumizu, "Extended flow matching: a method of conditional generation with generalized continuity equation," *CoRR*, vol. abs/2402.18839, 2024. [Online]. Available: <https://doi.org/10.48550/arXiv.2402.18839>
- <span id="page-18-18"></span>[89] I. Dunn and D. R. Koes, "Mixed continuous and categorical flow matching for 3d de novo molecule generation," *CoRR*, vol. abs/2404.19739, 2024. [Online]. Available: <https://doi.org/10.48550/arXiv.2404.19739>
- <span id="page-18-19"></span>[90] W. Zhou, C. I. Sprague, V. Viliuga, M. Tadiello, A. Elofsson, and H. Azizpour, "Energy-based flow matching for generating 3d molecular structure," in *Forty-second International Conference on Machine Learning, ICML 2025*. OpenReview.net, 2025.
- <span id="page-18-20"></span>[91] L. Wang, C. Cheng, Y. Liao, Y. Qu, and G. Liu, "Training free guided flow matching with optimal

control," *CoRR*, vol. abs/2410.18070, 2024. [Online]. Available: <https://doi.org/10.48550/arXiv.2410.18070>

- <span id="page-19-9"></span>[92] Z. Zhang, M. Wang, and Q. Liu, "Flexsbdd: Structurebased drug design with flexible protein modeling," in *Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024*, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: [http://papers.nips.cc/paper\\_files/paper/2024/hash/](http://papers.nips.cc/paper_files/paper/2024/hash/60fb8cf8000f0386063fb24ead366330-Abstract-Conference.html) [60fb8cf8000f0386063fb24ead366330-Abstract-Conferenc](http://papers.nips.cc/paper_files/paper/2024/hash/60fb8cf8000f0386063fb24ead366330-Abstract-Conference.html)e. [html](http://papers.nips.cc/paper_files/paper/2024/hash/60fb8cf8000f0386063fb24ead366330-Abstract-Conference.html)
- <span id="page-19-10"></span>[93] Z. Li, C. Zhou, X. Wang, X. Peng, and M. Zhang, "Geometric representation condition improves equivariant molecule generation," *CoRR*, vol. abs/2410.03655, 2024. [Online]. Available: [https://doi.org/10.48550/arXiv.2410.](https://doi.org/10.48550/arXiv.2410.03655) [03655](https://doi.org/10.48550/arXiv.2410.03655)
- <span id="page-19-11"></span>[94] L. Vost, V. Chenthamarakshan, P. Das, and C. M. Deane, "Improving structural plausibility in 3d molecule generation via property-conditioned training with distorted molecules," *bioRxiv*, pp. 2024–09, 2024.
- <span id="page-19-12"></span>[95] A. H. Cheng, A. Lo, K. L. K. Lee, S. Miret, and A. Aspuru-Guzik, "Stiefel flow matching for moment-constrained structure elucidation," *CoRR*, vol. abs/2412.12540, 2024. [Online]. Available: [https:](https://doi.org/10.48550/arXiv.2412.12540) [//doi.org/10.48550/arXiv.2412.12540](https://doi.org/10.48550/arXiv.2412.12540)
- <span id="page-19-13"></span>[96] Y. Zhang, Y. Su, C. Wang, T. Li, Z. Wefers, J. Nirschl, J. Burgess, D. Ding, A. Lozano, E. Lundberg *et al.*, "Cellflow: Simulating cellular morphology changes via flow matching," *arXiv preprint arXiv:2502.09775*, 2025.
- <span id="page-19-14"></span>[97] K. Kapusniak, P. Potaptchik, T. Reu, L. Zhang, A. Tong, M. Bronstein, J. Bose, and F. Di Giovanni, "Metric flow matching for smooth interpolations on the data manifold," *Advances in Neural Information Processing Systems*, vol. 37, pp. 135 011–135 042, 2024.
- <span id="page-19-15"></span>[98] S. Shrestha and X. Fu, "Diversified flow matching with translation identifiability," in *Forty-second International Conference on Machine Learning, ICML 2025*. Open-Review.net, 2025.
- <span id="page-19-16"></span>[99] L. Bogensperger, D. Narnhofer, A. Falk, K. Schindler, and T. Pock, "Flowsdf: Flow matching for medical image segmentation using distance transforms," *CoRR*, vol. abs/2405.18087, 2024. [Online]. Available: [https:](https://doi.org/10.48550/arXiv.2405.18087) [//doi.org/10.48550/arXiv.2405.18087](https://doi.org/10.48550/arXiv.2405.18087)
- <span id="page-19-17"></span>[100] M. Yazdani, Y. Medghalchi, P. Ashrafian, I. Hacihaliloglu, and D. Shahriari, "Flow matching for medical image synthesis: Bridging the gap between speed and quality," *CoRR*, vol. abs/2503.00266, 2025. [Online]. Available: <https://doi.org/10.48550/arXiv.2503.00266>
- <span id="page-19-18"></span>[101] D. Zhang, Q. Han, Y. Xiong, and H. Du, "Mutlimodal straight flow matching for accelerated MR imaging," *Comput. Biol. Medicine*, vol. 178, p. 108668, 2024. [Online]. Available: [https://doi.org/10.1016/j.](https://doi.org/10.1016/j.compbiomed.2024.108668) [compbiomed.2024.108668](https://doi.org/10.1016/j.compbiomed.2024.108668)
- <span id="page-19-19"></span>[102] T. Huang, T. Liu, M. Babadi, W. Jin, and R. Ying, "Scalable generation of spatial transcriptomics from histology images via whole-slide flow matching," *arXiv*

*preprint arXiv:2506.05361*, 2025.

- <span id="page-19-20"></span>[103] D. Haviv, A.-A. Pooladian, D. Pe'er, and B. Amos, "Wasserstein flow matching: Generative modeling over families of distributions," *arXiv preprint arXiv:2411.00698*, 2024.
- <span id="page-19-21"></span>[104] G. Wei and L. Ma, "Stream-level flow matching with gaussian processes," in *Forty-second International Conference on Machine Learning*, 2025. [Online]. Available: <https://openreview.net/forum?id=qg9p1I5lmp>
- <span id="page-19-22"></span>[105] P. Wang, Y. Qi, Y. Wang, and G. Pan, "Flow matching for few-trial neural adaptation with stable latent dynamics," in *Forty-second International Conference on Machine Learning*, 2025. [Online]. Available: <https://openreview.net/forum?id=nKJEAQ6JCY>
- <span id="page-19-0"></span>[106] M. Ruth, B. Hannon, M. Ruth, and B. Hannon, *Modeling dynamic biological systems*. Springer, 1997.
- <span id="page-19-1"></span>[107] G. M. Edelman and J. A. Gally, "Degeneracy and complexity in biological systems," *Proceedings of the national academy of sciences*, vol. 98, no. 24, pp. 13 763– 13 768, 2001.
- <span id="page-19-2"></span>[108] J. W. Haefner, *Modeling biological systems:: principles and applications*. Springer Science & Business Media, 2005.
- <span id="page-19-3"></span>[109] A. Rhie, S. A. McCarthy, O. Fedrigo, J. Damas, G. Formenti, S. Koren, M. Uliano-Silva, W. Chow, A. Fungtammasan, J. Kim *et al.*, "Towards complete and error-free genome assemblies of all vertebrate species," *Nature*, vol. 592, no. 7856, pp. 737–746, 2021.
- <span id="page-19-4"></span>[110] J. L.-M. et al, "One thousand plant transcriptomes and the phylogenomics of green plants," *Nature*, vol. 574, no. 7780, pp. 679–685, 2019.
- <span id="page-19-5"></span>[111] D. Kim, J.-Y. Lee, J.-S. Yang, J. W. Kim, V. N. Kim, and H. Chang, "The architecture of sars-cov-2 transcriptome," *Cell*, vol. 181, no. 4, pp. 914–921, 2020.
- <span id="page-19-6"></span>[112] U. Sahin, K. Karikó, and Ö. Türeci, "mrna-based therapeutics—developing a new class of drugs," *Nature reviews Drug discovery*, vol. 13, no. 10, pp. 759–780, 2014.
- <span id="page-19-7"></span>[113] D. Baker and A. Sali, "Protein structure prediction and structural genomics," *Science*, vol. 294, no. 5540, pp. 93–96, 2001.
- <span id="page-19-8"></span>[114] M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong, L. N. Kinch, R. D. Schaeffer *et al.*, "Accurate prediction of protein structures and interactions using a three-track neural network," *Science*, vol. 373, no. 6557, pp. 871– 876, 2021.
- <span id="page-19-23"></span>[115] A. Jabbar, X. Li, and B. Omar, "A survey on generative adversarial networks: Variants, applications, and training," *ACM Computing Surveys (CSUR)*, vol. 54, no. 8, pp. 1–49, 2021.
- <span id="page-19-24"></span>[116] X. Xia, X. Pan, N. Li, X. He, L. Ma, X. Zhang, and N. Ding, "Gan-based anomaly detection: A review," *Neurocomputing*, vol. 493, pp. 497–535, 2022.
- <span id="page-19-25"></span>[117] J. G. Greener, S. M. Kandathil, L. Moffat, and D. T. Jones, "A guide to machine learning for biologists," *Nature reviews Molecular cell biology*, vol. 23, no. 1, pp. 40–55, 2022.

- <span id="page-20-0"></span>[118] P. Li, Y. Pei, and J. Li, "A comprehensive survey on design and application of autoencoder in deep learning," *Applied Soft Computing*, vol. 138, p. 110176, 2023.
- <span id="page-20-1"></span>[119] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, "Diffusion models in vision: A survey," *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 45, no. 9, pp. 10 850–10 869, 2023.
- <span id="page-20-2"></span>[120] S. Liang, Z. Pan, W. Liu, J. Yin, and M. De Rijke, "A survey on variational autoencoders in recommender systems," *ACM Computing Surveys*, vol. 56, no. 10, pp. 1–40, 2024.
- <span id="page-20-3"></span>[121] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z. Li, "A survey on generative diffusion models," *IEEE Transactions on Knowledge and Data Engineering*, 2024.
- <span id="page-20-4"></span>[122] M. M. Saad, R. O'Reilly, and M. H. Rehmani, "A survey on training challenges in generative adversarial networks for biomedical image analysis," *Artificial Intelligence Review*, vol. 57, no. 2, p. 19, 2024.
- <span id="page-20-5"></span>[123] X. Tang, H. Dai, E. Knight, F. Wu, Y. Li, T. Li, and M. Gerstein, "A survey of generative ai for de novo drug design: new frontiers in molecule and protein generation," *Briefings in Bioinformatics*, vol. 25, no. 4, p. bbae338, 2024.
- <span id="page-20-6"></span>[124] Y. Du, A. R. Jamasb, J. Guo, T. Fu, C. Harris, Y. Wang, C. Duan, P. Liò, P. Schwaller, and T. L. Blundell, "Machine learning-aided generative molecular design," *Nature Machine Intelligence*, vol. 6, no. 6, pp. 589–604, 2024.
- <span id="page-20-7"></span>[125] Q. Zhang, K. Ding, T. Lv, X. Wang, Q. Yin, Y. Zhang, J. Yu, Y. Wang, X. Li, Z. Xiang *et al.*, "Scientific large language models: A survey on biological & chemical domains," *ACM Computing Surveys*, vol. 57, no. 6, pp. 1–38, 2025.
- <span id="page-20-8"></span>[126] M. Mock, C. J. Langmead, P. Grandsard, S. Edavettal, and A. Russell, "Recent advances in generative biology for biotherapeutic discovery," *Trends in Pharmacological Sciences*, vol. 45, no. 3, pp. 255–267, 2024.
- <span id="page-20-9"></span>[127] D. B. Kell, S. Samanta, and N. Swainston, "Deep learning and generative methods in cheminformatics and chemical biology: navigating small molecule space intelligently," *Biochemical Journal*, vol. 477, no. 23, pp. 4559–4580, 2020.
- <span id="page-20-10"></span>[128] M. Liu, C. Li, R. Chen, D. Cao, and X. Zeng, "Geometric deep learning for drug discovery," *Expert Systems with Applications*, vol. 240, p. 122498, 2024.
- <span id="page-20-11"></span>[129] Z. Yang, X. Zeng, Y. Zhao, and R. Chen, "Alphafold2 and its applications in the fields of biology and medicine," *Signal Transduction and Targeted Therapy*, vol. 8, no. 1, p. 115, 2023.
- <span id="page-20-12"></span>[130] V. Mariani, M. Biasini, A. Barbato, and T. Schwede, "lddt: a local superposition-free score for comparing protein structures and models using distance difference tests," *Bioinformatics*, vol. 29, no. 21, pp. 2722–2728, 2013.
- <span id="page-20-13"></span>[131] H. Berman, K. Henrick, and H. Nakamura, "Announcing the worldwide protein data bank," *Nature structural & molecular biology*, vol. 10, no. 12, pp. 980–980, 2003.

- <span id="page-20-14"></span>[132] M. Shimoyama, J. De Pons, G. T. Hayman, S. J. Laulederkind, W. Liu, R. Nigam, V. Petri, J. R. Smith, M. Tutaj, S.-J. Wang *et al.*, "The rat genome database 2015: genomic, phenotypic and environmental variations and disease," *Nucleic acids research*, vol. 43, no. D1, pp. D743–D750, 2015.
- <span id="page-20-15"></span>[133] M. AlQuraishi, "Proteinnet: a standardized data set for machine learning of protein structure," *BMC bioinformatics*, vol. 20, pp. 1–10, 2019.
- <span id="page-20-16"></span>[134] S. Kim, P. A. Thiessen, E. E. Bolton, J. Chen, G. Fu, A. Gindulyte, L. Han, J. He, S. He, B. A. Shoemaker *et al.*, "Pubchem substance and compound databases," *Nucleic acids research*, vol. 44, no. D1, pp. D1202– D1213, 2016.
- <span id="page-20-17"></span>[135] D. P. Kingma, M. Welling *et al.*, "Auto-encoding variational bayes," 2013.
- <span id="page-20-18"></span>[136] ——, "An introduction to variational autoencoders," *Foundations and Trends® in Machine Learning*, vol. 12, no. 4, pp. 307–392, 2019.
- <span id="page-20-19"></span>[137] L. Girin, S. Leglaive, X. Bie, J. Diard, T. Hueber, and X. Alameda-Pineda, "Dynamical variational autoencoders: A comprehensive review," *arXiv preprint arXiv:2008.12595*, 2020.
- <span id="page-20-20"></span>[138] Y. Pu, Z. Gan, R. Henao, X. Yuan, C. Li, A. Stevens, and L. Carin, "Variational autoencoder for deep learning of images, labels and captions," *Advances in neural information processing systems*, vol. 29, 2016.
- <span id="page-20-21"></span>[139] M. J. Kusner, B. Paige, and J. M. Hernández-Lobato, "Grammar variational autoencoder," in *International conference on machine learning*. PMLR, 2017, pp. 1945–1954.
- <span id="page-20-22"></span>[140] G. Bredell, K. Flouris, K. Chaitanya, E. Erdil, and E. Konukoglu, "Explicitly minimizing the blur error of variational autoencoders," *arXiv preprint arXiv:2304.05939*, 2023.
- <span id="page-20-23"></span>[141] Y. Takida, W.-H. Liao, C.-H. Lai, T. Uesaka, S. Takahashi, and Y. Mitsufuji, "Preventing oversmoothing in vae via generalized variance parameterization," *Neurocomputing*, vol. 509, pp. 137–156, 2022.
- <span id="page-20-24"></span>[142] B. Dai, Z. Wang, and D. Wipf, "The usual suspects? reassessing blame for vae posterior collapse," in *International conference on machine learning*. PMLR, 2020, pp. 2313–2322.
- <span id="page-20-25"></span>[143] M. Arjovsky, S. Chintala, and L. Bottou, "Wasserstein GAN," *CoRR*, vol. abs/1701.07875, 2017. [Online]. Available: <http://arxiv.org/abs/1701.07875>
- <span id="page-20-26"></span>[144] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley, "Least squares generative adversarial networks," in *Proceedings of the IEEE international conference on computer vision*, 2017, pp. 2794–2802.
- <span id="page-20-27"></span>[145] M. Mirza and S. Osindero, "Conditional generative adversarial nets," *arXiv preprint arXiv:1411.1784*, 2014.
- <span id="page-20-28"></span>[146] S. M. Bafti, C. S. Ang, G. Marcelli, M. M. Hossain, S. Maxamhud, and A. D. Tsaousis, "Biogan: An unpaired gan-based image to image translation model for microbiological images," *arXiv preprint arXiv:2306.06217*, 2023.
- <span id="page-20-29"></span>[147] P. Chaudhari, H. Agrawal, and K. Kotecha, "Data

augmentation using mg-gan for improved cancer classification on gene expression data," *Soft Computing*, vol. 24, pp. 11 381–11 391, 2020.

- <span id="page-21-0"></span>[148] H. Yang, Z. Xiang, X. Li, and W. Zhang, "An improved gan-based data augmentation model for addressing data scarcity in srms," *Measurement Science and Technology*, vol. 36, no. 2, p. 026129, 2025.
- <span id="page-21-1"></span>[149] A. Osokin, A. Chessel, R. E. Carazo Salas, and F. Vaggi, "Gans for biological image synthesis," in *Proceedings of the IEEE international conference on computer vision*, 2017, pp. 2233–2242.
- <span id="page-21-2"></span>[150] D. Rezende and S. Mohamed, "Variational inference with normalizing flows," in *International conference on machine learning*. PMLR, 2015, pp. 1530–1538.
- <span id="page-21-3"></span>[151] I. Kobyzev, S. J. Prince, and M. A. Brubaker, "Normalizing flows: An introduction and review of current methods," *IEEE transactions on pattern analysis and machine intelligence*, vol. 43, no. 11, pp. 3964–3979, 2020.
- <span id="page-21-4"></span>[152] L. Dinh, D. Krueger, and Y. Bengio, "NICE: nonlinear independent components estimation," in *3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings*, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: [http://arxiv.org/abs/](http://arxiv.org/abs/1410.8516) [1410.8516](http://arxiv.org/abs/1410.8516)
- <span id="page-21-5"></span>[153] L. Dinh, J. Sohl-Dickstein, and S. Bengio, "Density estimation using real NVP," in *5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net, 2017. [Online]. Available: <https://openreview.net/forum?id=HkpbnH9lx>
- <span id="page-21-6"></span>[154] D. P. Kingma and P. Dhariwal, "Glow: Generative flow with invertible 1x1 convolutions," in *Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*, S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., 2018, pp. 10 236–10 245. [Online]. Available: [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html) [d139db6a236200b21cc7f752979132d0-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html)
- <span id="page-21-7"></span>[155] G. Papamakarios, I. Murray, and T. Pavlakou, "Masked autoregressive flow for density estimation," in *Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 2338–2347. [Online]. Available: [https://proceedings.neurips.cc/paper/2017/hash/](https://proceedings.neurips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html) [6c1da886822c67822bcf3679d04369fa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html)
- <span id="page-21-8"></span>[156] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, "Neural ordinary differential equations," *Advances in neural information processing systems*, vol. 31, 2018.
- <span id="page-21-9"></span>[157] J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli, "Deep unsupervised learning using nonequilibrium thermodynamics," in *Proceedings of*

*the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015*, ser. JMLR Workshop and Conference Proceedings, F. R. Bach and D. M. Blei, Eds., vol. 37. JMLR.org, 2015, pp. 2256–2265. [Online]. Available: <http://proceedings.mlr.press/v37/sohl-dickstein15.html>

- <span id="page-21-10"></span>[158] Y. Song and S. Ermon, "Generative modeling by estimating gradients of the data distribution," in *Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada*, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 11 895–11 907. [Online]. Available: [https://proceedings.neurips.cc/paper/2019/hash/](https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html) [3001ef257407d5a371a96dcd947c7d93-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html)
- <span id="page-21-11"></span>[159] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, "Score-based generative modeling through stochastic differential equations," in *9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net, 2021. [Online]. Available: <https://openreview.net/forum?id=PxTIG12RRHS>
- <span id="page-21-12"></span>[160] J. Song, C. Meng, and S. Ermon, "Denoising diffusion implicit models," in *9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net, 2021. [Online]. Available: [https://openreview.net/forum?id=](https://openreview.net/forum?id=St1giarCHLP) [St1giarCHLP](https://openreview.net/forum?id=St1giarCHLP)
- <span id="page-21-13"></span>[161] A. Campbell, J. Benton, V. D. Bortoli, T. Rainforth, G. Deligiannidis, and A. Doucet, "A continuous time framework for discrete denoising models," in *Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022*, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. [Online]. Available: [http://papers.nips.cc/paper\\_files/paper/2022/hash/](http://papers.nips.cc/paper_files/paper/2022/hash/b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conference.html) [b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conferenc](http://papers.nips.cc/paper_files/paper/2022/hash/b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conference.html)e. [html](http://papers.nips.cc/paper_files/paper/2022/hash/b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conference.html)
- <span id="page-21-14"></span>[162] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. van den Berg, "Structured denoising diffusion models in discrete state-spaces," in *Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual*, M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 17 981–17 993. [Online]. Available: [https://proceedings.neurips.cc/paper/2021/hash/](https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html) [958c530554f78bcd8e97125b70e6973d-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html)
- <span id="page-21-15"></span>[163] T. Salimans and J. Ho, "Progressive distillation for fast sampling of diffusion models," in *The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022*. OpenReview.net, 2022. [Online]. Available: [https://openreview.net/forum?](https://openreview.net/forum?id=TIdIXIpzhoI)

[id=TIdIXIpzhoI](https://openreview.net/forum?id=TIdIXIpzhoI)

- <span id="page-22-0"></span>[164] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, "Dpm-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps," in *Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022*, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. [Online]. Available: [http://papers.nips.cc/paper\\_files/paper/2022/hash/](http://papers.nips.cc/paper_files/paper/2022/hash/260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html) [260a14acce2a89dad36adc8eefe7c59e-Abstract-Conferenc](http://papers.nips.cc/paper_files/paper/2022/hash/260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html)e. [html](http://papers.nips.cc/paper_files/paper/2022/hash/260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html)
- <span id="page-22-1"></span>[165] Y. Lipman, M. Havasi, P. Holderrieth, N. Shaul, M. Le, B. Karrer, R. T. Chen, D. Lopez-Paz, H. Ben-Hamu, and I. Gat, "Flow matching guide and code," *arXiv preprint arXiv:2412.06264*, 2024.
- <span id="page-22-2"></span>[166] X. Liu, C. Gong, and Q. Liu, "Flow straight and fast: Learning to generate and transfer data with rectified flow," *arXiv preprint arXiv:2209.03003*, 2022.
- <span id="page-22-3"></span>[167] I. Dunn and D. R. Koes, "Exploring discrete flow matching for 3d de novo molecule generation," *ArXiv*, pp. arXiv–2411, 2024.
- <span id="page-22-4"></span>[168] S. Tarafder and D. Bhattacharya, "Rnabpflow: Base pairaugmented se (3)-flow matching for conditional rna 3d structure generation," *bioRxiv*, pp. 2025–01, 2025.
- <span id="page-22-5"></span>[169] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling, "Equivariant diffusion for molecule generation in 3d," in *International conference on machine learning*. PMLR, 2022, pp. 8867–8887.
- <span id="page-22-6"></span>[170] Q. Liu, M. Allamanis, M. Brockschmidt, and A. L. Gaunt, "Constrained graph variational autoencoders for molecule design," in *Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*, S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., 2018, pp. 7806–7815. [Online]. Available: [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/b8a03c5c15fcfa8dae0b03351eb1742f-Abstract.html) [b8a03c5c15fcfa8dae0b03351eb1742f-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/b8a03c5c15fcfa8dae0b03351eb1742f-Abstract.html)
- <span id="page-22-7"></span>[171] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard, "Digress: Discrete denoising diffusion for graph generation," in *The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net, 2023. [Online]. Available: [https://openreview.net/forum?](https://openreview.net/forum?id=UaAD-Nu86WX) [id=UaAD-Nu86WX](https://openreview.net/forum?id=UaAD-Nu86WX)
- <span id="page-22-8"></span>[172] S. Luo, J. Guan, J. Ma, and J. Peng, "A 3d generative model for structure-based drug design," in *Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual*, M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 6229–6239. [Online]. Available: [https://proceedings.neurips.cc/paper/2021/hash/](https://proceedings.neurips.cc/paper/2021/hash/314450613369e0ee72d0da7f6fee773c-Abstract.html) [314450613369e0ee72d0da7f6fee773c-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/314450613369e0ee72d0da7f6fee773c-Abstract.html)

- <span id="page-22-9"></span>[173] X. Peng, S. Luo, J. Guan, Q. Xie, J. Peng, and J. Ma, "Pocket2mol: Efficient molecular sampling based on 3d protein pockets," in *International conference on machine learning*. PMLR, 2022, pp. 17 644–17 655.
- <span id="page-22-10"></span>[174] F. Noé, A. Tkatchenko, K.-R. Müller, and C. Clementi, "Machine learning for molecular simulation," *Annual review of physical chemistry*, vol. 71, no. 1, pp. 361– 390, 2020.
- <span id="page-22-11"></span>[175] S. A. Hollingsworth and R. O. Dror, "Molecular dynamics simulation for all," *Neuron*, vol. 99, no. 6, pp. 1129–1143, 2018.
- <span id="page-22-12"></span>[176] W. P. Walters and R. Barzilay, "Applications of deep learning in molecule generation and molecular property prediction," *Accounts of chemical research*, vol. 54, no. 2, pp. 263–270, 2020.
- <span id="page-22-13"></span>[177] Y. Du, A. R. Jamasb, J. Guo, T. Fu, C. Harris, Y. Wang, C. Duan, P. Liò, P. Schwaller, and T. L. Blundell, "Machine learning-aided generative molecular design," *Nat. Mac. Intell.*, vol. 6, no. 6, pp. 589– 604, 2024. [Online]. Available: [https://doi.org/10.1038/](https://doi.org/10.1038/s42256-024-00843-5) [s42256-024-00843-5](https://doi.org/10.1038/s42256-024-00843-5)
- <span id="page-22-14"></span>[178] S. Axelrod and R. Gomez-Bombarelli, "Geom, energyannotated molecular conformations for property prediction and molecular generation," *Scientific Data*, vol. 9, no. 1, p. 185, 2022.
- <span id="page-22-15"></span>[179] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. Von Lilienfeld, "Quantum chemistry structures and properties of 134 kilo molecules," *Scientific data*, vol. 1, no. 1, pp. 1–7, 2014.
- <span id="page-22-16"></span>[180] Z. Guo, K. Guo, B. Nan, Y. Tian, R. G. Iyer, Y. Ma, O. Wiest, X. Zhang, W. Wang, C. Zhang, and N. V. Chawla, "Graph-based molecular representation learning," in *Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China*. ijcai.org, 2023, pp. 6638–6646. [Online]. Available: <https://doi.org/10.24963/ijcai.2023/744>
- <span id="page-22-17"></span>[181] N. De Cao and T. Kipf, "Molgan: An implicit generative model for small molecular graphs," *arXiv preprint arXiv:1805.11973*, 2018.
- <span id="page-22-18"></span>[182] Y. Li, L. Zhang, and Z. Liu, "Multi-objective de novo drug design with conditional graph generative model," *Journal of cheminformatics*, vol. 10, pp. 1–24, 2018.
- <span id="page-22-19"></span>[183] B. Baillif, J. Cole, P. McCabe, and A. Bender, "Deep generative models for 3d molecular structure," *Current Opinion in Structural Biology*, vol. 80, p. 102566, 2023.
- <span id="page-22-20"></span>[184] X. Peng, J. Guan, Q. Liu, and J. Ma, "Moldiff: Addressing the atom-bond inconsistency problem in 3d molecule diffusion generation," in *International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA*, ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202. PMLR, 2023, pp. 27 611–27 629. [Online]. Available: [https://proceedings.mlr.press/v202/peng23b.](https://proceedings.mlr.press/v202/peng23b.html) [html](https://proceedings.mlr.press/v202/peng23b.html)
- <span id="page-22-21"></span>[185] L. Huang, H. Zhang, T. Xu, and K. Wong, "MDM: molecular diffusion model for 3d molecule generation,"

in *Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023*, B. Williams, Y. Chen, and J. Neville, Eds. AAAI Press, 2023, pp. 5105–5112. [Online]. Available: [https://doi.org/10.1609/aaai.v37i4.](https://doi.org/10.1609/aaai.v37i4.25639) [25639](https://doi.org/10.1609/aaai.v37i4.25639)

- <span id="page-23-0"></span>[186] W. Zhou, C. I. Sprague, and H. Azizpour, "Energy-based flow matching for molecular docking," 2025.
- <span id="page-23-1"></span>[187] J. Huang and D. Zhang, "Molform: Multi-modal flow matching for structure-based drug design," *arXiv preprint arXiv:2507.05503*, 2025.
- <span id="page-23-2"></span>[188] Y. Qu, K. Qiu, Y. Song, J. Gong, J. Han, M. Zheng, H. Zhou, and W.-Y. Ma, "Molcraft: Structure-based drug design in continuous parameter space," in *Forty-first International Conference on Machine Learning*.
- <span id="page-23-3"></span>[189] K. Xue, Y. Zhou, S. Nie, X. Min, X. Zhang, J. Zhou, and C. Li, "Unifying bayesian flow networks and diffusion models through stochastic differential equations," in *International Conference on Machine Learning*. PMLR, 2024, pp. 55 656–55 681.
- <span id="page-23-4"></span>[190] X. Peng, R. Guo, Y. Xu, J. Guan, Y. Jia, Y. Huang, M. Zhang, J. Peng, J. Sun, C. Han *et al.*, "Decipher fundamental atomic interactions to unify generative molecular docking and design," *bioRxiv*, pp. 2024–10, 2024.
- <span id="page-23-5"></span>[191] J. Yim, A. Campbell, A. Y. Foong, M. Gastegger, J. Jiménez-Luna, S. Lewis, V. G. Satorras, B. S. Veeling, R. Barzilay, T. Jaakkola *et al.*, "Fast protein backbone generation with se (3) flow matching," *arXiv preprint arXiv:2310.05297*, 2023.
- <span id="page-23-6"></span>[192] A. J. Bose, T. Akhound-Sadegh, G. Huguet, K. Fatras, J. Rector-Brooks, C.-H. Liu, A. C. Nica, M. Korablyov, M. Bronstein, and A. Tong, "Se (3)-stochastic flow matching for protein backbone generation," *arXiv preprint arXiv:2310.02391*, 2023.
- <span id="page-23-7"></span>[193] J. Yan, Z. Cui, W. Yan, Y. Chen, M. Pu, S. Li, and S. Ye, "Robust and reliable de novo protein design: A flow-matching-based protein generative model achieves remarkably high success rates," *bioRxiv*, pp. 2025–04, 2025.
- <span id="page-23-8"></span>[194] S. Wagner, L. Seute, V. Viliuga, N. Wolf, F. Gräter, and J. Stühmer, "Generating highly designable proteins with geometric algebra flow matching," *arXiv preprint arXiv:2411.05238*, 2024.
- <span id="page-23-9"></span>[195] G. Huguet, J. Vuckovic, K. Fatras, E. Thibodeau-Laufer, P. Lemos, R. Islam, C.-H. Liu, J. Rector-Brooks, T. Akhound-Sadegh, M. Bronstein *et al.*, "Sequenceaugmented se (3)-flow matching for conditional protein backbone generation," *arXiv preprint arXiv:2405.20313*, 2024.
- <span id="page-23-10"></span>[196] T. Geffner, K. Didi, Z. Zhang, D. Reidenbach, Z. Cao, J. Yim, M. Geiger, C. Dallago, E. Kucukbenli, A. Vahdat *et al.*, "Proteina: Scaling flow-based protein structure generative models," *arXiv preprint arXiv:2503.00710*, 2025.

- <span id="page-23-11"></span>[197] H. Stark, B. Jing, T. Geffner, J. Yim, T. Jaakkola, A. Vahdat, and K. Kreis, "Protcomposer: Compositional protein structure generation with 3d ellipsoids," *arXiv preprint arXiv:2503.05025*, 2025.
- <span id="page-23-12"></span>[198] S. Yang, L. Ju, P. Cheng, J. Zhou, Y. Cai, and D. Feng, "Co-design protein sequence and structure in discrete space via generative flow," *Bioinformatics*, vol. 41, no. 5, p. btaf248, 2025.
- <span id="page-23-13"></span>[199] R. Chen, D. Xue, X. Zhou, Z. Zheng, X. Zeng, and Q. Gu, "An all-atom generative model for designing protein complexes," *arXiv preprint arXiv:2504.13075*, 2025.
- <span id="page-23-14"></span>[200] J. Yim, A. Campbell, E. Mathieu, A. Y. Foong, M. Gastegger, J. Jiménez-Luna, S. Lewis, V. G. Satorras, B. S. Veeling, F. Noé *et al.*, "Improved motif-scaffolding with se (3) flow matching," *ArXiv*, pp. arXiv–2401, 2024.
- <span id="page-23-15"></span>[201] Y. Huang, Y. Liu, L. Wu, H. Lin, C. Tan, O. Zhang, Z. Gao, S. Li, Z. Liu, Y. Liu *et al.*, "Eva: Geometric inverse design for fast protein motif-scaffolding with coupled flow," in *The Thirteenth International Conference on Learning Representations*.
- <span id="page-23-16"></span>[202] J. Liu, S. Li, C. Shi, Z. Yang, and J. Tang, "Design of ligand-binding proteins with atomic flow matching," *arXiv preprint arXiv:2409.12080*, 2024.
- <span id="page-23-17"></span>[203] H. Stark, B. Jing, R. Barzilay, and T. Jaakkola, "Harmonic self-conditioned flow matching for joint multiligand docking and binding site design," in *Forty-first International Conference on Machine Learning*, 2024.
- <span id="page-23-18"></span>[204] Z. Zhang, M. Zitnik, and Q. Liu, "Generalized protein pocket generation with prior-informed flow matching," *arXiv preprint arXiv:2409.19520*, 2024.
- <span id="page-23-19"></span>[205] M. S. Jones, S. Khanna, and A. L. Ferguson, "Flowback: A generalized flow-matching approach for biomolecular backmapping," *J. Chem. Inf. Model.*, vol. 65, no. 2, pp. 672–692, 2025. [Online]. Available: <https://doi.org/10.1021/acs.jcim.4c02046>
- <span id="page-23-20"></span>[206] B. Jing, B. Berger, and T. Jaakkola, "Alphafold meets flow matching for generating protein ensembles," *arXiv preprint arXiv:2402.04845*, 2024.
- <span id="page-23-21"></span>[207] Y. Jin, Q. Huang, Z. Song, M. Zheng, D. Teng, and Q. Shi, "P2dflow: A protein ensemble generative model with se (3) flow matching," *Journal of Chemical Theory and Computation*, vol. 21, no. 6, pp. 3288–3296, 2025.
- <span id="page-23-22"></span>[208] J. S. Lee and P. M. Kim, "Flowpacker: Protein side-chain packing with torsional flow matching," *Bioinformatics*, p. btaf010, 2025.
- <span id="page-23-23"></span>[209] Y.-L. Liao, B. Wood, A. Das, and T. Smidt, "Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations," *arXiv preprint arXiv:2306.12059*, 2023.
- <span id="page-23-24"></span>[210] J. Li, C. Cheng, Z. Wu, R. Guo, S. Luo, Z. Ren, J. Peng, and J. Ma, "Full-atom peptide design based on multimodal flow matching," *arXiv preprint arXiv:2406.00735*, 2024.
- <span id="page-23-25"></span>[211] A. Morehead and J. Cheng, "Flowdock: Geometric flow matching for generative protein-ligand docking and affinity prediction," *ArXiv*, pp. arXiv–2412, 2025.
- <span id="page-23-26"></span>[212] H. Lin, O. Zhang, H. Zhao, D. Jiang, L. Wu, Z. Liu,

Y. Huang, and S. Z. Li, "Ppflow: Target-aware peptide design with torsional flow matching," *bioRxiv*, pp. 2024– 03, 2024.

- <span id="page-24-0"></span>[213] D. Huang and S. Tu, "Non-linear flow matching for fullatom peptide design," *arXiv preprint arXiv:2502.15855*, 2025.
- <span id="page-24-1"></span>[214] Z. Kong, Y. Zhu, Y. Xu, H. Zhou, M. Yin, J. Wu, H. Xu, C.-Y. Hsieh, T. Hou, and J. Wu, "Protflow: Fast protein sequence design via flow matching on compressed protein language model embeddings," *arXiv preprint arXiv:2504.10983*, 2025.
- <span id="page-24-2"></span>[215] J. Zhang, Z. Liu, S. Bai, H. Cao, Y. Li, and L. Zhang, "Efficient antibody structure refinement using energyguided SE(3) flow matching," in *IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2024, Lisbon, Portugal, December 3-6, 2024*, M. Cannataro, H. J. Zheng, L. Gao, J. Cheng, J. L. de Miranda, E. Zumpano, X. Hu, Y. Cho, and T. Park, Eds. IEEE, 2024, pp. 146–153. [Online]. Available: <https://doi.org/10.1109/BIBM62325.2024.10821730>
- <span id="page-24-3"></span>[216] S. Griffiths-Jones, A. Bateman, M. Marshall, A. Khanna, and S. R. Eddy, "Rfam: an rna family database," *Nucleic Acids Research*, vol. 31, no. 1, pp. 439–441, 2003. [Online]. Available: <https://doi.org/10.1093/nar/gkg006>
- <span id="page-24-4"></span>[217] Y. Chu, D. Yu, Y. Li, K. Huang, Y. Shen, L. Cong, J. Zhang, and M. Wang, "A 5' utr language model for decoding untranslated regions of mrna and function predictions," *Nature Machine Intelligence*, vol. 6, no. 4, pp. 449–460, 2024. [Online]. Available: <https://doi.org/10.1038/s42256-024-00823-9>
- <span id="page-24-5"></span>[218] B. Adamczyk, M. Antczak, and M. Szachniuk, "Rnasolo: a repository of cleaned pdb-derived rna 3d structures," *Bioinformatics*, vol. 38, no. 14, pp. 3668–3670, 2022.
- <span id="page-24-6"></span>[219] A. Bastidas-Ponce, S. Tritschler, L. Dony, K. Scheibner, M. Tarquis-Medina, C. Salinno, S. Schirge, I. Burtscher, A. Böttcher, F. J. Theis *et al.*, "Comprehensive single cell mrna profiling reveals a detailed roadmap for pancreatic endocrinogenesis," *Development*, vol. 146, no. 12, p. dev173849, 2019.
- <span id="page-24-7"></span>[220] S. R. Srivatsan, J. L. McFaline-Figueroa, V. Ramani, L. Saunders, J. Cao, J. Packer, H. A. Pliner, D. L. Jackson, R. M. Daza, L. Christiansen *et al.*, "Massively multiplex chemical transcriptomics at single-cell resolution," *Science*, vol. 367, no. 6473, pp. 45–51, 2020.
- <span id="page-24-8"></span>[221] M. D. Luecken, D. B. Burkhardt, R. Cannoodt, C. Lance, A. Agrawal, H. Aliee, A. T. Chen, L. Deconinck, A. M. Detweiler, A. A. Granados *et al.*, "A sandbox for prediction and integration of dna, rna, and proteins in single cells," in *Thirty-fifth conference on neural information processing systems datasets and benchmarks track (Round 2)*, 2021.
- <span id="page-24-9"></span>[222] C. Derbois, M.-A. Palomares, J.-F. Deleuze, E. Cabannes, and E. Bonnet, "Single cell transcriptome sequencing of stimulated and frozen human peripheral blood mononuclear cells," *Scientific Data*, vol. 10, no. 1, p. 433, 2023.
- <span id="page-24-10"></span>[223] G. La Manno, R. Soldatov, A. Zeisel, E. Braun, H. Hochgerner, V. Petukhov, K. Lidschreiber, M. E. Kastriti, P. Lönnerberg, A. Furlan *et al.*, "Rna velocity

of single cells," *Nature*, vol. 560, no. 7719, pp. 494–498, 2018.

- <span id="page-24-11"></span>[224] L. Sikkema, C. Ramírez-Suástegui, D. C. Strobl, T. E. Gillett, L. Zappia, E. Madissoon, N. S. Markov, L.-E. Zaragosi, Y. Ji, M. Ansari *et al.*, "An integrated cell atlas of the lung in health and disease," *Nature medicine*, vol. 29, no. 6, pp. 1563–1577, 2023.
- <span id="page-24-12"></span>[225] F. A. Vieira Braga, G. Kar, M. Berg, O. A. Carpaij, K. Polanski, L. M. Simon, S. Brouwer, T. Gomes, L. Hesse, J. Jiang *et al.*, "A cellular census of human lungs identifies novel cell states in health and in asthma," *Nature medicine*, vol. 25, no. 7, pp. 1153–1163, 2019.
- <span id="page-24-13"></span>[226] N. Schaum, J. Karkanias, N. F. Neff, A. P. May, S. R. Quake, T. Wyss-Coray, S. Darmanis, J. Batson, O. Botvinnik, M. B. Chen *et al.*, "Single-cell transcriptomics of 20 mouse organs creates a tabula muris: The tabula muris consortium," *Nature*, vol. 562, no. 7727, p. 367, 2018.
- <span id="page-24-14"></span>[227] K. R. Moon, D. van Dijk, Z. Wang, S. Gigante, D. B. Burkhardt, W. S. Chen, K. Yim, A. van den Elzen, M. J. Hirn, R. R. Coifman, N. B. Ivanova, G. Wolf, and S. Krishnaswamy, "Visualizing structure and transitions in high-dimensional biological data," *Nature Biotechnology*, vol. 37, no. 12, pp. 1482–1492, 2019.
- <span id="page-24-15"></span>[228] C. Lance, M. D. Luecken, D. B. Burkhardt, R. Cannoodt, P. Rautenstrauch, A. Laddach, A. Ubingazhibov, Z.-J. Cao, K. Deng, S. Khan, Q. Liu, N. Russkikh, G. Ryazantsev, U. Ohler, A. O. Pisco, J. Bloom, S. Krishnaswamy, and F. J. Theis, "Multimodal single cell data integration challenge: Results and lessons learned," *bioRxiv*, 2022.
- <span id="page-24-16"></span>[229] J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman, "Zinc: a free tool to discover chemistry for biology," *Journal of chemical information and modeling*, vol. 52, no. 7, pp. 1757–1768, 2012.
- <span id="page-24-17"></span>[230] N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher, "Guacamol: benchmarking models for de novo molecular design," *Journal of chemical information and modeling*, vol. 59, no. 3, pp. 1096–1108, 2019.
- <span id="page-24-18"></span>[231] D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy, M. Veselov *et al.*, "Molecular sets (moses): a benchmarking platform for molecular generation models," *Frontiers in pharmacology*, vol. 11, p. 565644, 2020.
- <span id="page-24-19"></span>[232] M. Buttenschoen, G. M. Morris, and C. M. Deane, "Posebusters: Ai-based docking methods fail to generate physically valid poses or generalise to novel sequences," *Chemical Science*, vol. 15, pp. 3130– 3139, 2024. [Online]. Available: [https://doi.org/10.1039/](https://doi.org/10.1039/D3SC04185A) [D3SC04185A](https://doi.org/10.1039/D3SC04185A)
- <span id="page-24-20"></span>[233] J. Dunbar, K. Krawczyk, J. Leem, T. Baker, A. Fuchs, G. Georges, J. Shi, and C. M. Deane, "Sabdab: the structural antibody database," *Nucleic Acids Research*, vol. 42, no. D1, pp. D1140–D1146, 2014. [Online]. Available: <https://doi.org/10.1093/nar/gkt1043>
- <span id="page-24-21"></span>[234] J. Wu, X. Kong, N. Sun, J. Wei, S. Shan, F. Feng, F. Wu, J. Peng, L. Zhang, Y. Liu *et al.*, "Flowdesign: Improved design of antibody cdrs through flow matching and better

prior distributions," *Cell Systems*, 2025.

- <span id="page-25-0"></span>[235] L. Hu, M. L. Benson, R. D. Smith, M. G. Lerner, and H. A. Carlson, "Binding moad (mother of all databases)," *Proteins: Structure, Function, and Bioinformatics*, vol. 60, no. 3, pp. 333–340, 2005.
- <span id="page-25-1"></span>[236] P. G. Francoeur, T. Masuda, J. Sunseri, A. Jia, R. B. Iovanisci, I. Snyder, and D. R. Koes, "Three-dimensional convolutional neural networks and a cross-docked data set for structure-based drug design," *Journal of chemical information and modeling*, vol. 60, no. 9, pp. 4200–4215, 2020.
- <span id="page-25-2"></span>[237] R. Wang, X. Fang, Y. Lu, and S. Wang, "The pdbbind database: Collection of binding affinities for proteinligand complexes with known three-dimensional structures," *Journal of medicinal chemistry*, vol. 47, no. 12, pp. 2977–2980, 2004.
- <span id="page-25-3"></span>[238] P. Agrawal, H. Singh, H. K. Srivastava, S. Singh, G. Kishore, and G. P. Raghava, "Benchmarking of different molecular docking methods for protein-peptide docking," *BMC bioinformatics*, vol. 19, pp. 105–124, 2019.
- <span id="page-25-4"></span>[239] B. E. Suzek, Y. Wang, H. Huang, P. B. McGarvey, C. H. Wu, and U. Consortium, "Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches," *Bioinformatics*, vol. 31, no. 6, pp. 926–932, 2015.
- <span id="page-25-5"></span>[240] H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N. Shindyalov, and P. E. Bourne, "The protein data bank," *Nucleic acids research*, vol. 28, no. 1, pp. 235–242, 2000.
- <span id="page-25-6"></span>[241] J. Yim, B. L. Trippe, V. De Bortoli, E. Mathieu, A. Doucet, R. Barzilay, and T. Jaakkola, "Se (3) diffusion model with application to protein backbone generation," *arXiv preprint arXiv:2302.02277*, 2023.
- <span id="page-25-7"></span>[242] M. S. Jones, S. Khanna, and A. L. Ferguson, "Flowback: A generalized flow-matching approach for biomolecular backmapping," *Journal of Chemical Information and Modeling*, 2025.
- <span id="page-25-8"></span>[243] A. Cornman, J. West-Roberts, A. P. Camargo, S. Roux, M. Beracochea, M. Mirdita, S. Ovchinnikov, and Y. Hwang, "The omg dataset: An open metagenomic corpus for mixed-modality genomic language modeling," *bioRxiv*, pp. 2024–08, 2024.
- <span id="page-25-9"></span>[244] T. H. Olsen, F. Boyles, and C. M. Deane, "Observed antibody space: A diverse database of cleaned, annotated, and translated unpaired and paired antibody sequences," *Protein Science*, vol. 31, no. 1, pp. 141–146, 2022. [Online]. Available: <https://doi.org/10.1002/pro.4205>
- <span id="page-25-10"></span>[245] J. Adolf-Bryfogle, O. Kalyuzhniy, M. Kubitz, B. D. Weitzner, X. Hu, Y. Adachi, W. R. Schief, and R. L. J. Dunbrack, "RosettaAntibodyDesign (RAbD): A general framework for computational antibody design," *PLOS Computational Biology*, vol. 14, no. 4, p. e1006112, 2018. [Online]. Available: <https://doi.org/10.1371/journal.pcbi.1006112>
- <span id="page-25-11"></span>[246] T. UniProt Consortium, "Uniprot: the universal protein knowledgebase," *Nucleic acids research*, vol. 46, no. 5, pp. 2699–2699, 2018.

- <span id="page-25-12"></span>[247] A. Bairoch and R. Apweiler, "The swiss-prot protein sequence database and its supplement trembl in 2000," *Nucleic acids research*, vol. 28, no. 1, pp. 45–48, 2000.
- <span id="page-25-13"></span>[248] J.-M. Chandonia, L. Guan, S. Lin, C. Yu, N. K. Fox, and S. E. Brenner, "Scope: improvements to the structural classification of proteins–extended database to facilitate variant interpretation and machine learning," *Nucleic acids research*, vol. 50, no. D1, pp. D553–D559, 2022.
- <span id="page-25-14"></span>[249] Z. Wen, J. He, H. Tao, and S.-Y. Huang, "Pepbdb: a comprehensive structural database of biological peptide– protein interactions," *Bioinformatics*, vol. 35, no. 1, pp. 175–177, 2019.
- <span id="page-25-15"></span>[250] F. Wu, T. Xu, S. Jin, X. Tang, Z. Xu, J. Zou, and B. Hie, "D-flow: Multi-modality flow matching for d-peptide design," *arXiv preprint arXiv:2411.10618*, 2024.
- <span id="page-25-16"></span>[251] Y. Vander Meersche, G. Cretin, A. Gheeraert, J.-C. Gelly, and T. Galochkina, "Atlas: protein flexibility description from atomistic molecular dynamics simulations," *Nucleic acids research*, vol. 52, no. D1, pp. D384–D392, 2024.
- <span id="page-25-17"></span>[252] A. Kryshtafovych, T. Schwede, M. Topf, K. Fidelis, and J. Moult, "Critical assessment of methods of protein structure prediction (casp)—round xv," *Proteins: Structure, Function, and Bioinformatics*, vol. 91, no. 12, pp. 1539–1549, 2023.
- <span id="page-25-18"></span>[253] A. Morehead, J. Liu, P. Neupane, N. Giri, and J. Cheng, "Protein-ligand structure and affinity prediction in casp16 using a geometric deep learning ensemble and flow matching," *Proteins: Structure, Function, and Bioinformatics*, 2025.
- <span id="page-25-19"></span>[254] O. Abdin, S. Nim, H. Wen, and P. M. Kim, "Pepnn: a deep attention model for the identification of peptide binding sites," *Communications biology*, vol. 5, no. 1, p. 503, 2022.
- <span id="page-25-20"></span>[255] C. Zhang, X. Zhang, L. Freddolino, and Y. Zhang, "Biolip2: an updated structure database for biologically relevant ligand–protein interactions," *Nucleic acids research*, vol. 52, no. D1, pp. D404–D412, 2024.
- <span id="page-25-21"></span>[256] V. Ljosa, K. L. Sokolnicki, and A. E. Carpenter, "Annotated high-throughput microscopy image sets for validation," *Nature Methods*, vol. 9, no. 7, p. 637, 2012.
- <span id="page-25-22"></span>[257] J. Taylor, B. Earnshaw, B. Mabey, M. Victors, and J. Yosinski, "RxRx1: An Image Set for Cellular Morphological Variation Across Many Experimental Batches," in *ICLR AI for Social Good Workshop*, 2019.
- <span id="page-25-23"></span>[258] S. N. Chandrasekaran, J. Ackerman, E. Alix, D. M. Ando, J. Arevalo, and *et al.*, "JUMP Cell Painting dataset: morphological impact of 136,000 chemical and genetic perturbations," *bioRxiv*, 2023, preprint.
- <span id="page-25-24"></span>[259] N. Kumar, R. Verma, D. Anand, Y. Zhou, O. F. Onder, E. Tsougenis, H. Chen, P. Heng, J. Li, Z. Hu, Y. Wang, N. A. Koohbanani, M. Jahanifar, N. Z. Tajeddin, A. Gooya, N. Rajpoot, X. Ren, S. Zhou, Q. Wang, D. Shen, C. Yang, C. Weng, W. Yu, C. Yeh, S. Yang, S. Xu, P. Yeung, P. Sun, A. Mahbod, G. Schaefer, I. Ellinger, R. Ecker, O. Smedby, C. Wang, B. Chidester, T. Ton, M. Tran, J. Ma, M. N. Do, . . . , and A. Sethi, "A multi-organ nucleus segmentation challenge," *IEEE Transactions on Medical Imaging*, vol. 39, no. 5, pp.

1380–1391, May 2020.

- <span id="page-26-0"></span>[260] K. Sirinukunwattana, J. P. W. Pluim, H. Chen, X. Qi, P. Heng, Y. B. Guo, L. Y. Wang, B. J. Matuszewski, E. Bruni, U. Sanchez, A. Böhm, O. Ronneberger, B. B. Cheikh, D. Racoceanu, P. Kainz, M. Pfeiffer, M. Urschler, D. R. J. Snead, and N. M. Rajpoot, "Gland segmentation in colon histology images: The glas challenge contest," *Medical Image Analysis*, vol. 35, pp. 489–502, Jan. 2017.
- <span id="page-26-1"></span>[261] S. Leclerc, E. Smistad, J. Pedrosa, A. Ostvik, F. Cervenansky, F. Espinosa, T. Espeland, E. A. R. Berg, P.-M. Jodoin, T. Grenier, C. Lartizien, J. D'hooge, L. Lovstakken, and O. Bernard, "Deep learning for segmentation using an open large-scale dataset in 2d echocardiography," *IEEE Transactions on Medical Imaging*, vol. 38, no. 9, pp. 2198–2210, 2019.
- <span id="page-26-2"></span>[262] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers, B. van Ginneken, M. Bilello, P. Bilic, P. F. Christ, R. K. G. Do, M. J. Gollub, S. H. Heckers, H. Huisman, W. R. Jarnagin, M. K. McHugo, S. Napel, J. S. G. Pernicka, K. Rhode *et al.*, "The medical segmentation decathlon," *Nature Communications*, vol. 13, no. 1, p. 4128, 2022.
- <span id="page-26-3"></span>[263] F. Knoll, J. Zbontar, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, J. Pinkerton, D. Wang, N. Yakubova, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, "fastmri: A publicly available raw kspace and dicom dataset of knee images for accelerated mr image reconstruction using machine learning," *Radiology: Artificial Intelligence*, vol. 2, no. 1, p. e190007, 2020.
- <span id="page-26-4"></span>[264] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, L. Lanczi, E. Gerstner, M.-A. Weber, T. Arbel, B. B. Avants, N. Ayache, P. Buendia, D. L. Collins, N. Cordier, J. J. Corso, A. Criminisi, T. Das, H. Delingette, C. Demiralp, C. R. Durst, M. Dojat, S. Doyle, J. Festa, F. Forbes, E. Geremia, B. Glocker, P. Golland, X. Guo, A. Hamamci, K. M. Iftekharuddin, R. Jena, N. M. John, E. Konukoglu, D. Lashkari, J. A. Mariz, R. Meier, S. Pereira, D. Precup, S. J. Price, T. R. Raviv, S. M. S. Reza, M. Ryan, D. Sarikaya, L. Schwartz, H.-C. Shin, J. Shotton, C. A. Silva, N. Sousa, N. K. Subbanna, G. Szekely, T. J. Taylor, O. M. Thomas, N. J. Tustison, G. Unal, F. Vasseur, M. Wintermark, D. H. Ye, L. Zhao, B. Zhao, D. Zikic, M. Prastawa, M. Reyes, and K. Van Leemput, "The multimodal brain tumor image segmentation benchmark (brats)," *IEEE Transactions on Medical Imaging*, vol. 34, no. 10, pp. 1993–2024, 2015.
- <span id="page-26-5"></span>[265] G. Jaume, P. Doucet, A. H. Song, M. Y. Lu, C. Almagro-Pérez, S. J. Wagner, A. J. Vaidya, R. J. Chen, D. F. K. Williamson, A. Kim, and F. Mahmood, "HEST-1k: A Dataset for Spatial Transcriptomics and Histology Image Analysis," *arXiv preprint arXiv:2406.16192*, 2024.
- <span id="page-26-6"></span>[266] J. Chen, M. Zhou, W. Wu, J. Zhang, Y. Li, and

D. Li, "STimage-1K4M: A Histopathology Image–Gene Expression Dataset for Spatial Transcriptomics," *arXiv preprint arXiv:2406.06393*, 2024.

- <span id="page-26-7"></span>[267] T. Lohoff, S. Ghazanfar, A. Missarova, N. Koulena, E. Pierson, and *et al.*, "Integration of spatial and singlecell transcriptomic data elucidates mouse organogenesis," *Nature Biotechnology*, vol. 40, no. 1, pp. 74–85, 2022.
- <span id="page-26-8"></span>[268] E. Stephenson, G. Reynolds, R. A. Botting, F. J. Calero-Nieto, M. D. Morgan, and *et al.*, "Single-cell multi-omics analysis of the immune response in covid-19," *Nature Medicine*, vol. 27, no. 5, pp. 904–916, 2021.
- <span id="page-26-9"></span>[269] N. A. Steinmetz, P. Zatka-Haas, M. Carandini, and K. D. Harris, "Distributed coding of choice, action and engagement across the mouse brain," *Nature*, vol. 576, no. 7786, pp. 266–273, 2019.
- <span id="page-26-10"></span>[270] R. D. Flint, Z. A. Wright, M. R. Scheid, and M. W. Slutzky, "Long-term stability of neural prosthetic control signals from silicon cortical arrays in rhesus macaque motor cortex," *Journal of Neural Engineering*, vol. 9, no. 5, p. 056009, 2012.
- <span id="page-26-11"></span>[271] M. M. Churchland, J. P. Cunningham, M. T. Kaufman, S. I. Ryu, and K. V. Shenoy, "Neural population dynamics during reaching," *Nature*, vol. 487, pp. 51–56, 2012.
- <span id="page-26-12"></span>[272] E. J. Cornblath, E. Heravi, J. P. Cunningham, and D. Sussillo, "An empirical evaluation of neural population dynamics models for motor cortex," in *Neural Latents Benchmark Workshop at NeurIPS*, 2021.
- <span id="page-26-13"></span>[273] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang *et al.*, "A survey on evaluation of large language models," *ACM transactions on intelligent systems and technology*, vol. 15, no. 3, pp. 1–45, 2024.
- <span id="page-26-14"></span>[274] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and P. S. Yu, "Generalizing to unseen domains: A survey on domain generalization," *IEEE transactions on knowledge and data engineering*, vol. 35, no. 8, pp. 8052–8072, 2022.
- <span id="page-26-15"></span>[275] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," in *2009 IEEE conference on computer vision and pattern recognition*. Ieee, 2009, pp. 248–255.
- <span id="page-26-16"></span>[276] V. P. Dwivedi, C. K. Joshi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson, "Benchmarking graph neural networks," *Journal of Machine Learning Research*, vol. 24, no. 43, pp. 1–48, 2023.