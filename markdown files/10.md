ANJIANG WEI, Stanford University, USA ROHAN YADAV, Stanford University, USA HANG SONG, Stanford University, USA WONCHAN LEE, NVIDIA, USA KE WANG, Nanjing University, China ALEX AIKEN, Stanford University, USA

Optimizing parallel programs for distributed heterogeneous systems remains a complex task, often requiring significant code modifications. Task-based programming systems improve modularity by separating performance decisions from core application logic, but their mapping interfaces are often too low-level. In this work, we introduce Mapple, a high-level, declarative programming interface for mapping distributed applications. Mapple provides transformation primitives to resolve dimensionality mismatches between iteration and processor spaces, including a key primitive, decompose, that helps minimize communication volume. We implement Mapple on top of the Legion runtime by translating Mapple mappers into its low-level C++ interface. Across nine applications, including six matrix multiplication algorithms and three scientific computing workloads, Mapple reduces mapper code size by 14× and enables performance improvements of up to 1.34× over expert-written C++ mappers. In addition, the decompose primitive achieves up to 1.83× improvement over existing dimensionality-resolution heuristics. These results demonstrate that Mapple simplifies the development of high-performance mappers for distributed applications.

## <span id="page-0-0"></span>1 Introduction

Optimizing the performance of parallel programs on modern distributed heterogeneous systems (where nodes combine different types of processors, such as CPUs and GPUs) remains a complex and labor-intensive task despite decades of research efforts. Programmers must navigate intricate low-level programming interfaces and often make substantial code modifications to achieve high performance. As a result, even experts in high-performance computing find performance tuning a persistent challenge.

Two dominant programming paradigms exist today. The first is the Message Passing Interface (MPI)+X model [\[Chandra](#page-25-0) [2001;](#page-25-0) [Dagum and Menon](#page-26-0) [1998;](#page-26-0) [Gropp et al.](#page-26-1) [1999;](#page-26-1) [Sanders and Kandrot](#page-27-0) [2010;](#page-27-0) [Yang et al.](#page-27-1) [2011\]](#page-27-1) where MPI is used for distributed-memory parallelism (across computers), and X is typically a shared-memory programming model used for intra-node parallelism (within one computer), such as OpenMP or CUDA. This model gives programmers fine-grained control over performance-critical aspects but significantly compromises productivity. Achieving performance gains often requires significant changes throughout the application code, making it difficult to optimize without inadvertently affecting application logic. The second paradigm is task-based programming systems [\[Augonnet et al.](#page-25-1) [2009;](#page-25-1) [Chamberlain et al.](#page-25-2) [2007;](#page-25-2) [Duran et al.](#page-26-2) [2011;](#page-26-2) [Fatahalian et al.](#page-26-3) [2006;](#page-26-3) [Kaiser et al.](#page-26-4) [2014;](#page-26-4) [Kale and Krishnan](#page-26-5) [1993;](#page-26-5) [Slaughter et al.](#page-27-2) [2015\]](#page-27-2), which decouple performance decisions from the core application logic. These systems provide a dedicated interface for specifying performance-related policies, such as task and data placement across machines, enabling developers to tune program performance independently of application logic. This separation of concerns enhances modularity and makes performance tuning more systematic and less error-prone.

Authors' Contact Information: Anjiang Wei, Stanford University, USA, anjiang@cs.stanford.edu; Rohan Yadav, Stanford University, USA, rohany@cs.stanford.edu; Hang Song, Stanford University, USA, songhang@stanford.edu; Wonchan Lee, NVIDIA, USA, wonchanl@nvidia.com; Ke Wang, Nanjing University, China, kwg@nju.edu.cn; Alex Aiken, Stanford University, USA, aiken@cs.stanford.edu.

In task-based systems, the programming interface responsible for performance-critical decisions is known as the mapping interface, and the concrete implementation of these decisions is called a mapper. A central aspect of mapping is to partition tasks and data and assign them to processors in a distributed system. Distributed algorithms—such as those for matrix multiplication [\[Agarwal et al.](#page-25-3) [1995;](#page-25-3) [Cannon](#page-25-4) [1969;](#page-25-4) [Choi et al.](#page-26-6) [1994;](#page-26-6) [Kwasniewski et al.](#page-26-7) [2019;](#page-26-7) [Solomonik and Demmel](#page-27-3) [2011;](#page-27-3) [Van](#page-27-4) [De Geijn and Watts](#page-27-4) [1997\]](#page-27-4)—rely on carefully crafted tensor partitioning and processor mappings. For instance, the Cannon's algorithm [\[Cannon](#page-25-4) [1969\]](#page-25-4) can experience up to a 3.5× slowdown when runtime heuristics are used instead of the intended mapping strategy.

A major limitation of existing mapping interface designs is their low-level nature and lack of abstraction, which exposes users to the complexity and intricacies of the underlying runtime system. These interfaces are often tightly coupled with the runtime, making them difficult to use, especially for application developers unfamiliar with their internals. For instance, defining a new mapper in the Chapel framework [\[Chamberlain et al.](#page-25-2) [2007\]](#page-25-2) requires implementing 19 functions according to 33 pages of documentation [\[Chamberlain et al.](#page-25-5) [2011,](#page-25-5) [2010\]](#page-25-6).

In this work, we introduce Mapple, a new mapping interface design. Mapple provides a high-level abstraction that hides low-level runtime details while still exposing performance-critical mapping decisions. Compared to the original interface, Mapple reduces code size by 14× without sacrificing performance (see Section [6.1\)](#page-18-0).

The core challenge in any mapping interface targeting distributed applications is to determine how a multi-dimensional iteration space, defined by the algorithm's logical loop structure, maps onto a multi-dimensional processor space, defined by the machine's hierarchical architecture (e.g., a cluster of nodes with GPUs per node). This mapping process is directly responsible for the amount of data movement across processors, which is a key factor in achieving high performance on distributed systems. This challenge is compounded by the fact that these two spaces often differ in dimensionality, making the task of writing an effective mapping inherently complex.

To address the potential mismatch in dimensionality between the two spaces, Mapple introduces a set of transformation primitives that operate on the processor space. Building on a theoretical analysis, we identify a key primitive, decompose, which assists users in writing mappers that reduce data communication. Using this primitive, Mapple achieves up to 83% performance improvement over a commonly used heuristic for resolving dimensionality differences.

While Mapple's transformation primitives on the processor space may appear similar to classical loop transformations, such as tiling, fusion, and fission [\[Chen et al.](#page-26-8) [2008,](#page-26-8) [2018a;](#page-26-9) [Ragan-Kelley](#page-27-5) [et al.](#page-27-5) [2013;](#page-27-5) [Wolf](#page-27-6) [1992\]](#page-27-6), they differ fundamentally in both purpose and scope. Traditional loop transformations aim to improve locality and parallelism within a single shared-memory system. In contrast, Mapple focuses on mapping iterations across a distributed processor space. Likewise, existing scheduling languages' approach of separating schedules from algorithms may share some similarities to Mapple's design [\[Ahrens et al.](#page-25-7) [2022;](#page-25-7) [Senanayake et al.](#page-27-7) [2020;](#page-27-7) [Zhang et al.](#page-27-8) [2018\]](#page-27-8). However, they do not address the problem of mapping iteration spaces onto processor spaces with potentially mismatched dimensionality, nor do they explicitly minimize data movement across heterogeneous processors.

We implement Mapple on top of the Legion [\[Bauer et al.](#page-25-8) [2012\]](#page-25-8) parallel programming system. As a high-level interface, Mapple mappers are translated into Legion's low-level C++ mapping interface. To evaluate Mapple, we apply it to a diverse set of nine applications, including six advanced distributed matrix multiplication algorithms and three scientific computing workloads. Mapple reduces the lines of code required to implement these mappers by 14× compared to handwritten C++ mappers, without introducing any observable performance overhead. In addition, we demonstrate that Mapple enables effective performance tuning: mappers written in Mapple outperform expert-crafted C++ mappers by up to 1.34×.

<span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)

(b) Simplified code excerpts from a C++ mapper.

Fig. 1. Comparison between a Mapple mapper and its partial C++ counterpart. The Mapple mapper uses a high-level, declarative design that abstracts away the complexity of low-level C++ implementations while still supporting performance optimization. The boxed block2d function is realized through two separate APIs in the C++ mapper, illustrating the conciseness of Mapple.

In summary, the contributions of this work are:

- (1) A high-level programming interface for mapping distributed applications to heterogeneous machines.
- (2) A set of transformation primitives for resolving dimensionality mismatches between iteration and processor spaces, including a key primitive, decompose, which helps minimize data communication volume.
- (3) An implementation of Mapple that reduces mapper code size by 14× compared to the low-level interface, without incurring additional overhead.
- (4) A comprehensive evaluation showing that mappers written in Mapple can outperform expert-written C++ mappers by up to 1.34×, and that the decompose primitive yields up to 1.83× performance improvement.

## 2 Design Goals and Non-Goals

Goal: A High-Level Interface Supporting Explicit Mapping and Optimization. We aim to provide a high-level programming interface that abstracts away the complexities of the underlying low-level runtime. This design allows users to focus on the logic of mapping decisions rather than the intricate details of system internals. These mapping decisions generally include:

- how to partition and distribute loops across distributed machines;
- where to place data in memory;
- how to lay out data in memory;
- whether to perform garbage collection; and
- how to make scheduling decisions.

To support this, we introduce explicit, modular abstractions and transformation primitives that make such decisions directly programmable and easy to change. For example, block2d specifies loop distribution, Region and Layout control data placement and layout, GarbageCollect handles memory management, and Backpressure guides scheduling. As shown in Figure [1,](#page-2-0) this approach achieves the same functionality as the C++ implementation but with significantly less code and greater clarity. We comprehensively quantify the reduction of code sizes in Section [6.1.](#page-18-0)

Moreover, our design enables systematic exploration of different mapping strategies. Programmers can easily experiment with different mappers, often outperforming hand-tuned C++ mappers. Figure [1a](#page-2-0) presents only a subset of Mapple's features. We present the full features of Mapple in Section [7.1](#page-22-0) and demonstrate its impact on performance tuning in Section [6.2.](#page-20-0)

Non-Goal: Portability across systems. We do not aim to support seamless portability across different systems. This non-goal reflects the inherent differences among distributed runtimes, which often prevent direct reuse of implementations. Nonetheless, we believe the ideas and abstractions introduced here can inform the design of other distributed systems.

#### <span id="page-3-0"></span>3 The Core Challenge: Mapping Iteration Space to Processor Space

A central challenge for any mapping interface targeting distributed applications is efficiently assigning computation to hardware. In these settings, the algorithm defines a multi-dimensional iteration space, representing the logical structure of loops and computations. In contrast, the hardware exposes a multi-dimensional processor space, reflecting its hierarchical architecture—for example, a cluster of nodes with GPUs per node. The core problem, known as index mapping, is to map the iteration space onto the processor space in a way that minimizes data movement, which is critical for achieving high performance on distributed systems. Notably, the iteration and processor spaces often differ not only in size but also in dimensionality, adding further complexity to the mapping task.

In light of this challenge, we first survey existing approaches to index mapping. Motivated by their limitations, we introduce Mapple's transformation primitives designed for index mapping in distributed systems. We defer the discussion of how Mapple handles dimensionality mismatches between iteration and processor spaces, using the decompose primitive, to Section [4.](#page-9-0)

#### 3.1 Existing Approaches

Existing task-based systems support mapper development through one of three design styles, as illustrated in Figure [2.](#page-4-0) The enumeration-based interface [\[Adaptive MPI](#page-25-9) [2025\]](#page-25-9) allows users to explicitly specify how each tile of the distributed array is mapped to a processor. While this interface is expressive, it lacks generality: enumeration-based mappers are hard-coded to a specific size of inputs or machines.

In contrast, keyword-based designs [\[Keyword Distribution](#page-26-10) [2025\]](#page-26-10) provide a higher-level abstraction by letting users choose from a predefined set of standard distributions (e.g., BlockDist for block distribution). However, this approach sacrifices flexibility—the fixed keyword-based options cannot express non-standard mapping strategies. For example, none of the distributions used in matrixmultiplication algorithms in Section [6.1](#page-18-0) can be represented using keyword-based interfaces.

<span id="page-4-0"></span>

| Enumeration   |                                                    | Keyword                                          | Programmatic                                             |  |  |
|---------------|----------------------------------------------------|--------------------------------------------------|----------------------------------------------------------|--|--|
| Tile<br>Index | Processor<br>Index                                 | Standard Distributions:<br>•<br>BlockDist        | 3 classes to be implemented:<br>class GlobalDistribution |  |  |
| 0             | 0                                                  | •<br>CyclicDist                                  | class GlobalDomain                                       |  |  |
| 1             | 0                                                  | •<br>BlockCyclicDist                             | class GlobalArray                                        |  |  |
| 2             | 1                                                  | Space = {1L1, 1L2};                              | 19 functions to be implemented:                          |  |  |
|               |                                                    |                                                  | proc dsiNewRectangularDom()                              |  |  |
| 7<br>3        | D = Dist.createDomain(Space);<br>var A, B: [D] int | proc dsiAccess()<br>proc dsiIndexToLocale()<br>… |                                                          |  |  |
|               | …                                                  | …                                                | Dist = new BlockDist(Space);                             |  |  |

Fig. 2. Three existing interface designs for mapping iteration space to processor space: the enumeration-based, keyword-based, and programmatic approaches.

The programmatic approach provides users with a low-level programming interface composed of classes and functions that must be implemented to control mapping behavior. While this design offers flexibility, it exposes significant system detail and complexity. For instance, defining a new mapping in Chapel requires implementing 19 functions based on 33 pages of documentation [\[Chamberlain et al.](#page-25-5) [2011,](#page-25-5) [2010\]](#page-25-6), and Legion's programmatic interface directly exposes the runtime's internal abstractions, accompanied by 19 pages of documentation [\[Bauer et al.](#page-25-8) [2012\]](#page-25-8). In our experience, only users with deep expertise in the underlying runtime system are able to successfully develop mappers using such interfaces.

## 3.2 Motivating Examples

We present three example mappers in Mapple to illustrate how iteration spaces are mapped onto processor spaces. First, we describe a standard block distribution in Section [3.2.1](#page-4-1) to introduce the core concepts. Next, we discuss a custom cyclic distribution in Section [3.2.2,](#page-5-0) which is not supported in keyword-based mapping interfaces. Finally, we use a mapper from Solomonik's 2.5D matrix multiplication algorithm [\[Solomonik and Demmel](#page-27-3) [2011\]](#page-27-3) in Section [3.2.3](#page-5-1) to demonstrate the complexity introduced by mismatched dimensionality between the iteration and processor spaces.

<span id="page-4-1"></span>3.2.1 Standard Block Distribution. Block distribution is a common default strategy widely supported by parallel programming systems. We use it here to illustrate the core concepts of index mapping in Mapple. In Mapple, users define mappers by writing a function that maps each iteration point in the iteration space to a target processor in the processor space. Figure [3](#page-5-2) shows the function block2D, which implements a block distribution for an iteration space of size (6, 6) (that is, 0 ≤ < 6 and 0 ≤ < 6) and a processor space of size (2, 2), representing two nodes with two GPUs per node. The function assigns each iteration point to a specific processor.

The GPU processor space can be accessed using Machine(GPU). In the mapper function, the iteration point ipoint, iteration space ispace, and processor space size m.size are all 2D integer tuples. These values are used to compute the processor indices node\_idx and gpu\_idx, which specify the destination processor for each iteration point. As shown in Figure [3,](#page-5-2) the shaded iteration point (2, 3) is mapped to node 0 and GPU 1. The function returns a specific processor for each iteration point, resulting in a block distribution over the processor space.

<span id="page-5-2"></span>![](_page_5_Figure_1.jpeg)

Fig. 3. Block mapping from the iteration space of (6, 6) to the processor space of (2, 2) indicating a machine with 2 nodes and 2 GPUs per node. A node index and a GPU index within the node indicate a specific GPU processor. The shaded iteration point (2, 3) is mapped to node 0 and GPU 1.

<span id="page-5-0"></span>3.2.2 Custom Cyclic Distribution. We showcase a custom distribution expressible in Mapple that is not yet supported by most other parallel programming systems, which typically restrict users to a fixed set of standard, keyword-based distributions. This non-standard cyclic distribution is inspired by a simplified variant of a distributed matrix multiplication algorithm. Figure [4](#page-6-0) illustrates the mapping function linearCyclic. In the resulting distribution, the subdiagonal iteration points (shaded in the iteration space) are mapped to the first processor (shaded in the processor space).

The mapper begins with the merge transformation primitive, which fuses dimension 0 and dimension 1 of the original 2D processor space into a single dimension. We will formally define the merge primitive in Section [3.3.](#page-7-0) After applying this transformation, the resulting processor space m becomes one-dimensional with size 4. In the mapping function, the 2D iteration point is first linearized, then assigned to processors using a round-robin distribution over the 4 processors.

<span id="page-5-1"></span>3.2.3 Mapping Under Dimensionality Mismatch. We use this example to illustrate the complexity introduced by a mismatch between the dimensionality of the iteration space and the processor space, which in turn raises deeper research questions. Figure [5](#page-6-1) shows a mapper for the Solomonik's algorithm executed on a 2-node machine, where each node has 4 GPUs. The iteration space is three-dimensional, and the algorithm specifies a hierarchical distribution: the iteration space is first partitioned along the x-axis so that each node handles half of the x-dimension, and within each node, the 4 GPUs perform a 2D block distribution over the y-z plane. This example highlights how mismatched dimensions require careful coordination between hierarchical and multi-dimensional mapping strategies.

There is a mismatch in dimensionality between the iteration space, which is three-dimensional, and the initial processor space, which is two-dimensional. To enable the mapping required by the algorithm, we apply the split transformation primitive four times (highlighted in red in the code). The first two splits align the node-level processor dimension with the x-axis of the iteration space, while the last two splits align the GPU-level processor dimensions with the y and z axes. This results in a six-dimensional processor space, which we visualize as two separate 3D spaces: one for

<span id="page-6-0"></span>![](_page_6_Figure_1.jpeg)

Fig. 4. A custom cyclic distribution. The merge primitive transforms the 2D processor space into a 1D space. The mapping function linearizes each 2D iteration point and applies a round-robin distribution over the resulting 1D processor space.

<span id="page-6-1"></span>![](_page_6_Figure_3.jpeg)

Fig. 5. A mapper illustrating the complexity caused by mismatched dimensionality between the iteration and processor spaces, as required by the Solomonik's algorithm on a 2-node machine with 4 GPUs per node. The original 2D processor space is transformed into a 6D space using the split primitive, visualized as two 3D spaces.

<span id="page-7-1"></span>

| Transformation               | Semantics                                                     |  |  |  |  |  |
|------------------------------|---------------------------------------------------------------|--|--|--|--|--|
|                              | 𝑚′<br>[𝑎0, , 𝑎𝑛]<br>𝑚[𝑏0, , 𝑏𝑛−1]<br>≔                        |  |  |  |  |  |
| 𝑚′ =<br>𝑚.split(𝑖, 𝑑)        | 𝑎𝑡<br>𝑡<br><<br>𝑖<br>                                      |  |  |  |  |  |
|                              | 𝑏𝑡<br><br>𝑎𝑖<br>𝑎𝑖+1<br>𝑑<br>𝑡<br>𝑖<br>=<br>+<br>·<br>=     |  |  |  |  |  |
|                              | 𝑎𝑡+1<br>𝑡<br>><br>𝑖<br>                                    |  |  |  |  |  |
|                              | <br>B<br>𝑚′<br>[𝑎0, , 𝑎𝑛−2]<br>𝑚[𝑏0, , 𝑏𝑛−1]                 |  |  |  |  |  |
|                              | <<br>or<br><<br><<br>𝑎𝑡<br>𝑡<br>𝑝<br>𝑝<br>𝑡<br>𝑞<br>       |  |  |  |  |  |
| 𝑚′ =<br>𝑚.merge(𝑝, 𝑞)        | <br>𝑎𝑝<br>mod<br>𝑚.size[𝑝]<br>𝑡<br>𝑝<br>=<br>𝑏𝑡<br><br>= |  |  |  |  |  |
|                              | 𝑎𝑝<br>j<br>k<br>𝑡<br>𝑞<br>=<br>𝑚.size[𝑝 ]                     |  |  |  |  |  |
|                              | <br>𝑎𝑡−1<br>𝑡<br>><br>𝑞<br>                             |  |  |  |  |  |
|                              | <br>𝑚′<br>[𝑎0, , 𝑎𝑛−1]<br>𝑚[𝑏0, , 𝑏𝑛−1]<br>≔                 |  |  |  |  |  |
| 𝑚′ =<br>𝑚.swap(𝑝, 𝑞)         | 𝑎𝑞<br>𝑡<br>𝑝<br>=<br>                                      |  |  |  |  |  |
|                              | 𝑏𝑡<br><br>𝑎𝑝<br>𝑡<br>𝑞<br>=<br>=                            |  |  |  |  |  |
|                              | 𝑎𝑡<br>𝑡<br>𝑝<br>𝑡<br>𝑞<br><br>≠<br>∧<br>≠                  |  |  |  |  |  |
|                              | <br>𝑚′<br>[𝑎0, , 𝑎𝑛−1]<br>𝑚[𝑏0, , 𝑏𝑛−1]<br>≔                 |  |  |  |  |  |
| 𝑚′ =<br>𝑚.slice(𝑖,𝑙𝑜𝑤, ℎ𝑖𝑔ℎ) | (<br>𝑎𝑖<br>𝑙𝑜𝑤<br>𝑡<br>𝑖<br>+<br>=<br>𝑏𝑡<br>=                 |  |  |  |  |  |
|                              | 𝑎𝑡<br>𝑡<br>𝑖<br>≠                                             |  |  |  |  |  |
| 𝑚′ =<br>𝑚.decompose(𝑖,𝑇<br>) | 𝑇<br>(𝑙1, ,𝑙𝑘<br>=<br>)                                       |  |  |  |  |  |
|                              | Formally defined in Section<br>4                              |  |  |  |  |  |

Fig. 6. Semantics of transformation primitives expressed as mappings from the indices of the transformed processor space to the indices of the original processor space.

nodes of size (2, 1, 1) and one for GPUs of size (1, 2, 2). We formally define the split transformation primitive in Section [3.3.](#page-7-0)

This example also raises deeper research questions. Can we automatically determine the appropriate splitting factors? What underlying principles guide the mapping decisions in matrix multiplication algorithms? Is it possible to address dimensionality mismatches through a generalized transformation? We explore these questions in Section [4,](#page-9-0) where we introduce a new primitive called decompose to provide a unified solution.

#### <span id="page-7-0"></span>3.3 Transformation Primitives

We define the semantics of each Mapple transformation primitive in Figure [6,](#page-7-1) with the exception of the decompose primitive, which is deferred to Section [4.](#page-9-0) Each transformation primitive is a function that takes a processor space as input and returns a transformed processor space ′ , following the mapping illustrated on the right-hand side of Figure [6.](#page-7-1) We now describe each transformation in detail.

The split transformation takes two arguments: a dimension index and a splitting factor . Given a processor space of shape (0, . . . , −1), the operation ′ = .split(, ) produces a new processor space ′ of shape (0, . . . , , /, . . . , −1). This transformation is invertible, allowing mappers to operate on ′ while Mapple maps back to the original space . The index relationship is:

$$
m'[a_0, \ldots, a_i, a_{i+1}, \ldots, a_n] = m[a_0, \ldots, a_i + a_{i+1} \times d, \ldots, a_n].
$$

<span id="page-8-0"></span>

| Distribution | Iteration<br>Space | Processor<br>Space | Transformation                                     | Mapping Function                                                                                 |  |  |  |
|--------------|--------------------|--------------------|----------------------------------------------------|--------------------------------------------------------------------------------------------------|--|--|--|
| block2D      |                    |                    | m = Machine(GPU)                                   | def block2D(Tuple ipoint, Tuple ispace):<br>idx = ipoint * m.size / ispace<br>return m[*idx]     |  |  |  |
| block1D_x    |                    |                    | m = Machine(GPU)<br>m1 = m.merge(0, 1).split(0, 1) | def block1D_x(Tuple ipoint, Tuple ispace):<br>idx = ipoint * m1.size / ispace<br>return m1[*idx] |  |  |  |
| block1D_y    |                    |                    | m = Machine(GPU)<br>m2 = m.merge(0, 1).split(0, 4) | def block1D_y(Tuple ipoint, Tuple ispace):<br>idx = ipoint * m2.size / ispace<br>return m2[*idx] |  |  |  |
| cyclic2D     |                    |                    | m = Machine(GPU)                                   | def cyclic2D(Tuple ipoint, Tuple ispace):<br>idx = ipoint % m.size<br>return m[*idx]             |  |  |  |
| cyclic1D_x   |                    |                    | m = Machine(GPU)<br>m1 = m.merge(0, 1).split(0, 1) | def cyclic1D_x(Tuple ipoint, Tuple ispace):<br>idx = ipoint % m1.size<br>return m1[idx]          |  |  |  |
| cyclic1D_y   |                    |                    | m = Machine(GPU)<br>m2 = m.merge(0, 1).split(0, 4) | def cyclic1D_y(Tuple ipoint, Tuple ispace):<br>idx = ipoint % m2.size<br>return m2[idx]          |  |  |  |
| block-cyclic |                    |                    | m = Machine(GPU)                                   | def blockcyclic(Tuple ipoint, Tuple ispace):<br>idx = ipoint / m.size % m.size<br>return m[*idx] |  |  |  |

Fig. 7. Common distributions expressed in Mapple. The shaded region in the iteration space is mapped to the corresponding shaded processor. The transformation code reshapes the original (2, 2) processor space into the desired processor space, which is then used in the user-defined mapping function.

The merge transformation takes two dimensions and of the processor space and fuses them into one. Given a processor space of shape (0, . . . , −1), the operation ′ = .merge(, ) produces a new processor space ′ of shape (0, . . . , · , . . . , −1), where the two dimensions at positions and in are combined into a single dimension at position in′ . This transformation is invertible. The index mapping is given by:

$$
m'[a_0, ..., a_{n-2}] = m[a_0, ..., a_p \mod s_p, ..., \lfloor a_p/s_p \rfloor, ..., a_{n-2}].
$$

Transformation primitives in Mapple can be composed sequentially. Consider a 2D processor space , and let ′ = .split(0, ), followed by ′′ = ′ .merge(0, 1). The resulting processor space ′′ is again 2D. We now derive the index transformation from ′′ to the original space by sequentially applying the semantics of the merge and split transformations.

$$
m''[a_0, a_1] = m'[a_0 \bmod d, \lfloor a_0/d \rfloor, a_1] = m[(a_0 \bmod d) + \lfloor a_0/d \rfloor \times d, a_1].
$$

Because the expression (<sup>0</sup> mod ) + ⌊0/⌋ × simplifies to <sup>0</sup> for all integers, it follows that

$$
m''[a_0,a_1] = m[a_0,a_1].
$$

This demonstrates that the split and merge primitives are exact inverses when applied in sequence with the same parameters.

The swap transformation exchanges two dimensions of the processor space. Combined with merge, which flattens two dimensions into one, it lets users control whether merging follows row-major or column-major order.

The slice transformation takes a dimension along with its lower and upper bounds, applying a constant offset to that dimension. It is useful for mapping an iteration space to a subset of the processor space, such as when assigning concurrent tasks to separate groups of processors.

Using these primitives, Mapple can transform processor spaces in flexible ways. Figure [7](#page-8-0) illustrates how Mapple supports common distribution patterns. Suppose we want to map a 2D iteration space onto a 2-node machine with 2 GPUs per node. In the figure, the shaded region of the iteration space is mapped to the corresponding shaded processor in the processor space.

For the block2D distribution, the mapping function is equivalent to the one in Figure [3,](#page-5-2) but simpler due to tuple arithmetic support. As seen in the mapping functions for block2D, block1D\_x, and block1D\_y, the only difference lies in the transformed processor space. Although all use 2D processor spaces, the dimensions differ, resulting in different block distributions. The same pattern applies to cyclic2D, cyclic1D\_x, and cyclic1D\_y.

#### <span id="page-9-0"></span>4 Decompose Transformation Primitive

As discussed in Section [3.2.3,](#page-5-1) mismatches between the dimensionality of the iteration and processor spaces pose significant challenges for mapping decisions. In this section, we address this issue by analyzing its underlying principles and introducing a generalized transformation primitive, decompose. Section [4.1](#page-9-1) analyzes a suboptimal heuristic used by a commonly used programming system to handle dimensionality mismatches. We then formally define the decompose primitive in Section [4.2,](#page-10-0) grounded in a communication volume analysis. Finally, Section [4.3](#page-13-0) describes our search-based optimization algorithm and analyzes its complexity.

#### <span id="page-9-1"></span>4.1 Suboptimal Existing Heuristics for Resolving Dimensionality Mismatch

We analyze how existing task-based systems handle dimensionality mismatches between iteration and processor spaces. Most frameworks, to the best of our knowledge, bypass this issue by linearizing both spaces and applying a default 1D block mapping. However, they rarely document the specifics of their linearization process, making direct comparisons challenging. An exception is Chapel [\[Chamberlain et al.](#page-25-2) [2007\]](#page-25-2), a widely used parallel programming framework, which explicitly describes its approach to dimensionality mismatch. To reveal the limitations of this strategy, we present an intuitive example showing how it can lead to suboptimal mappings.

Suppose we have 6 processors and a 2D iteration space. To match the dimensionality, we must split the 6 processors into a 2D tuple. There are four possible factorizations: (6, 1), (3, 2), (2, 3), and (1, 6). The existing algorithm used to determine the processor grid, as shown in Algorithm [1,](#page-10-1) does not consider the actual size of the iteration space.

The algorithm takes as input the number of processors to split, denoted by , and the dimensionality of the iteration space, denoted by . It returns an integer array of length whose product equals . The algorithm follows a greedy strategy to compute this array, aiming to produce factors that are as balanced as possible in magnitude. It maintains a running product for each dimension and assigns the next prime factor of to the dimension with the smallest current product.

In the example above, the algorithm selects the grid (3, 2) by prioritizing balanced factorization, without considering the shape of the iteration space. This can lead to suboptimal mappings. We illustrate this with two iteration spaces, (12, 18) and (18, 12), corresponding to an application with spatial locality, as shown in Figure [8.](#page-10-2) Each iteration space is mapped to the processor grid (3, 2) using the block2D function, which partitions the iteration space into rectangular blocks assigned to each processor. The orange regions in the figure indicate data that must be transferred across processor boundaries due to this mapping.

For the (12, 18) iteration space, the total inter-processor communication volume is 96 elements, while for the (18, 12) iteration space it is 84 elements. This difference highlights the sensitivity of communication cost to how the iteration space is aligned with the processor grid. If the algorithm had taken the shape of the iteration space into account and chosen the processor grid (2, 3) for the

<span id="page-10-1"></span>Algorithm 1: Greedy Heuristic for Processor Grid Selection (Suboptimal) Input: # Number of processors # Dimensionality of the iteration space Function Greedy(, ): primes ← PrimeFactorization () # Sorted list of prime factors, = <sup>1</sup> ≤ · · · ≤ factors ← new int[] for ∈ {0, . . . , −1} do factors[] ← 1 for ∈ primes do ← ArgMin (factors) # Index of smallest current product factors[] ← factors[] × Sort (factors, descending=True) # Sort for consistent ordering return factors

<span id="page-10-2"></span>![](_page_10_Figure_2.jpeg)

Fig. 8. For applications with spatial locality, the mapper computed by Algorithm [1](#page-10-1) selects a fixed processor grid of (3, 2) for 2D block mapping. Data elements that must be transferred across processor boundaries are shown in orange. The (12, 18) iteration space incurs higher inter-processor communication volume than the (18, 12) iteration space, indicating that the chosen mapping is suboptimal for the former. A lower communication volume for the (12, 18) space could be achieved by using a (2, 3) processor grid instead.

(12, 18) space, the communication volume could have matched the more efficient 84-element case, avoiding unnecessary communication overhead.

## <span id="page-10-0"></span>4.2 Formal Definition of Decompose and Intuitive Solution

We formally define the optimization problem that the decompose primitive aims to solve, based on an analysis of inter-processor communication volume. We also present the underlying intuition to clarify how the solution addresses this problem.

Suppose we apply decompose to a processor space of size (0, . . . , −1), written as ′ = .decompose(, (1, . . . , )). The decompose primitive splits the -th dimension into natural numbers <sup>1</sup> , . . . , , producing a new processor space ′ of size (0, . . . , <sup>1</sup> , . . . , , . . . , −1). The transformation preserves the total size of the processor space, so it must satisfy Î =1 = .

Conceptually, decompose is a shorthand for applying a sequence of split transformations. Specifically, applying = 1.decompose(, ) is equivalent to performing +<sup>1</sup> = .split( + − 1, ) for 1 ≤ < .

There can be multiple ways to factor into natural numbers <sup>1</sup> , . . . , . The decompose primitive selects a factorization by solving the following optimization problem:

minimize 
$$
\sum_{m=1}^{k} \frac{d_{i_m}}{l_m}
$$
  
subject to 
$$
\prod_{m=1}^{k} d_{i_m} = d_i, \quad d_{i_m} \in \mathbb{N} \quad \forall m
$$

An equivalent formulation introduces the workload vector = for 1 ≤ ≤ , where represents the amount of iteration space assigned to each processor in the -th dimension. Rewriting the problem in terms of the workload vector gives:

minimize 
$$
\sum_{m=1}^{k} \frac{1}{w_m}
$$
  
subject to 
$$
\prod_{m=1}^{k} w_m = \frac{\prod_{m=1}^{k} l_m}{d_i}, \quad \frac{l_m}{w_m} \in \mathbb{N} \quad \forall m
$$

We now illustrate the optimization problem using the two examples shown in Figure [9.](#page-12-0) The lefthand side depicts a 2D case with = 2 and processor count = 4. To compute the communication volume, we count the number of elements transferred across processor boundaries. This volume equals 2 · (<sup>1</sup> + 2) · − 2 · (<sup>1</sup> + 2), where the first term is the total perimeter of the blocks of size (1,2), and the second term is the perimeter of the entire iteration space of size (1,2).

Given fixed input dimensions <sup>1</sup> and <sup>2</sup> and processor count , the second term is constant, and <sup>1</sup> ·<sup>2</sup> = <sup>1</sup> ·<sup>2</sup> is also fixed. Therefore, minimizing the communication volume reduces to minimizing <sup>1</sup> +2, which is equivalent to minimizing <sup>1</sup> 1 + 1 2 . This matches the objective of our optimization formulation in the 2D case.

On the right-hand side, the figure shows a 3D example with = 3 and = 16. Here, the communication volume corresponds to the total inter-block surface area. The surface area of the cuboids of size (1,2,3) is given by:

$$
2S = 2(w_1w_2 + w_1w_3 + w_2w_3) \cdot d_i - 2(l_1l_2 + l_1l_3 + l_2l_3).
$$

Given fixed input dimensions 1,2,<sup>3</sup> and processor count , the term 12<sup>3</sup> = 12<sup>3</sup> is constant. Thus, minimizing is equivalent to minimizing 1<sup>2</sup> + 1<sup>3</sup> + 23. Under the fixed-product constraint, this can be transformed into minimizing <sup>1</sup> 1 + 1 2 + 1 3 , again aligning with the objective of our optimization formulation in the 3D case.

This analysis generalizes to any -dimensional case. Let denote the total inter-surface area in the -dimensional setting. Then,

$$
2S = SA(w_1, w_2, \dots, w_k) \cdot d_i - SA(l_1, l_2, \dots, l_k),
$$

<span id="page-12-0"></span>![](_page_12_Figure_1.jpeg)

Fig. 9. We show two examples of how to compute the communication volume (namely, the number of inter-processor elements). In the 2D example, we compute the length of the inter-rectangle line by summing up the perimeter of all 4 rectangles of (1,2) and subtracting the perimeter of the (1,2) rectangle. In the 3D example, we compute the area of the inter-cuboid surface by summing up the surface area of all 16 cuboids of (1,2,3) and subtracting the surface area of the (1,2,3) cuboid.

where (1, 2, . . . , ) denotes the surface area of a -dimensional hyperrectangle, defined as

$$
SA(x_1, x_2, \ldots, x_k) = 2 \cdot \left( \prod_{m=1}^k x_m \right) \cdot \left( \sum_{m=1}^k \frac{1}{x_m} \right).
$$

Given that Î =1 = Î =1 is constant for fixed input size and processor count, minimizing reduces to minimizing Í =1 1 —which matches the objective of the optimization problem introduced earlier.

The solution to the optimization problem is not immediately obvious. In the following, we present an intuitive justification for the solution, which is based on the classic inequality stated below.

Theorem. Given a list of positive numbers 1, 2, . . . , , the following inequality holds:

$$
\frac{1}{n}\sum_{m=1}^n a_m \ge \left(\prod_{m=1}^n a_m\right)^{1/n},
$$

with equality if and only if <sup>1</sup> = <sup>2</sup> = · · · = .

This is the well-known arithmetic-geometric mean (AM-GM) inequality. It states that the arithmetic mean of a set of positive numbers is always greater than or equal to their geometric mean, with equality only when all values are equal.

Note that in the optimization problem, the term Î =1 is a constant determined by the input size and the number of processors. We can now apply the AM–GM inequality to the objective:

$$
\frac{1}{k} \sum_{m=1}^{k} \frac{1}{w_m} \ge \left(\prod_{m=1}^{k} \frac{1}{w_m}\right)^{1/k}.
$$
 Using the constraint  $\prod_{m=1}^{k} w_m = \frac{\prod_{m=1}^{k} l_m}{d_i}$ , we obtain:  
$$
\sum_{m=1}^{k} \frac{1}{w_m} \ge k \cdot \left(\frac{d_i}{\prod_{m=1}^{k} l_m}\right)^{1/k}.
$$

Equality is achieved when all terms <sup>1</sup> are equal, which implies <sup>1</sup> = <sup>2</sup> = · · · = . In practice, however, exact equality may not be attainable due to the integrality constraint that each must be a natural number.

In the 2D iteration space example shown in Figure [8,](#page-10-2) the mapping for the (18, 12) space is optimal because the workload vector is (1,2) = 1 1 , 2 2 = (6, 6), where the two components are equal. Similarly, the 3D example in Figure [9](#page-12-0) uses an iteration space of (4, 8, 4) mapped onto 16 processors. The solution shown in the figure yields a workload vector (1,2,3) = (2, 2, 2), which satisfies the sufficient condition for attaining the minimum communication volume.

These examples suggest that the optimal solution is achieved when the iteration space is divided among the processor dimensions in a balanced manner. However, in practice, this ideal may not be attainable due to the integrality constraint that each must be a natural number. To address this, we introduce a search-based algorithm for solving the optimization problem, which we describe in detail in Section [4.3.](#page-13-0)

### <span id="page-13-0"></span>4.3 Algorithm Implementation and Complexity Analysis

At a high level, the integrality constraint requires us to enumerate all possible ways of factoring into positive integers <sup>1</sup> , . . . , such that Î =1 = , and then select the factorization that minimizes the objective function Í =1 . The main challenge lies in efficiently and exhaustively enumerating all valid factorizations to ensure optimality.

We begin with a simple case: suppose = 3 and = 16 = 2 4 . To enumerate all possible factorizations, we must determine all ways to distribute the four factors of 2 across the three dimensions. This reduces to finding all non-negative integer solutions to the equation 1+2+<sup>3</sup> = 4, which can be solved using recursion or backtracking.

A more complex example is when = 3 and = 48 = 2 4 · 3 1 . In this case, we must find all non-negative integer solutions to both <sup>1</sup> + <sup>2</sup> + <sup>3</sup> = 4 and <sup>1</sup> + <sup>2</sup> + <sup>3</sup> = 1, corresponding to the distribution of the prime factors 2 and 3, respectively. Each valid factorization corresponds to a tuple of the form (2 1 · 3 1 , 2 2 · 3 2 , 2 3 · 3 3 ) for some choice of (, ).

In the general case where = 1 1 · 2 2 · . . . · , the prime factorization naturally decomposes the enumeration task. We can independently enumerate the placement strategies for each prime factor by solving separate integer partition problems of the form <sup>1</sup> + . . . + = for each , and then compute the Cartesian product of their solutions to generate all valid factorizations.

We now analyze the complexity of the search-based algorithm. Since the algorithm exhaustively enumerates all valid factorization strategies, its complexity is determined by the size of the search space. As shown in the earlier example, enumerating non-negative integer solutions to the equation <sup>1</sup> + <sup>2</sup> + <sup>3</sup> = 4 is equivalent to counting integer partitions with repetition, which can be mapped to the problem of selecting two dividers among six gaps. This corresponds to the number of combinations 6 2 = 15.

In general, for a processor count with prime factorization = 1 1 · 2 2 · · · , the total number of factorization strategies is:

$$
\prod_{j=1}^t \binom{a_j + k - 1}{k - 1}
$$

Each term in the product counts the number of ways to distribute the copies of prime across dimensions. In practice, the exponents are usually small (typically less than 10), and the number of dimensions is often no greater than 3. As a result, the search space remains small, and the algorithm is computationally efficient in real-world scenarios.

We believe that enumeration is necessary to guarantee an optimal solution. Consider a greedy algorithm that, at each step, assigns prime factors of = 72 (i.e., 2, 2, 2, 3, 3) to dimensions in a way that minimizes the maximum difference between elements of the workload vector. Given an iteration space of (1,2) = (8, 9), this greedy strategy produces a suboptimal workload vector of 4 3 , 3 4 , which results in imbalanced partitioning. In contrast, our search-based algorithm identifies the optimal factorization that yields a perfectly balanced workload vector of (1, 1).

#### 5 Implementation

We describe the challenges and implementation of translating Mapple into task-based runtime systems. These systems employ a highly asynchronous and pipelined execution model, where tasks progress through multiple stages to maximize throughput. Mapping decisions—determining where tasks and data are placed—must be made at various points along this pipeline and are exposed via many distinct callback functions, each tied to a specific stage in a task's lifecycle. This results in a fragmented and low-level interface that mirrors the internal structure of the runtime rather than providing an intuitive programming model for developers.

The core challenge is to design a high-level, unified mapping abstraction that can be faithfully translated into this low-level execution model. To illustrate this semantic gap, we first describe the runtime's execution semantics in Section [5.1,](#page-14-0) then present our translation strategy in Section [5.2.](#page-17-0)

#### <span id="page-14-0"></span>5.1 Execution Semantics of the Runtime System

Task-based runtime systems are designed to maximize throughput through an asynchronous, pipelined execution model, where tasks advance through several distinct stages during their lifetime. To illustrate the low-level nature of the mapping interface, we present the execution semantics of such systems. This exposes a key source of complexity: the interface is tightly coupled to internal execution stages and operates at a low level of abstraction. Consequently, a central challenge is to design a high-level, centralized mapping DSL that abstracts away low-level details while still capturing critical mapping decisions.

Before presenting the semantics, we first describe the key relationships between tasks that govern how and when they execute. Consider the following simple program:

```
task f(a, b) {
   g(a); // writes a
   h(b); // writes b
   k(a, b); // reads a, b and writes b
}
```

The task f(a, b) is the parent of child tasks g(a), h(b), and k(a, b). Each child begins only after the parent starts, and the parent completes only after all its children finish. Since tasks execute sequentially, parallelism arises from asynchronous launches. The order in which the parent invokes its children induces a program order ⪯. In this example, g(a) ⪯ h(b) ⪯ k(a, b), meaning k(a, b) has sibling predecessors g(a) and h(b).

Task dependencies are represented by the relation ≤, indicating that one task may read or write a value produced by another. In this example, g(a) ≤ k(a, b) and h(b) ≤ k(a, b) capture that k(a, b) may read values written by g(a) and h(b), and therefore cannot begin execution until both complete. However, this does not prevent all progress on k(a, b)—in particular, its mapping (i.e., assignment

<span id="page-15-0"></span>State Execution State ::= [] ∗ [] Execution Log ::= set Execution Log Entry ::= enqueued() | mapped(, ) | launched(, ) | executed(, ) Node Queues ::= ( queue) Processor ::= Tasks Point ::= (1, . . . ,) Task ::= Index(, set) Relations Task Tree Relation ::= parent(, ) | task() Task Tree ::= set Dependence Relation ::= ≤ ′ Dependencies ::= set Sibling Relation ::= ⪯ ′

Fig. 10. Abstract Syntax for Execution Model

Siblings ::= set

to a processor) can proceed before its dependencies finish. Since there is no dependency between g(a) and h(b), they may execute in parallel.

Figure [10](#page-15-0) defines the structures used in the execution semantics. The execution state consists of two vectors of queues—one for tasks enqueued on each node, and one for tasks that have been mapped. These represent two of the four stages in a task's lifetime; the others are when a task is launched and when it is executed. A task progresses through these stages as follows:

- Enqueued: A task is enqueued once its parent and all sibling predecessors have been enqueued, preserving program order and control dependencies.
- Mapped: A task is mapped after all sibling predecessors it depends on have been mapped, ensuring their processor locations are known for scheduling data movement.
- Launched: A task can launch after all dependence predecessors have executed, ensuring their effects are complete and visible.
- Executed: A task is considered executed once it finishes running and all of its children have also completed, since their effects contribute to the parent's semantics.

Figure [11](#page-16-0) formalizes the task execution rules described above using an operational semantics. Each judgment has the form

$$
L; (E, M) \xrightarrow{T; D; W} L'; (E', M')
$$

which should be read as: given a task tree , a dependence relation , and a sibling predecessor relation , the current execution log and execution state (, ), representing the sets of enqueued and mapped tasks on each node, transition to a new execution log ′ and new state ( ′ , ′ ). The rules rely on two user-supplied mapping functions, SHARD and MAP, with the following signatures:

SHARD: 
$$
t \to \text{distribute}((t * p) * (t * p)) + \text{local}(t)
$$
  
MAP:  $t \to p$ 

<span id="page-16-0"></span>

```
\ntask(t) ∈ T\nt = Index(id, P)\nenquued(t) ∉ L\nparent(tp, t) ∈ T\n\nIaunched(tp, p) ∈ L\n\nVts : parent(tp, ts) ∈ T ∧ ts ≤ t, enquued(ts) ∈ L\n\ntt : (E, M) 
$$
\xrightarrow{T,D;W}
$$
 L ∪ {enquened(t)}; (E + p t, M)\n\nt = Index(id, P)\n\nt1 = (id, P)\n\nt2 = (id, P)\n\nt3 = (id, P)\n\nt4 = U + P\n\nt5 = E - t\n\nt6 = E - t\n\nt7 = E - t\n\nt8 = E - t\n\nt9 = P + U + P\n\nt1 = (i, m, m, L)\n\nr1 = (i, m, m, L)\n\nr2 = E - t\n\nt8 = E - t\n\nt9 = E - t\n\nt1 = (E, M)  $\xrightarrow{T,D;W}$  L; ((E' + p1 t) + p2 t2, M)\n\nSHARD(t) = local(t)\n\nt8 = E - p t\n\nMAP(t) = p\nM' = M - t\n\nt1 = (E, M)  $\xrightarrow{T,D;W}$  L; (E', M + p t)\n\nVt' : t' ≤ t, mapped(t', ) ∈ L\nMAP(t) = p\nM' = M - t\n\nt2 = (E, M)  $\xrightarrow{T,D;W}$  L ∪ {mapped(t, p)}; (E, M')\n\nVt' : t' ≤ t, executed(t', ) ∈ L\n\nL3 = (E, M)  $\xrightarrow{T,D;W}$  L ∪ {launched(t, p)}; (E, M)\n\nVt' : t' ≤ t, executed(t', ) ∈ L\nVc : parent(t, c), executed(c, ) ∈ L\n\nVc : parent(t, c), executed(t, p); (E, M)\n\nVc = (E + E + E)\n\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc = (E + E + E)\nVc
```

Fig. 11. Transition relation judgments.

The SHARD function takes an index task and either partitions it into smaller tasks for distribution across nodes or assigns it to the local node. MAP then selects a specific processor within the target node for execution. Although these are two separate callback functions in the low-level interface, Mapple unifies them by interpreting both as part of a single index transformation from iteration space to processor space.

Figure [11](#page-16-0) formalizes the rules described above as execution judgments. The rules use a new operator + , which adds a task to the th queue of a vector of queues:

$$
(Q_1,\ldots,Q_i,\ldots Q_n)+_i t=(Q_1,\ldots,t:Q_i,\ldots Q_n)
$$

We also need the dual dequeuing operation:

$$
(Q_1,\ldots,Q_i:t,\ldots Q_n)-_it=(Q_1,\ldots,Q_i,\ldots Q_n)
$$

We briefly summarize each rule. [Enqeue] places a task on the same node as its parent once the parent has launched and all sibling predecessors are enqueued. [Distribute] partitions an index task into independent sub-tasks for mapping. [Local] selects the target node based on the user-defined SHARD function. [Map] assigns the task to a specific processor within that node using the MAP function. [Launch] and [Execute] mark the start and completion of execution and involve no further mapping decisions.

The semantics of task execution underscore the low-level nature of mapping in task-based systems. To achieve high performance, tasks progress through multiple pipelined stages, each requiring user-defined mapping decisions. For brevity, we focus on Shard and MAP, though in practice, 19 distinct callback APIs are invoked across a task's lifetime. This design fragments mapping logic across numerous low-level callbacks, making the APIs complex and opaque to most application developers.

#### <span id="page-17-0"></span>5.2 Translation

The translation poses two key challenges. First, as discussed in Section [5.1,](#page-14-0) mapping logic is spread across 19 callback functions triggered at different stages of a task's lifetime. While this mirrors the runtime's internal pipeline, it forces developers to work with a scattered and unintuitive interface (Figure [1\)](#page-2-0). Second, the low-level API lacks abstraction over the machine architecture—there is no logical view of the hardware, nor support for aligning mapping decisions with loop structures or restructuring index spaces to facilitate coordination.

We implement Mapple by targeting Legion's programmatic mapping interface, comprising roughly 7,000 lines of C++ code—much of it dedicated to handling low-level runtime details. Mapple overrides a set of callback functions invoked at different stages of the task execution pipeline to decide task placement. The two core overrides, corresponding to the user-defined SHARD and MAP functions, are shown in Figure [1b](#page-2-0) and explained in Section [5.1.](#page-14-0)

In Mapple, the machine's logical view is defined with m = Machine(GPU), and users apply index transformations using provided primitives. The runtime invokes the SHARD function to assign a task to a node, followed by MAP to select a processor on that node. Both are implemented by interpreting user-defined mapping functions written in Mapple, evaluated per iteration point. The core translation computes the node (for SHARD) and processor (for MAP) identifiers via a sequence of invertible transformations over the processor space. As defined in Figure [6,](#page-7-1) these transformations yield a pair of unsigned integers representing coordinates in the original 2D processor space specified by Machine. These coordinates are then returned as the node and processor identifiers.

#### 6 Results

Experiments were run on a cluster where each node has 40 IBM Power9 CPU cores and 4 NVIDIA V100 GPUs connected with NVLink 2.0 and an Infiniband EDR interconnect. To account for performance variability, each measurement was repeated five times, and the average is reported.

Benchmark. Our approach is evaluated using a diverse set of nine benchmarks selected to demonstrate the generality and effectiveness of Mapple across representative distributed workloads. Matrix multiplication is one of the most fundamental and heavily studied problems in parallel computing, serving as a cornerstone for many scientific applications. It is also a classic stress test for distributed systems, due to the need to balance computation and communication across complex processor hierarchies. The first six benchmarks therefore, focus on advanced parallel matrix multiplication algorithms: Cannon's [\[Cannon](#page-25-4) [1969\]](#page-25-4), SUMMA [\[Van De Geijn and Watts](#page-27-4) [1997\]](#page-27-4), PUMMA [\[Choi et al.](#page-26-6) [1994\]](#page-26-6), Johnson's [\[Agarwal et al.](#page-25-3) [1995\]](#page-25-3), Solomonik's [\[Solomonik and](#page-27-3) [Demmel](#page-27-3) [2011\]](#page-27-3), and COSMA [\[Kwasniewski et al.](#page-26-7) [2019\]](#page-26-7). They are divided into 2D and non-2D categories. The 2D algorithms, such as Cannon's, PUMMA, and SUMMA, enhance communication efficiency by partitioning matrices into 2D tiles and mapping them onto processor space, utilizing techniques like pipelining. Conversely, non-2D algorithms like Johnson's, Solomonik's, and COSMA, employ 3D partitioning or optimize processor grids to minimize communication overhead and boost performance.

The remaining three benchmarks are drawn from scientific simulation workloads, which represent a different class of distributed applications with distinct computation models and data access patterns. Circuit [\[Bauer et al.](#page-25-8) [2012\]](#page-25-8) simulates electrical circuit behavior by modeling currents and voltages across interconnected nodes and wires. Stencil [\[Van der Wijngaart and Mattson](#page-27-9) [2014\]](#page-27-9) operates on a 2D grid, updating each point's value based on a stencil pattern derived from its neighbors. Pennant [\[Ferenbaugh](#page-26-11) [2015\]](#page-26-11) models unstructured mesh Lagrangian staggered-grid hydrodynamics, essential for simulating compressible flow.

Together, these benchmarks span a wide spectrum of distributed computation patterns. Demonstrating strong results on these diverse workloads provides convincing evidence of Mapple's ability to express complex mapping strategies and deliver high performance across application domains.

#### <span id="page-18-0"></span>6.1 Lines of Code Reduction

We show the lines of code comparison between Mapple and C++ mappers in Table [1.](#page-18-1) For each mapper, we count the number of non-blank, non-comment lines of code written for the Mapple mapper and the C++ mapper. Using Mapple to implement mappers leads to a reduction in lines of code from 406 to 29 on average, i.e., a 14× reduction across all benchmarks. In Figure [12,](#page-19-0) we present example functions utilized by the mappers for distributed matrix-multiplication algorithms. The DSL effectively captures all the mapping decisions made by the C++ mappers in a concise manner.

We manually verify that both approaches make identical mapping decisions and achieve matching performance (i.e., identical throughput), indicating that any overhead introduced by Mapple is negligible. This confirms the fidelity of our Mapple mappers when compared against prior low-level C++ implementations [\[Bauer et al.](#page-25-8) [2012;](#page-25-8) [Yadav et al.](#page-27-10) [2022a\]](#page-27-10).

<span id="page-18-1"></span>Table 1. Lines of Code (LoC) comparison between DSL and C++ mappers. The DSL achieves a 14× average reduction in LoC while maintaining the same performance as the C++ mappers.

| Application                 | 1         | 2         | 3         | 4         | 5         | 6         | 7         | 8         | 9         | Avg.      |
|-----------------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| LoC in C++<br>LoC in Mapple | 347<br>16 | 306<br>14 | 379<br>16 | 447<br>38 | 437<br>38 | 430<br>38 | 428<br>33 | 433<br>38 | 448<br>32 | 406<br>29 |
| LoC Reduction               | 22×       | 22×       | 24×       | 12×       | 12×       | 11×       | 13×       | 11×       | 14×       | 14×       |

<span id="page-19-0"></span>

| Helper functions,<br>Global variable | def block_primitive(Tuple ipoint, Tuple ispace, Tuple pspace, int dim1, int dim2):<br>return ipoint[dim1] * pspace[dim2] / ispace[dim1]<br>def cyclic_primitive(Tuple ipoint, Tuple ispace, Tuple pspace, int dim1, int dim2):<br>return ipoint[dim1] % pspace[dim2]<br>m_2d = Machine(GPU)                                                                                                                                                                                                           |
|--------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Solomonik's<br>(function 1)          | def hierarchical_block3D(Tuple ipoint, Tuple ispace):<br># split the 0th dimension into 3 dimensions<br>m_4d = m_2d.decompose(0, ispace);<br># split the GPU dimension into 3 dimensions<br># sub iteration space for each node: ispace / m_4d[:-1]<br>m_6d = m_4d.decompose(3, ispace / m_4d[:-1])<br>upper = tuple(block_primitive(ipoint, ispace, m_6d, i, i) for i in (0,1,2))<br>lower = tuple(cyclic_primitive(ipoint, ispace, m_6d, i, i + 3) for i in (0,1,2))<br>return m_6d[*upper, *lower] |
| Cannon's<br>PUMMA<br>SUMMA           | def hierarchical_block2D(Tuple ipoint, Tuple ispace):<br># Similar to hierarchical_block3D except for the dimension of iteration space<br>m_3d = m_2d.decompose(0, ispace)<br>m_4d = m_3d.decompose(2, ispace / m_3d[:-1])<br>upper = tuple(block_primitive(ipoint, ispace, m_4d, i, i) for i in (0, 1))<br>lower = tuple(cyclic_primitive(ipoint, ispace, m_4d, i, i + 2) for i in (0, 1))<br>return m_4d[*upper, *lower]                                                                            |
| Solomonik's<br>(function 2)          | def linearize_cyclic(Tuple ipoint, Tuple ispace):<br>linearized = ipoint[0] + ispace[0] * ipoint[1] + ispace[0] * ispace[1] * ipoint[2]<br># cyclic over node dimension and GPU dimension<br>node_idx = linearized % m_2d.size[0]<br>gpu_idx = (linearized / m_2d.size[0]) % m_2d.size[1]<br>return m_2d[node_idx, gpu_idx]                                                                                                                                                                           |
| COSMA                                | def special_linearize3D(Tuple ipoint, Tuple ispace):<br># split the node dimension as equal as possible<br>m_5d = m_2d.decompose(0, (1, 1, 1))<br>gx = m_5d.size[2]<br>gy = m_5d.size[1]<br>linearized = ipoint[0] + ipoint[1] * gx + ipoint[2] * gx * gy<br>return m_2d[linearized % m_2d.size[0], 0]                                                                                                                                                                                                |
| Johnson's                            | def conditional_linearize3D(Tuple ipoint, Tuple ispace):<br>grid_size = ispace[0] > ispace[2] ? ispace[0] : ispace[2]<br>linearized = ipoint[0] + ipoint[1] * grid_size + ipoint[2] * grid_size * grid_size<br>return m_2d[linearized % m_2d.size[0], 0]                                                                                                                                                                                                                                              |

Fig. 12. Example functions used by the mappers for the distributed matrix-multiplication algorithms.

| Application          | 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     |
|----------------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| Mapple Tuned Speedup | 1.34× | 1.02× | 1.04× | 1.09× | 1.09× | 1.09× | 1.09× | 1.07× | 1.31× |

<span id="page-20-1"></span>Table 2. Performance improvements of Mapple mappers over expert-designed C++ mappers.

#### <span id="page-20-0"></span>6.2 Performance Tuning

We demonstrate that Mapple facilitates effective performance tuning by enabling the creation of mappers that outperform expert-designed C++ mappers. As shown in Table [2,](#page-20-1) Mapple achieves up to 1.34× speedup over the baseline C++ implementations. Even with a simple search-based algorithm, Mapple uncovers substantial optimization opportunities, helping users discover mappers that achieve higher throughput.

We analyze the sources of the observed performance improvements. For the six matrix multiplication applications (indexed 4 to 9), the speedup is entirely due to tuning the index mappings, although other features of Mapple contribute to code size reduction. In contrast, for the three scientific applications, the performance gains stem from assigning memories differently than the expert-written mappers—a capability enabled by Mapple's full feature set, as discussed in Section [7.1.](#page-22-0)

We also present a case study highlighting the critical role of precise control over mapping strategies. Our investigation demonstrates significant performance variations resulting from minor alterations in the hierarchical\_block2D function for the Cannon's, PUMMA, and SUMMA algorithms. Specifically, we contrast the mapping function specified by the algorithm with an alternative approach integrating runtime heuristics: the runtime system dynamically assigns the iteration point to one of the four GPUs on the node, selecting the GPU with the least workload at runtime, rather than adhering to a predetermined distribution.

The throughput result depicted in Figure [13](#page-21-0) underscores the pronounced performance discrepancies between the two approaches. The x-axis indicates different machine sizes, and the y-axis indicates the throughput per node. The "Algorithm Specification" line shows the throughput achieved by correctly implementing the corresponding algorithms, which match the performance results reported in prior work. The "Runtime Heuristics" line shows the throughput achieved by the alternative approach. In the 1-node scenario, the slowdown can reach up to 3.5×. This disparity stems from the additional data movement induced by divergent mapping decisions. Furthermore, the runtime heuristics-based mapping leads to out-of-memory (OOM) errors on 32-GPU runs for the PUMMA and SUMMA algorithms, because mapping decisions influence where the data is physically materialized in memory. This example shows that it is important to precisely control the mapping to minimize data movement and optimize memory resource utilization.

#### 6.3 Performance Study of the Decompose Primitive

To investigate the impact of the decompose primitive, we compare it against the default heuristics for reshaping machine spaces from Algorithm [1.](#page-10-1) We evaluate the end-to-end performance of both mappers on stencil-pattern applications, which are a dominant communication pattern in image processing and scientific computing. Each point in the iteration space updates its value based on its nearest neighbors, making these applications highly sensitive to how computations are partitioned and mapped. As a result, demonstrating effectiveness on stencil applications provides strong evidence of the practical benefits of decompose for reducing communication in widely used computational workloads.

To ensure a fair comparison, we systematically vary the 2D stencil parameters as shown in Table [3.](#page-22-1) We test six aspect ratios from 1 : 1 to 1 : 32, reflecting shapes commonly seen in scientific

<span id="page-21-0"></span>![](_page_21_Figure_1.jpeg)

Fig. 13. Throughput comparison between the original hierarchical\_block2D mapping function (specified by the Cannon's, PUMMA, and SUMMA algorithm) and a runtime heuristics-based mapper. The runtime heuristics-based mapping can lead to out-of-memory error (denoted by "OOM").

<span id="page-21-1"></span>![](_page_21_Figure_3.jpeg)

Fig. 14. The distribution of improvement percentage over different configurations.

![](_page_21_Figure_5.jpeg)

Fig. 16. Geometric mean of improvement percentage w.r.t. area of iteration space per node.

![](_page_21_Figure_7.jpeg)

Fig. 15. Geometric mean of improvement percentage w.r.t. aspect ratios of iteration space.

![](_page_21_Figure_9.jpeg)

Fig. 17. Geometric mean of improvement percentage w.r.t. machine sizes.

simulations (e.g., liquid films [\[Nave et al.](#page-26-12) [2010\]](#page-26-12), shock tubes [\[Wong et al.](#page-27-11) [2019\]](#page-27-11)). As discussed in Section [4.1,](#page-9-1) Algorithm [1](#page-10-1) can lead to suboptimal results. We also vary the iteration space per node across five sizes (10<sup>6</sup> to 4 × 10<sup>8</sup> ) to capture different communication-to-computation ratios, and scale the number of GPUs from 4 to 128 in powers of two. In total, we evaluate 180 configurations.

<span id="page-22-1"></span>

| Parameter                                                     | Values                                                                  |  |  |
|---------------------------------------------------------------|-------------------------------------------------------------------------|--|--|
|                                                               |                                                                         |  |  |
| Aspect ratio of total iteration space (𝑥<br>:<br>𝑦)           | 1 : 1,<br>1 : 2,<br>1 : 4,<br>1 : 8,<br>1 : 16,<br>1 : 32               |  |  |
| Area of iteration space per node (𝑥<br>#𝑛𝑜𝑑𝑒𝑠)<br>𝑦<br>∗<br>/ | 106<br>107<br>108<br>108<br>108<br>2<br>4<br>,<br>,<br>,<br>,<br>×<br>× |  |  |
| Number of GPUs                                                | 4, 8, 16, 32, 64, 128                                                   |  |  |

Table 3. Parameter space in performance comparison

<span id="page-22-2"></span>Terminals: TaskName, RegionName, var, int

| Program    | ::=<br>Statement+                                                                              |
|------------|------------------------------------------------------------------------------------------------|
| Statement  | ::=<br>TaskMap<br>  DataMap<br>  DataLayout<br>  FuncDef<br>                                   |
|            | IndexTaskMap TaskName var<br>                                                                  |
| TaskMap    | ::=<br>Task TaskName Proc+                                                                     |
| DataMap    | ::=<br>Region TaskName RegionName Proc Memory+                                                 |
| Proc       | ::=<br>CPU<br>  GPU<br>  OMP                                                                   |
| Memory     | ::=<br>SYSMEM<br>  FBMEM<br>  ZCMEM                                                            |
| DataLayout | ::=<br>Layout TaskName RegionName Proc Constraint+                                             |
| Constraint | ::=<br>SOA<br>  AOS<br>  C_order<br>  F_order<br>  Align == int                                |
| FuncDef    | ::=<br>def var(var+): FuncStmt+                                                                |
| FuncStmt   | ::=<br>var = Expr<br>  return Expr                                                             |
| Expr       | ::=<br>var<br>  var(Expr+)<br>  Machine(Proc)<br>  Expr.Expr<br>  Expr Op Expr<br>  (Expr)<br> |
|            | Expr[Expr]<br>  *Expr<br>  Expr ? Expr : Expr<br>  Primitive                                   |
| Primitive  | ::=<br>split<br>  merge<br>  reorder<br>  slice<br>  decompose                                 |
|            |                                                                                                |

Fig. 18. Mapple's Grammar

We report the percentage improvement, with the distribution shown in Figure [14,](#page-21-1) ranging from 0% to 83%. The geometric mean improvement is 16%. We also plot the geometric mean improvement across varying values of each parameter. In Figure [15,](#page-21-1) the y-axis shows the geometric mean improvement percentage, and the x-axis shows the iteration space aspect ratio. As the ratio increases from 1 : 1 to 1 : 32, the improvement rises from 7% to 27%, highlighting the growing advantage of mappers using the decompose primitive. This trend is expected, as Algorithm [1](#page-10-1) evenly splits processors—a strategy that performs well when the iteration space is nearly square.

Figure [16](#page-21-1) shows how the improvement percentage varies with the iteration space size per node. As the area increases from 10<sup>6</sup> to 4 × 10<sup>8</sup> , the improvement drops from 32% to 5%. Larger iteration spaces reduce the communication-to-computation ratio, diminishing the relative benefit of minimizing communication. Figure [17](#page-21-1) plots improvement against the number of nodes and GPUs. The improvement peaks at 26% on 4 nodes (16 GPUs). From 1 to 4 nodes, growing internode communication enhances the benefit of reducing bandwidth-bound traffic. Beyond 4 nodes, inter-rack latency becomes the bottleneck, diminishing the impact of improved mappings.

#### 7 Discussion

#### <span id="page-22-0"></span>7.1 Additional Performance Optimization Features in Mapple

While Section [3](#page-3-0) and Section [4](#page-9-0) focus on index mapping, the full optimization space includes several additional dimensions that contribute to the performance gains observed in the three scientific applications evaluated in Section [6.2.](#page-20-0) Figure [18](#page-22-2) presents a simplified grammar capturing the core constructs of Mapple. A primary axis of control is processor selection, expressed via the TaskMap directive. This determines whether a given task is assigned to GPUs, CPUs, or the OpenMP runtime. The decision is made per task and depends on factors such as task granularity, GPU memory capacity, and kernel launch overhead. For example, small tasks may favor CPUs to avoid the cost of GPU launches, and large-memory tasks may also favor CPUs if they exceed GPU memory limits.

Another critical dimension is memory placement, specified through the DataMap construct. This governs where task arguments are stored: in GPU FrameBuffer memory for high-speed access, in ZeroCopy regions to enable CPU-GPU sharing, or in host memory when capacity is a concern. Each location involves trade-offs between access latency, available memory, and transfer costs. This mapping is defined per task and per argument.

The optimization space also includes memory layout, configured with the DataLayout statement. This involves selecting between layouts such as Struct of Arrays (SoA) versus Array of Structures (AoS), specifying memory ordering (e.g., Fortran vs. C order), and applying alignment constraints (e.g., 128-byte alignment). These layout choices directly impact cache behavior and performance. This decision is made per task, per data region, and per target processor. Additional features, not shown in the figure, include scheduling policies (e.g., task prioritization), garbage collection strategies, and load-balancing directives.

#### 7.2 Generalization of the Decompose Primitive

While Section [4](#page-9-0) addresses isotropic communication across iteration space dimensions, the decompose primitive can be extended to anisotropic patterns, such as uneven halo widths and dimension-specific all-to-all exchanges (e.g., for data transposes). This generalization supports a much wider range of computations on block-structured grids. In both cases, only the objective function changes; the same minimization procedure from Section [4.3](#page-13-0) still applies.

7.2.1 Anisotropic halo communication. To generalize, we extend the communication area from Section [4.2](#page-10-0) to a volume that captures the total data exchanged between distributed machines. In isotropic cases, where halo width is uniform, volume reduces to area. For anisotropic patterns, varying halo width across dimensions must be considered. Letting ℎ denote the width in dimension , the total communication volume in a -dimensional space is = Í =1 ℎ Î ≠ . For = , the volume, , can also be expressed as = Í =1 ℎ Î =1 , where Î =1 is constant for a certain iteration space.

7.2.2 Transpose via all-to-all communication along certain dimensions. For transpose operations via all-to-all communication, the data volume per partition along the -th dimension is given by = −1 Î =1. This reflects that each partition splits its data evenly into groups, one of which remains local while the others are sent to distinct peers along the -th dimension. Thus, the total communication volume within a pencil of aligned processors is . To optimize the mapping, we minimize the total communication volume, + Í ∈<sup>T</sup> ∗ , where T denotes the set of dimensions requiring transposes. The volume along dimension , ∗ , is defined as:

$$
V_n^* = v_n \prod_{m=1}^k d_{i_m} = \left(1 - \frac{1}{d_{i_n}}\right) \left(\prod_{m=1}^k w_m\right) d_i
$$

.

## 8 Related Work

#### 8.1 Task-based Programming Systems

Table [4](#page-24-0) summarizes mapping features in HPC systems, all of which are supported by Mapple. Systems like Legion [\[Bauer et al.](#page-25-8) [2012\]](#page-25-8) and StarPU [\[Augonnet et al.](#page-25-1) [2009\]](#page-25-1) offer low-level C/C++

| Systems                             | Task Placement | Data Placement | Data Layout | Scheduling | Load Balancing |
|-------------------------------------|----------------|----------------|-------------|------------|----------------|
| Legion [Bauer et al.<br>2012]       | ✓              | ✓              | ✓           | ✓          | ✓              |
| StarPU [Augonnet et al.<br>2010]    | ✓              | ✓              | ✓           | ✓          | ✓              |
| Chapel [Chamberlain et al.<br>2007] | ✓              |                | ✓           |            | ✓              |
| Charm++ [Kale and Krishnan<br>1993] | ✓              | ✓              |             | ✓          | ✓              |
| PaRSEC [Danalis et al.<br>2015]     | ✓              |                | ✓           | ✓          | ✓              |
| X10 [Charles et al.<br>2005]        | ✓              | ✓              |             | ✓          | ✓              |
| HPX [Heller et al.<br>2017]         | ✓              | ✓              |             | ✓          | ✓              |
| OpenMP [Chandra et al.<br>2001]     | ✓              |                |             |            | ✓              |
| OmpSs [Duran et al.<br>2011]        | ✓              |                |             | ✓          | ✓              |
| Mapple                              | ✓              | ✓              | ✓           | ✓          | ✓              |

<span id="page-24-0"></span>Table 4. Mapping features exposed to application control by different systems and covered by Mapple.

mapping interfaces via asynchronous callbacks, requiring deep familiarity with internal runtime abstractions. Other HPC systems offer less direct mapping control. ZPL [\[Deitz et al.](#page-26-15) [2004\]](#page-26-15), Chapel [\[Chamberlain et al.](#page-25-2) [2007\]](#page-25-2), and X10 [\[Charles et al.](#page-25-11) [2005\]](#page-25-11) expose predefined data distributions (e.g., blocked, cyclic). HPX [\[Heller et al.](#page-26-14) [2017\]](#page-26-14) and PaRSEC [\[Danalis et al.](#page-26-13) [2015\]](#page-26-13) rely on schedulers with tunable policies for heterogeneous execution. OpenMP [\[Chandra et al.](#page-25-12) [2001\]](#page-25-12), OmpSs [\[Duran](#page-26-2) [et al.](#page-26-2) [2011\]](#page-26-2), Charm++ [\[Kale and Krishnan](#page-26-5) [1993\]](#page-26-5), and Chapel's low-level APIs allow programmers to specify task placement directly in the source code, ranging from processor annotations (OpenMP, OmpSs) to custom C++ mapping classes (Charm++). Task-based systems in data analytics and AI such as MapReduce [\[Dean and Ghemawat](#page-26-16) [2008\]](#page-26-16), Spark [\[Zaharia et al.](#page-27-12) [2010\]](#page-27-12), Dask [\[Rocklin](#page-27-13) [2015\]](#page-27-13), Ray [\[Moritz et al.](#page-26-17) [2018\]](#page-26-17), and Pathways [\[Barham et al.](#page-25-13) [2022\]](#page-25-13) rely on runtime mapping heuristics. This eases development but limits performance tuning when heuristics fall short. Mapple offers similar ease of use while enabling fine-grained control.

#### 8.2 Loop Transformations

A large body of work on compiler algorithms uses loop transformations to maximize parallelism and minimize communication, including unimodular transformations [\[Banerjee](#page-25-14) [2007;](#page-25-14) [Banerjee et al.](#page-25-15) [1990;](#page-25-15) [Wolf](#page-27-6) [1992;](#page-27-6) [Wolf and Lam](#page-27-14) [1991b;](#page-27-14) [Wolfe](#page-27-15) [1982\]](#page-27-15) (i.e., combinations of loop interchange, reversal, and skewing), loop block/tiling, fusion, fission, reindexing, and scaling [\[Lim et al.](#page-26-18) [1999,](#page-26-18) [2001;](#page-26-19) [Wolf](#page-27-16) [and Lam](#page-27-16) [1991a;](#page-27-16) [Xue](#page-27-17) [2000\]](#page-27-17), and affine transformations used by the polyhedral model [\[Acharya et al.](#page-25-16) [2018;](#page-25-16) [Bondhugula](#page-25-17) [2013;](#page-25-17) [Bondhugula et al.](#page-25-18) [2008\]](#page-25-18). These works share a similar motivation to Mapple, though the goals and specific transformations are different as explained in Section [1.](#page-0-0)

Another line of work uses scheduling languages [\[Baghdadi et al.](#page-25-19) [2019;](#page-25-19) [Chen et al.](#page-26-8) [2008,](#page-26-8) [2018a;](#page-26-9) [Kjolstad et al.](#page-26-20) [2017;](#page-26-20) [Ragan-Kelley et al.](#page-26-21) [2012;](#page-26-21) [Senanayake et al.](#page-27-7) [2020;](#page-27-7) [Yadav et al.](#page-27-18) [2022b;](#page-27-18) [Zhang](#page-27-8) [et al.](#page-27-8) [2018\]](#page-27-8) to separate the algorithm from the schedule, based on traditional loop optimizations such as split, collapse, and loop reordering. This separation enables programmers to just change the schedule when moving to different hardware. A growing body of research focuses on autoscheduling [\[Adams et al.](#page-25-20) [2019;](#page-25-20) [Chen et al.](#page-26-22) [2018b;](#page-26-22) [Mullapudi et al.](#page-26-23) [2016;](#page-26-23) [Tollenaere et al.](#page-27-19) [2023;](#page-27-19) [Zheng](#page-27-20) [et al.](#page-27-20) [2020,](#page-27-20) [2022\]](#page-27-21) to reduce the manual effort required in writing schedules. Task-based systems share a similar approach in separating the algorithm from its schedule or mapping. However, prior work on scheduling languages does not address the challenges of mapping iteration spaces onto processor spaces of differing dimensionality in distributed settings.

#### 9 Conclusion

We present Mapple, a high-level interface that simplifies the development of mappers for task-based parallel systems. Mapple overcomes key limitations of existing mapping interfaces by abstracting low-level runtime details while exposing transformation primitives to resolve dimensionality mismatches between iteration and processor spaces. At the core of Mapple is the decompose primitive, which minimizes communication volume based on theoretical analysis. Across nine distributed applications, Mapple reduces mapper code size by 14× and achieves up to 1.34× speedup over expert-written C++ mappers. Moreover, the decompose primitive outperforms existing heuristics by up to 1.83×. These results demonstrate the practical effectiveness of Mapple in enabling the development of concise, high-performance mappers for distributed applications.

#### References

- <span id="page-25-16"></span>Aravind Acharya, Uday Bondhugula, and Albert Cohen. 2018. Polyhedral auto-transformation with no integer linear programming. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation. 529–542.
- <span id="page-25-20"></span>Andrew Adams, Karima Ma, Luke Anderson, Riyadh Baghdadi, Tzu-Mao Li, Michaël Gharbi, Benoit Steiner, Steven Johnson, Kayvon Fatahalian, Frédo Durand, et al. 2019. Learning to optimize halide with tree search and random programs. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1–12.
- <span id="page-25-9"></span>Adaptive MPI 2025. Adaptive MPI. [https://charm.readthedocs.io/en/latest/ampi/04-extensions.html#user-defined-initial](https://charm.readthedocs.io/en/latest/ampi/04-extensions.html##user-defined-initial-mapping)[mapping.](https://charm.readthedocs.io/en/latest/ampi/04-extensions.html##user-defined-initial-mapping)
- <span id="page-25-3"></span>Ramesh C Agarwal, Susanne M Balle, Fred G Gustavson, Mahesh Joshi, and Prasad Palkar. 1995. A three-dimensional approach to parallel matrix multiplication. IBM Journal of Research and Development 39, 5 (1995), 575–582.
- <span id="page-25-7"></span>Willow Ahrens, Fredrik Kjolstad, and Saman Amarasinghe. 2022. Autoscheduling for sparse tensor algebra with an asymptotic cost model. In Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation. 269–285.
- <span id="page-25-10"></span>Cédric Augonnet, Samuel Thibault, and Raymond Namyst. 2010. StarPU: a runtime system for scheduling tasks over accelerator-based multicore machines. Ph. D. Dissertation. INRIA.
- <span id="page-25-1"></span>Cédric Augonnet, Samuel Thibault, Raymond Namyst, and Pierre-André Wacrenier. 2009. StarPU: a unified platform for task scheduling on heterogeneous multicore architectures. In European Conference on Parallel Processing. Springer, 863–874.
- <span id="page-25-19"></span>Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo, Abdurrahman Akkas, Yunming Zhang, Patricia Suriana, Shoaib Kamil, and Saman Amarasinghe. 2019. Tiramisu: A polyhedral compiler for expressing fast and portable code. In 2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO). IEEE, 193–205.
- <span id="page-25-14"></span>Utpal Banerjee. 2007. Loop transformations for restructuring compilers: the foundations. Springer Science & Business Media.
- <span id="page-25-15"></span>Utpal Banerjee et al. 1990. Unimodular transformations of double loops. University of Illinois at Urbana-Champaign, Center for Supercomputing . . . .
- <span id="page-25-13"></span>Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, et al. 2022. Pathways: Asynchronous distributed dataflow for ML. Proceedings of Machine Learning and Systems 4 (2022), 430–449.
- <span id="page-25-8"></span>Michael Bauer, Sean Treichler, Elliott Slaughter, and Alex Aiken. 2012. Legion: Expressing locality and independence with logical regions. In SC'12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. IEEE, 1–11.
- <span id="page-25-17"></span>Uday Bondhugula. 2013. Compiling affine loop nests for distributed-memory parallel architectures. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. 1–12.
- <span id="page-25-18"></span>Uday Bondhugula, Albert Hartono, Jagannathan Ramanujam, and Ponnuswamy Sadayappan. 2008. A practical automatic polyhedral parallelizer and locality optimizer. In Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation. 101–113.
- <span id="page-25-4"></span>Lynn Elliot Cannon. 1969. A cellular computer to implement the Kalman filter algorithm. Montana State University.
- <span id="page-25-2"></span>Bradford L Chamberlain, David Callahan, and Hans P Zima. 2007. Parallel programmability and the chapel language. The International Journal of High Performance Computing Applications 21, 3 (2007), 291–312.
- <span id="page-25-5"></span>Bradford L Chamberlain, Sung-Eun Choi, Steven J Deitz, David Iten, and Vassily Litvinov. 2011. Authoring user-defined domain maps in Chapel. In Cray Users Group Conference (CUG).
- <span id="page-25-6"></span>Bradford L Chamberlain, Steven J Deitz, David Iten, and Sung-Eun Choi. 2010. User-defined distributions and layouts in Chapel: Philosophy and framework. In Proceedings of the 2nd USENIX conference on Hot topics in parallelism. 12–12.
- <span id="page-25-0"></span>Rohit Chandra. 2001. Parallel programming in OpenMP. Morgan kaufmann.
- <span id="page-25-12"></span>Rohit Chandra, Leo Dagum, David Kohr, Ramesh Menon, Dror Maydan, and Jeff McDonald. 2001. Parallel programming in OpenMP. Morgan kaufmann.
- <span id="page-25-11"></span>Philippe Charles, Christian Grothoff, Vijay Saraswat, Christopher Donawa, Allan Kielstra, Kemal Ebcioglu, Christoph Von Praun, and Vivek Sarkar. 2005. X10: an object-oriented approach to non-uniform cluster computing. Acm Sigplan Notices 40, 10 (2005), 519–538.

- <span id="page-26-8"></span>Chun Chen, Jacqueline Chame, and Mary Hall. 2008. A framework for composing high-level loop transformations. Technical Report 08–897, USC Computer Science Technical Report (2008).
- <span id="page-26-9"></span>Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018a. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 578–594.
- <span id="page-26-22"></span>Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018b. Learning to optimize tensor programs. Advances in Neural Information Processing Systems 31 (2018).
- <span id="page-26-6"></span>Jaeyoung Choi, David W Walker, and Jack J Dongarra. 1994. PUMMA: Parallel universal matrix multiplication algorithms on distributed memory concurrent computers. Concurrency: Practice and Experience 6, 7 (1994), 543–570.
- <span id="page-26-0"></span>Leonardo Dagum and Ramesh Menon. 1998. OpenMP: an industry standard API for shared-memory programming. IEEE computational science and engineering 5, 1 (1998), 46–55.
- <span id="page-26-13"></span>Anthony Danalis, Heike Jagode, George Bosilca, and Jack Dongarra. 2015. Parsec in practice: Optimizing a legacy chemistry application through distributed task-based execution. In 2015 IEEE International Conference on Cluster Computing. IEEE, 304–313.
- <span id="page-26-16"></span>Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing on large clusters. Commun. ACM 51, 1 (2008), 107–113.
- <span id="page-26-15"></span>Steven J Deitz, Bradford L Chamberlain, and Lawrence Snyder. 2004. Abstractions for dynamic data distribution. In Ninth International Workshop on High-Level Parallel Programming Models and Supportive Environments, 2004. Proceedings. IEEE, 42–51.
- <span id="page-26-2"></span>Alejandro Duran, Eduard Ayguadé, Rosa M Badia, Jesús Labarta, Luis Martinell, Xavier Martorell, and Judit Planas. 2011. Ompss: a proposal for programming heterogeneous multi-core architectures. Parallel processing letters 21, 02 (2011), 173–193.
- <span id="page-26-3"></span>Kayvon Fatahalian, Daniel Reiter Horn, Timothy J Knight, Larkhoon Leem, Mike Houston, Ji Young Park, Mattan Erez, Manman Ren, Alex Aiken, William J Dally, et al. 2006. Sequoia: Programming the memory hierarchy. In Proceedings of the 2006 ACM/IEEE Conference on Supercomputing. 83–es.
- <span id="page-26-11"></span>Charles R Ferenbaugh. 2015. PENNANT: an unstructured mesh mini-app for advanced architecture research. Concurrency and Computation: Practice and Experience 27, 17 (2015), 4555–4572.
- <span id="page-26-1"></span>William Gropp, Ewing Lusk, and Anthony Skjellum. 1999. Using MPI: portable parallel programming with the message-passing interface. Vol. 1. MIT press.
- <span id="page-26-14"></span>Thomas Heller, Patrick Diehl, Zachary Byerly, John Biddiscombe, and Hartmut Kaiser. 2017. Hpx–an open source c++ standard library for parallelism and concurrency. Proceedings of OpenSuCo 5 (2017).
- <span id="page-26-4"></span>Hartmut Kaiser, Thomas Heller, Bryce Adelstein-Lelbach, Adrian Serio, and Dietmar Fey. 2014. Hpx: A task based programming model in a global address space. In Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models. 1–11.
- <span id="page-26-5"></span>Laxmikant V Kale and Sanjeev Krishnan. 1993. Charm++ a portable concurrent object oriented system based on c++. In Proceedings of the eighth annual conference on Object-oriented programming systems, languages, and applications. 91–108.
- <span id="page-26-10"></span>Keyword Distribution 2025. Keyword Distribution. [https://chapel-lang.org/docs/1.28/modules/dists/BlockDist.html.](https://chapel-lang.org/docs/1.28/modules/dists/BlockDist.html)
- <span id="page-26-20"></span>Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. 2017. The tensor algebra compiler. Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1–29.
- <span id="page-26-7"></span>Grzegorz Kwasniewski, Marko Kabić, Maciej Besta, Joost VandeVondele, Raffaele Solcà, and Torsten Hoefler. 2019. Red-blue pebbling revisited: near optimal parallel matrix-matrix multiplication. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 1–22.
- <span id="page-26-18"></span>Amy W Lim, Gerald I Cheong, and Monica S Lam. 1999. An affine partitioning algorithm to maximize parallelism and minimize communication. In Proceedings of the 13th international conference on Supercomputing. 228–237.
- <span id="page-26-19"></span>Amy W Lim, Shih-Wei Liao, and Monica S Lam. 2001. Blocking and array contraction across arbitrarily nested loops using affine partitioning. In Proceedings of the eighth ACM SIGPLAN symposium on Principles and practices of parallel programming. 103–112.
- <span id="page-26-17"></span>Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. 2018. Ray: A distributed framework for emerging {AI} applications. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 561–577.
- <span id="page-26-23"></span>Ravi Teja Mullapudi, Andrew Adams, Dillon Sharlet, Jonathan Ragan-Kelley, and Kayvon Fatahalian. 2016. Automatically scheduling halide image processing pipelines. ACM Transactions on Graphics (TOG) 35, 4 (2016), 1–11.
- <span id="page-26-12"></span>J-C Nave, XD Liu, and Sanjoy Banerjee. 2010. Direct numerical simulation of liquid films with large interfacial deformation. Studies in Applied Mathematics 125, 2 (2010), 153–177.
- <span id="page-26-21"></span>Jonathan Ragan-Kelley, Andrew Adams, Sylvain Paris, Marc Levoy, Saman Amarasinghe, and Frédo Durand. 2012. Decoupling algorithms from schedules for easy optimization of image processing pipelines. ACM Transactions on Graphics (TOG) 31, 4 (2012), 1–12.

- <span id="page-27-5"></span>Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. Acm Sigplan Notices 48, 6 (2013), 519–530.
- <span id="page-27-13"></span>Matthew Rocklin. 2015. Dask: Parallel computation with blocked algorithms and task scheduling. In Proceedings of the 14th python in science conference, Vol. 130. Citeseer, 136.
- <span id="page-27-0"></span>Jason Sanders and Edward Kandrot. 2010. CUDA by example: an introduction to general-purpose GPU programming. Addison-Wesley Professional.
- <span id="page-27-7"></span>Ryan Senanayake, Changwan Hong, Ziheng Wang, Amalee Wilson, Stephen Chou, Shoaib Kamil, Saman Amarasinghe, and Fredrik Kjolstad. 2020. A sparse iteration space transformation framework for sparse tensor algebra. Proceedings of the ACM on Programming Languages 4, OOPSLA (2020), 1–30.
- <span id="page-27-2"></span>Elliott Slaughter, Wonchan Lee, Sean Treichler, Michael Bauer, and Alex Aiken. 2015. Regent: A high-productivity programming language for HPC with logical regions. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 1–12.
- <span id="page-27-3"></span>Edgar Solomonik and James Demmel. 2011. Communication-optimal parallel 2.5 D matrix multiplication and LU factorization algorithms. In Euro-Par 2011 Parallel Processing: 17th International Conference, Euro-Par 2011, Bordeaux, France, August 29-September 2, 2011, Proceedings, Part II 17. Springer, 90–109.
- <span id="page-27-19"></span>Nicolas Tollenaere, Guillaume Iooss, Stéphane Pouget, Hugo Brunie, Christophe Guillon, Albert Cohen, P Sadayappan, and Fabrice Rastello. 2023. Autotuning convolutions is easier than you think. ACM Transactions on Architecture and Code Optimization 20, 2 (2023), 1–24.
- <span id="page-27-4"></span>Robert A Van De Geijn and Jerrell Watts. 1997. SUMMA: Scalable universal matrix multiplication algorithm. Concurrency: Practice and Experience 9, 4 (1997), 255–274.
- <span id="page-27-9"></span>Rob F Van der Wijngaart and Timothy G Mattson. 2014. The parallel research kernels. In 2014 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 1–6.
- <span id="page-27-6"></span>Michael Edward Wolf. 1992. Improving locality and parallelism in nested loops. stanford university.
- <span id="page-27-16"></span>Michael E Wolf and Monica S Lam. 1991a. A data locality optimizing algorithm. In Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation. 30–44.
- <span id="page-27-14"></span>Michael E Wolf and Monica S Lam. 1991b. A loop transformation theory and an algorithm to maximize parallelism. IEEE Transactions on Parallel & Distributed Systems 2, 04 (1991), 452–471.
- <span id="page-27-15"></span>Michael Joseph Wolfe. 1982. Optimizing supercompilers for supercomputers. University of Illinois at Urbana-Champaign.
- <span id="page-27-11"></span>Man Long Wong, Daniel Livescu, and Sanjiva K Lele. 2019. High-resolution Navier-Stokes simulations of Richtmyer-Meshkov instability with reshock. Physical Review Fluids 4, 10 (2019), 104609.
- <span id="page-27-17"></span>Jingling Xue. 2000. Loop tiling for parallelism. Vol. 575. Springer Science & Business Media.
- <span id="page-27-10"></span>Rohan Yadav, Alex Aiken, and Fredrik Kjolstad. 2022a. DISTAL: the distributed tensor algebra compiler. In Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation. 286–300.
- <span id="page-27-18"></span>Rohan Yadav, Alex Aiken, and Fredrik Kjolstad. 2022b. SpDISTAL: Compiling distributed sparse tensor computations. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–15.
- <span id="page-27-1"></span>Chao-Tung Yang, Chih-Lin Huang, and Cheng-Fang Lin. 2011. Hybrid CUDA, OpenMP, and MPI parallel programming on multicore GPU clusters. Computer Physics Communications 182, 1 (2011), 266–269.
- <span id="page-27-12"></span>Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, Ion Stoica, et al. 2010. Spark: Cluster computing with working sets. HotCloud 10, 10-10 (2010), 95.
- <span id="page-27-8"></span>Yunming Zhang, Mengjiao Yang, Riyadh Baghdadi, Shoaib Kamil, Julian Shun, and Saman Amarasinghe. 2018. Graphit: A high-performance graph dsl. Proceedings of the ACM on Programming Languages 2, OOPSLA (2018), 1–30.
- <span id="page-27-20"></span>Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, et al. 2020. Ansor: Generating {High-Performance} tensor programs for deep learning. In 14th USENIX symposium on operating systems design and implementation (OSDI 20). 863–879.
- <span id="page-27-21"></span>Size Zheng, Renze Chen, Anjiang Wei, Yicheng Jin, Qin Han, Liqiang Lu, Bingyang Wu, Xiuhong Li, Shengen Yan, and Yun Liang. 2022. AMOS: enabling automatic mapping for tensor computations on spatial accelerators with hardware abstraction. In Proceedings of the 49th Annual International Symposium on Computer Architecture. 874–887.