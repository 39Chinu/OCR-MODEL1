# Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains

Anisha Gunjal Anthony Wang\* Elaine Lau Vaskar Nath Bing Liu Sean Hendryx Scale AI anisha.gunjal@scale.com

Abstract

Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world tasks often requires balancing objective and subjective evaluation criteria. However, many such tasks lack a single, unambiguous ground truth—making it difficult to define reliable reward signals for post-training language models. While traditional preference-based methods offer a workaround, they rely on opaque reward functions that are difficult to interpret and prone to spurious correlations. We introduce Rubrics as Rewards (*RaR*), a framework that uses structured, checklist-style rubrics as interpretable reward signals for on-policy training with GRPO. Our best RaR method yields up to a 28% relative improvement on HealthBench-1k compared to simple Likertbased approaches, while matching or surpassing the performance of reward signals derived from expert-written references. By treating rubrics as structured reward signals, we show that RaR enables smaller-scale judge models to better align with human preferences and sustain robust performance across model scales.

# 1 Introduction

Reinforcement Learning with Verifiable Rewards (RLVR) has enabled large language models to elicit complex reasoning on tasks with verifiable outcomes, most prominently in mathematics and coding, where a reward model can be replaced with a scoring function and final answers are easily verifiable or test cases provide automated mechanisms for evaluation [\(Lambert et al.,](#page-8-0) [2024;](#page-8-0) [Guo et al.,](#page-8-1) [2025a;](#page-8-1) [Cui et al.,](#page-8-2) [2025\)](#page-8-2). However, many real-world tasks lack such explicit verifiable answers, leaving models without direct reward feedback. In practice, researchers often turn to RLHF via preference ranking—collecting human judgments over pairs or lists of model outputs to fill this gap. While preferencebased reward models can bootstrap performance,

they tend to overfit superficial artifacts (e.g. response length, formatting quirks, annotator biases) [\(Singhal et al.,](#page-9-0) [2023;](#page-9-0) [Wang et al.,](#page-9-1) [2024;](#page-9-1) [Chen et al.,](#page-8-3) [2024b;](#page-8-3) [Ye et al.,](#page-9-2) [2024;](#page-9-2) [Gudibande et al.,](#page-8-4) [2023\)](#page-8-4) and require large volumes of pairwise comparisons [\(Ouyang et al.,](#page-9-3) [2022\)](#page-9-3), making preference-based reward models brittle and costly.

To address this gap, we introduce Rubrics as Rewards *(RaR)* for on-policy Reinforcement Learning that treats structured criteria or *rubrics* as the core reward mechanism. We use rubrics as checklists [\(Arora et al.,](#page-8-5) [2025;](#page-8-5) [Sirdeshmukh et al.,](#page-9-4) [2025\)](#page-9-4) composed of independent subgoals allowing for automatable feedback aligned with expert intent. By decomposing "what makes a good response" into tangible, human-interpretable criteria, rubrics offer a middle ground between binary correctness signals and coarse preference rankings.

Previous works train generative reward models that learn to evaluate reasoning or final outputs with interpretable scores [\(Chen et al.,](#page-8-6) [2025;](#page-8-6) [White](#page-9-5)[house et al.,](#page-9-5) [2025;](#page-9-5) [Anugraha et al.,](#page-8-7) [2025;](#page-8-7) [Guo et al.,](#page-8-8) [2025b\)](#page-8-8), and some have even used a model's internal confidence estimates as a proxy for reward [\(Zhao](#page-9-6) [et al.,](#page-9-6) [2025\)](#page-9-6). Concurrently, recent efforts have extended verifiable datasets beyond STEM domains, broadening the applicability of RLVR methods to a wider range of tasks [\(Su et al.,](#page-9-7) [2025b;](#page-9-7) [Ma et al.,](#page-8-9) [2025\)](#page-8-9). However, developing a general-purpose approach for specifying reliable and scalable rewards remains an open challenge, particularly in settings where no single ground truth exists, or where both subjective and objective criteria must be considered to evaluate response quality. The Rubrics as Rewards strategy offers a flexible solution by repurposing structured evaluation criteria into multidimensional reward signals. Figure [1](#page-1-0) illustrates our approach for generating rubrics and leveraging them as reward signals for on-policy training.

Our key contributions are as follows: (i) We introduce Rubrics as Rewards (RaR), an on-

<sup>\*</sup>Work done while at Scale AI.

<span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)

Figure 1: Overview of Rubrics as Rewards (RaR). (i) Rubric Generation: We synthesize prompt-specific, selfcontained rubric criteria using a strong LLM guided by four core design principles, with reference answers serving as proxies for expert supervision. (ii) GRPO Training: These rubrics are used to prompt an LLM judge for reward estimation, which drives policy optimization via the GRPO on-policy learning loop.

policy reinforcement learning framework that uses checklist-style rubrics to supervise multi-criteria tasks, enabling stable training and improved performance in both reasoning and real-world domains. (ii) We apply our rubric generation approach to the medicine and science domains, producing two training datasets—*RaR-Medicine-20k* and *RaR-Science-20k*[1](#page-1-1) . (iii) We show that models trained with *RaR* match or outperform strong baselines, achieving notable gains in accuracy across diverse domains. (iv) By using rubrics as structured reward signals, *RaR* enables smaller judge models to achieve better alignment with human preferences and maintains robust performance across different model scales.

### 2 Rubrics as Rewards

#### <span id="page-1-3"></span>2.1 Problem Formulation

Let x denote an input prompt and yˆ ∼ πθ(· | x) be a sampled response from a model parameterized by θ. In domains without single ground-truth answers or automatic correctness signals, we define a structured reward function using prompt-specific rubric criteria.

Each prompt x is associated with a set of k rubric items {(w<sup>j</sup> , c<sup>j</sup> )} k <sup>j</sup>=1, where <sup>w</sup><sup>j</sup> <sup>∈</sup> <sup>R</sup> denotes the weight of criterion j, and c<sup>j</sup> : (x, yˆ) 7→ {0, 1} is a binary correctness function that indicates whether the response yˆ satisfies that criterion given the prompt. The final normalized scalar reward is computed as:

<span id="page-1-2"></span>
$$
r(x, \hat{y}) = \frac{\sum_{j=1}^{k} w_j \cdot c_j(x, \hat{y})}{\sum_{j=1}^{k} w_j}
$$
 (1)

This normalization ensures reward values are consistent across prompts with varying numbers and weights of rubric items.

Explicit Rubric Aggregation. Each criterion is evaluated independently using a generative reward model, and the reward is computed using Equation [1.](#page-1-2) While we currently assume binary values for c<sup>j</sup> , this framework supports extension to continuous-valued judgments.

Implicit Rubric Aggregation. An alternative approach passes all rubric descriptions and weights to a generative reward model, which internally computes a scalar reward:

$$
r_{\text{implicit}}(x, \hat{y}) = f_{\phi}(x, \hat{y}, \{(w_j, d_j)\}_{j=1}^k)
$$
 (2)

Here, f<sup>ϕ</sup> denotes an LLM-based judge that takes as input the prompt x, the generated response yˆ, and a set of weighted rubric criteria. Each pair (w<sup>j</sup> , d<sup>j</sup> ) consists of a scalar or categorical weights w<sup>j</sup> ∈ R and a rubric criterion d<sup>j</sup> describing a specific aspect of response quality. This formulation delegates the aggregation of evaluation criteria to the model itself, allowing it to compute a holistic reward score.

In practice, we use an LLM-as-judge with separate prompt templates containing prompt, response, and rubric items for explicit and implicit aggregation. The prompts used in our experiments are detailed in Appendix [E.](#page-12-0)

## 2.2 Generalization of RLVR with Rubrics as Rewards

Rubric-based reinforcement learning extends the standard RLVR (Reinforcement Learning with Verifiable Rewards) setting by supporting multidimensional, prompt-specific evaluation criteria. We formalize this relationship below.

Remark 1 (Rubrics as Rewards subsumes RLVR). The RLVR setting is a special case of rubric-based rewards defined in Equation [1,](#page-1-2) where

<span id="page-1-1"></span><sup>1</sup>The datasets will be released soon.

k = 1, w<sup>1</sup> = 1, and c1(x, yˆ) reduces to a single verifiable correctness function that compares the model output yˆ against the known correct answer y. For example, this could involve exact match or test case execution. Formally:

$$
r_{\text{RLVR}}(x, \hat{y}) = \text{match}(y, \hat{y}) \tag{3}
$$

where match(y, yˆ) ∈ {0, 1} indicates whether the response satisfies the verifiable correctness condition.

Rubric-based reward functions thus generalize RLVR by enabling multi-dimensional supervision, flexible weighting across criteria, and the incorporation of both objective and subjective aspects of response quality.

This formalization highlights that RLVR can be seen as a restricted instance of rubric-guided RL with a single essential criterion. In contrast, rubricbased rewards further enable structured supervision in settings where correctness is multifaceted and may not be strictly verifiable.

### <span id="page-2-0"></span>3 Rubric Generation

#### 3.1 Desiderata

We synthesize prompt-specific rubrics guided by a set of design principles. Each rubric defines the criteria for a high-quality response and enables human-interpretable supervision. We use large language models (LLMs) as expert proxies to generate these rubrics while ensuring adherence to the following desiderata:

Grounded in Expert-guidance Reference answers produced by human experts or stronger LLMs serve as proxies for expert guidance. Rubrics are grounded in these references to capture the key facts, reasoning steps, and conclusions necessary for correctness.

Comprehensive coverage Rubrics are designed to span multiple quality dimensions, including factual accuracy, logical structure, completeness, style, and common pitfalls. Negative "pitfall" criteria help penalize frequent or high-risk errors. The total number of rubric items per rubric (typically 7–20) is chosen to balance coverage with signal sparsity.

Semantic weighting Each criterion is labeled with a categorical importance level (e.g., *Essential*, *Important*, *Optional*, *Pitfall*) that reflects its relative priority in the final reward. This supports

interpretable reward aggregation. Future work may explore numeric or learned weighting for greater flexibility.

Self-contained evaluation Rubrics are written to be independently actionable-each item can be evaluated in isolation by either human annotators or LLM-based judges, without requiring external context or domain knowledge.

#### 3.2 Synthesis

We apply the above framework on real world science and Medicine reasoning domains. For each domain, the prompt (included in Appendix [H\)](#page-14-0) instructs the LLM to generate 7–20 rubric items based on the complexity of the input question. Each item is assigned a categorical weight (e.g., *Essential Criteria*, *Important Criteria*) to determine its importance to a correct answer. The rubrics are designed to be fully self-contained which means that non-expert readers should be able to evaluate response quality using only the rubric. The evaluation outcome can be directly used as reward functions during reinforcement learning.

We generate synthetic rubrics using OpenAI's o3-mini and gpt-4o models [\(OpenAI o3-mini,](#page-8-10) [2025;](#page-8-10) [Jaech et al.,](#page-8-11) [2024;](#page-8-11) [Hurst et al.,](#page-8-12) [2024\)](#page-8-12) as described in Section [3.](#page-2-0) The *RaR-Medical-20k* and *RaR-Science-20k* are released for public use. For both domains, we use the reference answers provided in the datasets as a proxy for human expertise while generating synthetic rubrics. These rubric sets are then used to supervise the training of smaller models through either explicit or implicit reward aggregation (Section [2.1\)](#page-1-3).

### 4 Experiments

#### 4.1 Datasets

We investigate the utility of rubrics as rewards across two reasoning domains *medicine* and *science*.

- RaR-Medical-20k: We curate a set of 20k prompts from medicine-related subsets of medical-o1-reasoning-SFT [\(Chen et al.,](#page-8-13) [2024a\)](#page-8-13), the natural\_reasoning dataset [\(Yuan et al.,](#page-9-8) [2025\)](#page-9-8), the SCP-116K dataset [\(Lu](#page-8-14) [et al.,](#page-8-14) [2025\)](#page-8-14), and the GeneralThought-430K dataset [\(General Reasoning,](#page-8-15) [2025\)](#page-8-15).
- RaR-Science-20k: We curate roughly 20k science prompts aligned with the

<span id="page-3-2"></span>

| Domain →                       | Medicine       | Science         |
|--------------------------------|----------------|-----------------|
| Method/Baseline ↓              | HealthBench-1k | GPQA_Diamond    |
| Qwen2.5-7b                     | 0.0818         | 0.3030 ± 0.0286 |
| Qwen2.5-7b-Instruct            | 0.2359         | 0.3598 ± 0.0077 |
| Simple-Likert                  | 0.2489         | 0.3409 ± 0.0104 |
| Reference-Likert               | 0.3155         | 0.3775 ± 0.0350 |
| Predefined RaR                 | 0.2472         | 0.3485 ± 0.0365 |
| RaR-Explicit (o3-mini rubrics) | 0.2559         | 0.3333 ± 0.0504 |
| RaR-Explicit (GPT-4o rubrics)  | 0.2979         | 0.3030 ± 0.0197 |
| RaR-Implicit (o3-mini rubrics) | 0.3107         | 0.3864 ± 0.0407 |
| RaR-Implicit (GPT-4o rubrics)  | 0.3194         | 0.3662 ± 0.0191 |

Table 1: Performance of baseline and RaR (Rubrics as Rewards) methods on medicine and science domains. For HealthBench-1k, we report the overall score; for GPQA\_Diamond, we report mean accuracy over 4 runs with standard deviation. All experiments use gpt-4o-mini as the judge model. (RaR) methods are trained using rubrics generated by either o3-mini or GPT-4o. Across both domains, the RaR-Implicit method consistently outperforms Simple-Likert and matches or exceeds the performance of Reference-Likert.

GPQA Diamond benchmark from the natural\_reasoning dataset [\(Yuan et al.,](#page-9-8) [2025\)](#page-9-8), the SCP-116K dataset [\(Lu et al.,](#page-8-14) [2025\)](#page-8-14), and the GeneralThought-430K dataset [\(General Reasoning,](#page-8-15) [2025\)](#page-8-15).

Refer Tables [7](#page-10-0) and [10](#page-10-1) in Appendix for the full topic distributions.

## 4.2 Training Details

We train all models using the GRPO algorithm [\(Shao et al.,](#page-9-9) [2024\)](#page-9-9), with Qwen2.5-7B as the base policy. Training is conducted with a batch size of 96, a learning rate of 5 × 10−<sup>6</sup> , and a constant learning rate schedule with 10% linear warmup. Full hyperparameter details are provided in Appendix [C.](#page-11-0) All models are trained on a single compute node equipped with 8 NVIDIA H100 GPUs.

Our training pipeline consists of the following key components:

Response Generation: For each prompt q, we sample k = 16 responses from the current policy πθ, using a context length of 3584 and a sampling temperature of 1.0.

Reward Computation: We use gpt-4o-mini as the judge model to assign rewards R<sup>q</sup> to the sampled responses. The reward functions are described in Sections [4.3](#page-3-0) and [4.4.](#page-3-1)

Policy Update: The policy is updated using GRPO based on the computed rewards.

#### <span id="page-3-0"></span>4.3 Baselines

We consider compare various baselines and off-theshelf post-trained models. All rubric-free baselines are with Qwen2.5-7b as the base policy.

For off-the-shelf baselines we evaluate performance on Qwen2.5-7b. We also include the performance of Qwen2.5-7b-Instruct to compare with instruction tuned variant of the base policy.

We describe the post-trained baselines without rubrics below:

- **SIMPLE-LIKERT**: LLM-judge is asked to output a Likert Score from 1-10 (normalized to 0-1) for each response-prompt pair.
- **REFERENCE-LIKERT**: The judge model compares the generated response against a high-quality reference answer and outputs a Likert score from 1–10, which is then normalized to the [0, 1] range. The reward for each *(prompt, response, reference)* triplet is defined as:

$$
R_{\text{ref}}(q, x) = \text{Norm}(\text{LikertScore}(q, x, x^*))
$$

where x <sup>∗</sup> denotes the reference answer, either written by a human or generated by a stronger LLM.

#### <span id="page-3-1"></span>4.4 Rubric-guided Methods

**PREDEFINED-RaR**: This method uses a fixed set of generic rubrics (not prompt-specific, as described in Appendix [C\)](#page-11-1) and computes the reward via an explicit weighted sum, following

Equation [1.](#page-1-2) All rubric criteria are weighted uniformly.

- **RaR-EXPLICIT**: This variant also employs explicit aggregation [\(1\)](#page-1-2) but uses prompt-specific rubrics generated by the method in Section [3.](#page-2-0) Since the generated rubrics include categorical labels, we assign numerical weights using a simple default scheme: {"Essential": 1.0, "Important": 0.7, "Optional": 0.3, "Pitfall": 0.8}, with higher penalty applied to violated pitfalls. [2](#page-4-0)
- **RaR-IMPLICIT**: This variant also leverages promptspecific rubrics; however, instead of aggregating criterion scores explicitly, it prompts the judge model to evaluate the response holistically and assign a single Likert rating (1–10). This rating is then normalized to the [0, 1] range:

Rimplicit(q, x) = Norm(LikertScore(q, x, Rq))

where R<sup>q</sup> denotes the list of rubrics associated with prompt q.

#### 4.5 Evaluation Setup

Medical Reasoning We evaluate models trained on curated medical reasoning data using HealthBench-1k [\(Arora et al.,](#page-8-5) [2025\)](#page-8-5), a 1,000-example subset of HealthBench.[3](#page-4-1) . We generate model responses with greedy decoding (*temperature=0*) and report the overall\_score.

Science Reasoning We evaluate models trained with curated science set on the GPQA-Diamond subset [\(Rein et al.,](#page-9-10) [2024\)](#page-9-10). Each evaluation is independently repeated 4 times to account for variance, where each run samples one response per prompt using greedy decoding (*temperature=0*). Answer choices are permuted per example, and model outputs are parsed for boxed answer formats (e.g.,

boxed{A}). If extraction fails, we fall back to a GPT-4o verifier that checks whether the response contains the correct option letter or text (see [F\)](#page-13-0). Final accuracy is reported as the mean of the 4 independent runs.

## <span id="page-4-2"></span>5 Results

We now present the main findings of our study.

Rubrics as Rewards Combine Interpretability with Strong Performance. Table [1](#page-3-2) reports results on the Medicine (HealthBench-1k) and Science (GPQA\_Diamond) benchmarks. Our method, RaR-Implicit, consistently outperforms baselines such as Simple-Likert, with the best variant achieving upto 28% relative improvement on HealthBench-1k and 13% on GPQA. It also surpasses both base and instruction-tuned policy models, demonstrating the effectiveness of rubricguided training for nuanced response evaluation.

Furthermore, RaR-Implicit matches or exceeds the performance of the Reference-Likert baseline. In our setup, synthetic rubrics are generated using reference answers as proxies for expert supervision, meaning their quality is inherently constrained by the completeness and clarity of those references. Despite this, transforming open-ended answers into structured rubric criteria produces reward signals that are both effective and aligned with evaluation objectives.

We also find that implicit aggregation outperforms explicit, weighted summation which indicates that allowing an LLM-based judge to infer how to balance rubric criteria per prompt is more effective than relying on fixed, hand-crafted weights. While explicit aggregation offers more transparency and control, implicit methods better adapt to real-world complexity.

Rubric-based supervision further enables transparent reward specification by breaking response quality into interpretable criteria. Unlike holistic reference comparisons, rubrics reduce stylistic bias and surface-level overfitting. Though referencebased Likert scoring may perform slightly better in some cases, rubrics offer a more scalable and controllable approach—particularly where references are scarce or model interpretability is essential.

Rubrics enhance alignment with human preferences across model scales To evaluate the effectiveness of rubric-based evaluations in reflecting human preferences, we generated datasets consisting preferred (chosen) and perturbed (rejected) responses (see Appendix [I](#page-16-0) for details on dataset generation). We use LLM-Judges to assign ratings from 1 to 10 for these responses both with (RaR-IMPLICIT ) and without the guidance of rubrics (SIMPLE-LIKERT). Figure [2](#page-5-0) shows how often judges

<span id="page-4-0"></span><sup>2</sup> Pitfall criteria are phrased in positive form (e.g., "The response avoids misinformation"), so satisfying them contributes positively to the score. If a pitfall is not satisfied, the corresponding reward is reduced or penalized.

<span id="page-4-1"></span><sup>3</sup>We introduce HealthBench-1k as a cost-effective evaluation split for medical reasoning tasks and will release the test set identifiers to support reproducibility.

<span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)

Figure 2: LLM Judge Alignment Across Model Scales. Rubric-guided evaluation (orange) consistently improves judge accuracy across model sizes compared to pure Likert-based scoring (blue). Rubrics without reference answers (green) perform better than Likert baseline, though still lag behind full rubric guidance. Smaller scale models benefit the most from rubric structure, narrowing the performance gap with larger judges.

correctly rated the preferred responses higher than the rejected ones across various model scales (from 3b to 72b to o3-mini). The results consistently show that rubric-guided evaluations provide clearer and more accurate signals, as evidenced by the higher accuracy with which preferred responses receive higher ratings when rubrics are involved. This indicates that rubrics improves the alignment with human preferences by offering detailed, context-specific criteria that help judges better discern subtle differences in quality compared to evaluations using only simple Likert-based ratings. Further study on LLM-Judge scaling impacts on policy training are discussed in Appendix [C.](#page-11-1)

Expert guidance is crucial for synthetic rubric generation Human guidance significantly influences the effectiveness of rubrics in capturing subtle human preferences. Figure [2](#page-5-0) highlights performance differences between rubric-based evaluations that include reference answers and those without them. The data shows that rubrics developed with reference answers achieve higher accuracy, emphasizing that human insights integrated during rubric generation enable granular criteria and improved alignment with human preferences.

### 6 Ablations

Elements of Rubric Design This ablation study examines how the structure and weighting of synthetic rubrics—generated using o3-mini—affect downstream performance on HealthBench. As shown in Table [2,](#page-5-1) predefined generic rubrics

<span id="page-5-1"></span>

| Ablation Setting           | Overall Score    |
|----------------------------|------------------|
| Predefined Generic Rubrics | 0.2898 ± 0.00526 |
| Essential-Only Rubrics     | 0.3562 ± 0.00976 |
| No Weights                 | 0.3756 ± 0.00479 |
| No-Pitfall Criteria        | 0.3837 ± 0.00420 |
| All Rubrics                | 0.3723 ± 0.00005 |

Table 2: Ablation results on HealthBench-1k (trained on HealthBench-3.5k subset with Qwen2.5-7B base policy). Rubrics generated using o3-mini with access to reference answers. Variations test the impact of rubric structure and weighting on implicit reward aggregation.

substantially underperform compared to promptspecific ones, underscoring the importance of contextualization. Rubrics that include a broader range of criteria—both positive and negative—consistently outperform those limited to essential checks, suggesting that richer evaluation signals lead to better learning. Interestingly, we observe minimal performance differences when including rubric weights or pitfall criteria during training. One possible explanation is that synthetically generating effective pitfall criteria is inherently difficult, as it requires anticipating the most common or critical failure modes of the model, a task that often demands human intuition and domain expertise. As a result, these synthetic negative criteria may lack the specificity or relevance needed to meaningfully penalize undesirable responses.

Impact of Rubric Generation Strategies in Real-World Domains How does the method of rubric

<span id="page-6-0"></span>

| Training Method              | Overall Score |
|------------------------------|---------------|
| Expert-Answer-SFT            | 0.204         |
| Simple-Likert                | 0.239         |
| Reference-Likert             | 0.317         |
| RaR-Implicit-Synthetic-NoRef | 0.320         |
| RaR-Implicit-Synthetic       | 0.359         |
| RaR-Implicit-Human           | 0.348         |

Table 3: Evaluation on HealthBench: Comparison of human- vs. synthetic-generated rubrics (with and without reference answers). RaR methods trained with GRPO significantly outperform Likert-only, Referencebased-likert and SFT baselines. Synthetic rubrics generated *without* access to reference answers perform notably worse, highlighting the importance of humangrounded guidance. Notably, human-authored rubrics and synthetic rubrics with access to references yield comparable performance.

generation affect downstream training in challenging, real-world settings? To investigate this, we evaluate on the HealthBench dataset using a curated subset of 3.5k human-annotated examples, split into train and test sets. Table [3](#page-6-0) summarizes the results.

We observe that all rubric-based methods outperform rubric-free baselines, including Reference-Likert. Notably, even the weakest RaR variant significantly surpasses Reference-Likert, underscoring the advantage of structured supervision in subjective, open-ended domains like healthcare. We attribute this to the finer granularity and clarity rubrics provide in assigning rewards-especially when correctness is not binary and answers may vary in tone, completeness, or safety relevance.

Moreover, we find that rubric quality is crucial: synthetic rubrics generated with reference-answer guidance consistently outperform those generated without it. This highlights the importance of incorporating expert signal, whether via human-in-theloop annotations or high-quality reference completions, for generating effective and aligned rubrics. Purely synthetic rubrics, while scalable, currently fall short in capturing the subtle criteria required for robust training in high-stakes domains.

# Impact of LLM Expertise on Rubric Quality

To assess how the capabilities of rubric-generating LLMs affect downstream performance, we generate synthetic rubrics without access to reference answers and use them to train policies on Health-

<span id="page-6-1"></span>

| Rubric Generation Model          | Overall Score |
|----------------------------------|---------------|
| O3-mini (Rubrics with reference) | 0.359         |
| GPT-4o                           | 0.342         |
| GPT-4o-mini                      | 0.327         |
| O3-mini                          | 0.324         |
| Qwen-72B-Instruct                | 0.327         |
| Qwen-32B-Instruct                | 0.311         |
| Qwen-7B-Instruct                 | 0.319         |

Table 4: Policy performance on HealthBench-1k when trained with GRPO using rubrics generated by different LLMs *without* reference answers. GPT-4o-generated rubrics yield the strongest performance, though they still fall short of rubrics generated with expert (referenceguided) supervision. Smaller aligned models (e.g., GPT-4o-mini, O3-mini) remain competitive with larger openweight models, underscoring the importance of alignment and reasoning ability in rubric generation.

Bench. This isolates the effect of LLM quality on reference-free rubric utility. Specifically, we evaluate on the HealthBench-1k subset, using models trained on rubrics generated for the remaining 4k training examples from HealthBench.

As shown in Table [4,](#page-6-1) larger or more capable LLMs generally produce more effective rubrics, with GPT-4o yielding the best performance among reference-free models. However, all remain below the performance of rubrics generated with reference guidance (e.g., O3-mini with access to reference answers). Additionally, model attributes such as instruction tuning and reasoning capabilities play a key role in the effectiveness of rubric generation.

# 7 Related Work

RLVR across domains Reinforcement learning with verifiable rewards (RLVR) is quickly extending its reach well beyond math and code. GENERAL-REASONER trains on a 200 k mixed corpus-physics, finance, policy, and more-and reports a ten-point jump on MMLU-Pro after GRPO fine-tuning [\(Ma et al.,](#page-8-9) [2025\)](#page-8-9). A follow-up study extends RLVR to medicine, chemistry, psychology, and economics, showing that a single cross-domain reward model can supervise all four areas without task-specific tweaks [\(Su et al.,](#page-9-11) [2025a\)](#page-9-11). Focusing on healthcare, MED-RLVR applies the same recipe to multiple-choice clinical QA, gaining eight accuracy points over supervised baselines while eliciting chain-of-thought reasoning from a 3B-parameter

base model [\(Zhang et al.,](#page-9-12) [2025\)](#page-9-12). These studies collectively show promising progress in taking RL beyond math and code, yet sparse reward signals, unreliable verifiers, and limited benchmark coverage remain open challenges.

Rubric-based Evaluation Concise, task-specific rubrics are now standard for evaluating frontier LLMs on open-ended benchmarks [\(Arora et al.,](#page-8-5) [2025;](#page-8-5) [Ruan et al.,](#page-9-13) [2025;](#page-9-13) [Hashemi et al.,](#page-8-16) [2024;](#page-8-16) [Pathak et al.,](#page-9-14) [2025\)](#page-9-14). Pathak et al. show that rubricprompted GPT-4 graders are more accurate and consistent than a single question-agnostic checklist [\(Pathak et al.,](#page-9-14) [2025\)](#page-9-14), yet their rubrics appear only at evaluation time, not in training. HEALTH-BENCH extends the idea to medicine, pairing 48 k clinician-written criteria with GPT-4 judges to score factuality, safety, and empathy [\(Arora et al.,](#page-8-5) [2025\)](#page-8-5). Moving from evaluation to tuning, Configurable Preference Tuning (CPT) synthesizes rubricconditioned preference pairs for DPO fine-tuning [\(Gallego,](#page-8-17) [2025\)](#page-8-17). None of these approaches, however, integrates rubrics with verifiable rewards. Rubrics are used only to grade held-out outputs or to generate preference pairs.

Learning from feedback signals RLHF tunes LLMs with thousands of human preference comparisons, but these subjective labels introduce noise and invite reward hacking [\(Ouyang et al.,](#page-9-3) [2022\)](#page-9-3). Reinforcement learning with verifiable rewards (RLVR) alleviates these issues by relying on programmatic checks—ranging from exact string matches on GSM8K and MATH to mixed-domain verifiers in GENERAL-REASONER and CROSS-DOMAIN RLVR [\(Ma et al.,](#page-8-9) [2025;](#page-8-9) [Su et al.,](#page-9-11) [2025a\)](#page-9-11). Although the resulting signals are low-variance and fully automatic, they can be sparse. Process supervision provides more detailed feedback by rewarding intermediate reasoning steps. MCTSgenerated labels and generative reward models such as THINKPRM improve performance, but at a significant annotation cost [\(Li et al.,](#page-8-18) [2025;](#page-8-18) [Khalifa](#page-8-19) [et al.,](#page-8-19) [2025\)](#page-8-19). Our rubric based RLVR closes this gap by turning rubric criteria into structured verifiers and using their scalar scores as dense rewards in on-policy RLVR training.

# 8 Future Work

Interpretability and Reward Robustness RaR offers a transparent alternative to black-box reward models by explicitly optimizing against humaninterpretable rubric criteria. Unlike neural reward models or preference-based methods such as DPO, RaR enables auditability and objective attribution of reward signals. This transparency may also increase robustness to reward hacking, as discrete, interpretable criteria are harder for models to exploit than opaque learned scores.

Future work can explore formal evaluations of RaR's resilience to reward hacking, including adversarial training scenarios where models attempt to game individual rubric components.

Extending to Real-World Domains and Use Cases Rubric-guided post-training has been underexplored in open-ended or agentic tasks. A promising direction is expanding RaR to domains with subjective or multi-step goals such as tool use, reasoning under uncertainty, or real-world decision support-where rubrics can provide structured, taskspecific guidance.

Rubrics as Curriculum Rubrics naturally encode hierarchical task structure—from essential facts to optional stylistic preferences—which can support implicit curriculum learning. Early in training, models may only satisfy simpler criteria; as learning progresses, more complex targets become attainable. Future work can investigate dynamic weighting or staged introduction of rubric items to further leverage this curriculum effect.

# 9 Conclusion

We introduced Rubrics as Rewards (RaR), a framework for post-training language models using structured, checklist-style rubrics as reward signals. By decomposing response evaluation into transparent, multi-criteria objectives—both subjective and objective—RaR provides a modular and interpretable alternative to preference-based methods. Our experiments demonstrate that rubric-guided training achieves strong performance across domains, significantly outperforming Likert-based baselines and matching or exceeding the performance of reference-based reward generation. Additionally, we show that RaR improves alignment with human preferences while reducing dependence on large judge models.

# Limitations

Our study is limited to two domains (medical and science), and further validation is needed to assess generalizability to other tasks, such as open-ended dialogue. We explore only two reward aggregation strategies—implicit and explicit—and do not investigate alternative weighting schemes. Additionally, we do not conduct controlled analyses of reward hacking risks. Finally, our judges are off-the-shelf LLMs; future work could explore dedicated evaluators with stronger reasoning capabilities.

### Acknowledgments

We thank Qin Lyu, Nikhil Barhate, and Zijian Hu for supporting this research through the development of the in-house post-training platform RLXF. We also thank Robert Vacareanu for valuable early feedback and discussions.

## References

- <span id="page-8-7"></span>David Anugraha, Zilu Tang, Lester James V Miranda, Hanyang Zhao, Mohammad Rifqi Farhansyah, Garry Kuwanto, Derry Wijaya, and Genta Indra Winata. 2025. R3: Robust rubric-agnostic reward models. *arXiv preprint arXiv:2505.13388*.
- <span id="page-8-5"></span>Rahul K Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. 2025. Healthbench: Evaluating large language models towards improved human health. *arXiv preprint arXiv:2505.08775*.
- <span id="page-8-13"></span>Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. 2024a. Huatuogpt-o1, towards medical complex reasoning with llms. *arXiv preprint arXiv:2412.18925*.
- <span id="page-8-3"></span>Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. 2024b. Odin: Disentangled reward mitigates hacking in rlhf. *arXiv preprint arXiv:2402.07319*.
- <span id="page-8-6"></span>Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. 2025. Rmr1: Reward modeling as reasoning. *arXiv preprint arXiv:2505.02387*.
- <span id="page-8-2"></span>Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. 2025. Process reinforcement through implicit rewards. *arXiv preprint arXiv:2502.01456*.
- <span id="page-8-17"></span>Víctor Gallego. 2025. Configurable preference tuning with rubric-guided synthetic data. *arXiv preprint arXiv:2506.11702*.

<span id="page-8-15"></span>General Reasoning. 2025. [General reasoning.](https://gr.inc/)

- <span id="page-8-4"></span>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms. *arXiv preprint arXiv:2305.15717*.
- <span id="page-8-1"></span>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. *arXiv preprint arXiv:2501.12948*.
- <span id="page-8-8"></span>Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and Furu Wei. 2025b. Reward reasoning model. *arXiv preprint arXiv:2505.14674*.
- <span id="page-8-16"></span>Helia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, and Chris Kedzie. 2024. Llm-rubric: A multidimensional, calibrated approach to automated evaluation of natural language texts. *arXiv preprint arXiv:2501.00274*.
- <span id="page-8-12"></span>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. *arXiv preprint arXiv:2410.21276*.
- <span id="page-8-11"></span>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. Openai o1 system card. *arXiv preprint arXiv:2412.16720*.
- <span id="page-8-19"></span>Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. 2025. Process reward models that think. *arXiv preprint arXiv:2504.16828*.
- <span id="page-8-0"></span>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. 2024. T\" ulu 3: Pushing frontiers in open language model post-training. *arXiv preprint arXiv:2411.15124*.
- <span id="page-8-18"></span>Shuangtao Li, Shuaihao Dong, Kexin Luan, Xinhan Di, and Chaofan Ding. 2025. Enhancing reasoning through process supervision with monte carlo tree search. *arXiv preprint arXiv:2501.01478*.
- <span id="page-8-14"></span>Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, and Yuan Qi. 2025. Scp-116k: A high-quality problem-solution dataset and a generalized pipeline for automated extraction in the higher education science domain. *arXiv preprint arXiv:2501.15587*.
- <span id="page-8-9"></span>Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. 2025. General-reasoner: Advancing llm reasoning across all domains. *arXiv preprint arXiv:2505.14652*.

<span id="page-8-10"></span>OpenAI o3-mini. 2025. [Openai o3-min.](https://openai.com/index/openai-o3-mini/)

- <span id="page-9-3"></span>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. *Advances in neural information processing systems*, 35:27730–27744.
- <span id="page-9-14"></span>Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Yashwanth Nakka, Aaryan Raj Jindal, Pratyush Ghosh, Arnav Ramamoorthy, Shreyash Verma, Aditya Mittal, Aashna Ased, et al. 2025. Rubric is all you need: Enhancing llm-based code evaluation with questionspecific rubrics. *arXiv preprint arXiv:2503.23989*.
- <span id="page-9-10"></span>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. 2024. Gpqa: A graduate-level google-proof q&a benchmark. In *First Conference on Language Modeling*.
- <span id="page-9-13"></span>Jie Ruan, Inderjeet Nair, Shuyang Cao, Amy Liu, Sheza Munir, Micah Pollens-Dempsey, Tiffany Chiang, Lucy Kates, Nicholas David, Sihan Chen, et al. 2025. Expertlongbench: Benchmarking language models on expert-level long-form generation tasks with structured checklists. *arXiv preprint arXiv:2506.01241*.
- <span id="page-9-9"></span>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. *arXiv preprint arXiv:2402.03300*.
- <span id="page-9-0"></span>Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. 2023. A long way to go: Investigating length correlations in rlhf. *arXiv preprint arXiv:2310.03716*.
- <span id="page-9-4"></span>Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. 2025. Multichallenge: A realistic multiturn conversation evaluation benchmark challenging to frontier llms. *arXiv preprint arXiv:2501.17399*.
- <span id="page-9-11"></span>Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. 2025a. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. *arXiv preprint arXiv:2503.23829*.
- <span id="page-9-7"></span>Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. 2025b. Expanding rl with verifiable rewards across diverse domains. *arXiv preprint arXiv:2503.23829*.
- <span id="page-9-1"></span>Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. 2024. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. *arXiv preprint arXiv:2402.18571*.
- <span id="page-9-5"></span>Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep

Saha. 2025. J1: Incentivizing thinking in llm-asa-judge via reinforcement learning. *arXiv preprint arXiv:2505.10320*.

- <span id="page-9-2"></span>Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, and Matthias Gallé. 2024. Improving reward models with synthetic critiques. *arXiv preprint arXiv:2405.20850*.
- <span id="page-9-8"></span>Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Ilia Kulikov, Kyunghyun Cho, Dong Wang, Yuandong Tian, Jason E Weston, et al. 2025. Naturalreasoning: Reasoning in the wild with 2.8 m challenging questions. *arXiv preprint arXiv:2502.13124*.
- <span id="page-9-12"></span>Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, and Hoifung Poon. 2025. Medrlvr: Emerging medical reasoning from a 3b base model via reinforcement learning. *arXiv preprint arXiv:2502.19655*.
- <span id="page-9-6"></span>Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. 2025. Learning to reason without external rewards. *arXiv preprint arXiv:2505.19590*.

### A Training dataset breakdown

In this section, we show the topic distribution breakdown of the medicine and science training dataset and the rubric type distribution breakdown for the corresponding dataset.

## A.1 Medicine training dataset - Overall Data Distribution

| Metric                       | Value  |
|------------------------------|--------|
| Total examples               | 20 166 |
| Avg. rubrics per question    | 7.5    |
| Avg. question length (words) | 45.0   |

Table 5: Aggregate statistics for the full medical (train and validation) dataset.

| Rubric Type | Count  | Percent |
|-------------|--------|---------|
| Important   | 52 748 | 34.1    |
| Essential   | 47 584 | 30.7    |
| Optional    | 34 261 | 22.1    |
| Pitfall     | 20 215 | 13.1    |

Table 6: Rubric-type distribution across all 20,166 examples.

<span id="page-10-0"></span>

| Topics                     | Count  | Percent |
|----------------------------|--------|---------|
| Total examples             | 20 166 | 100.0   |
| Medical Diagnosis          | 10 147 | 50.3    |
| Medical Treatment          | 3 235  | 16.0    |
| Medical Knowledge          | 2 557  | 12.7    |
| Medical Diag. and Mngmnt   | 2 033  | 10.1    |
| Medical Biology            | 770    | 3.8     |
| Other                      | 428    | 2.1     |
| Medical Ethics             | 377    | 1.9     |
| Health Physics             | 276    | 1.4     |
| Epidemiology & Pub. Health | 216    | 1.1     |
| General Medicine           | 113    | 0.6     |
| Forensic Medicine          | 14     | 0.1     |

Table 7: Distribution of topics in the medical training and validation dataset

## A.2 STEM Dataset - Overall Data Distribution

| Metric                       | Value  |
|------------------------------|--------|
| Total examples               | 20 625 |
| Avg. rubrics per question    | 7.5    |
| Avg. question length (words) | 52.6   |

Table 8: Aggregate statistics for the full medical (train and validation) dataset.

| Rubric Type | Count  | Percent |
|-------------|--------|---------|
| Important   | 52 315 | 34.8    |
| Essential   | 42 739 | 28.4    |
| Optional    | 33 622 | 22.3    |
| Pitfall     | 21 808 | 14.5    |

Table 9: Rubric-type distribution across all 20 625 examples.

<span id="page-10-1"></span>

| Topics                   | Count | Percent |
|--------------------------|-------|---------|
| Total examples           | 20625 | 100.0   |
| General Chemistry        | 3163  | 15.3    |
| Quantum Mechanics        | 3158  | 15.3    |
| Physical Chemistry       | 2761  | 13.4    |
| Statistical Mechanics    | 2530  | 12.3    |
| Organic Chemistry        | 2059  | 10.0    |
| General Physics          | 1439  | 7.0     |
| Condensed Matter Physics | 1387  | 6.7     |
| Genetics                 | 1378  | 6.7     |
| Molecular Biology        | 815   | 4.0     |
| Astrophysics             | 409   | 2.0     |
| Inorganic Chemistry      | 407   | 2.0     |
| Analytical Chemistry     | 398   | 1.9     |
| Electromagnetism         | 239   | 1.2     |
| Optics                   | 143   | 0.7     |
| High Energy Physics      | 116   | 0.6     |
| Electromagnetic Theory   | 105   | 0.5     |
| Electromagnetics         | 72    | 0.3     |
| Relativistic Mechanics   | 46    | 0.2     |

Table 10: Distribution of topics in the STEM training and validation dataset

## B Synthetic Preference Set Generation

We leverage the publicly-released HEALTHBENCH [\(Arora et al.,](#page-8-5) [2025\)](#page-8-5) corpus, which contains 5,000 health-related prompts accompanied by expert-written answers. Of these, 4,203 datapoints already include an *ideal* completion vetted by licensed clinicians. For every such prompt–ideal pair we automatically generate a *perturbed* counterpart using o3 with the structured template shown below. The template forces the model to (i) spell out a [reasoning] plan for degrading quality, (ii) emit the degraded [perturbed\_completion], and (iii) log exact [chunks\_added] and [chunks\_removed]. Perturbations are accepted only after manual screening confirms that they are *objectively worse*, along at least one axis of medical accuracy, completeness, clarity, safety, specificity, structure, or tone, while remaining coherent and free of dangerous advice. This procedure produces a balanced evaluation set of 4,203 preferred and 4,203 perturbed responses (8,406 total), which we use in the rubric-versus-Likert experiments in Section [5.](#page-4-2)

## <span id="page-11-1"></span>C Judge Quality impacts on Post-training

We assess whether rubric-guided evaluation improves judge effectiveness compared to rubric-free Likert scoring when used for GRPO training. Table [11](#page-11-0) reports judge accuracy on synthetic medical data, with all policies trained using Qwen2.5-7B and varying judge models.

<span id="page-11-0"></span>

| Judge Model       | RaR-Implicit | Simple-Likert |
|-------------------|--------------|---------------|
| GPT-4o-mini       | 0.279        | 0.253         |
| Qwen-32B-Instruct | 0.262        | 0.254         |
| Qwen-14B-Instruct | 0.250        | 0.249         |
| Qwen-7B-Instruct  | 0.267        | 0.220         |

Table 11: Judge quality comparison: rubric-based evaluation vs pure Likert scoring on synthetic medical rubrics.

Rubric-based evaluation consistently yields stronger policies across all judge sizes. The most pronounced improvement appears with Qwen-7B-Instruct (+0.047), where rubric guidance lifts it from weakest to nearly matching larger models. Additionally, rubric-based scores are more tightly clustered (0.250–0.279) than those from Likertonly judges (0.220–0.254), indicating improved consistency.

These results suggest that rubrics help smaller judges approximate high-quality supervision by breaking evaluation into interpretable, binary criteria. This structured approach mitigates scalerelated limitations, enabling more reliable reward modeling even with limited-capacity evaluators.

# D Training Details

The training hyperparameters are described in Table [12.](#page-11-2)

<span id="page-11-2"></span>

| Hyperparameters         |                      |
|-------------------------|----------------------|
| num_rollouts_per_prompt | 16                   |
| batch_size (effective)  | 96                   |
| sampling_temperature    | 1.0                  |
| warmup_ratio            | 0.1                  |
| learning_rate           | 5.0e-06              |
| lr_scheduler_type       | constant_with_warmup |
| max_length              | 3584                 |
| num_train_steps         | 300                  |

Table 12: GRPO hyperparameter settings for Medical and Science domains.

# <span id="page-12-0"></span>E LLM-Judge Prompts

## Prompt for RUBRIC-LIKERT-IMPLICIT Method

#### System Prompt:

You are an expert evaluator. Given a user prompt, a generated response, and a list of quality rubrics, please rate the overall quality of the response on a scale of 1 to 10 based on how well it satisfies the rubrics.

Consider all rubrics holistically when determining your score. A response that violates multiple rubrics should receive a lower score, while a response that satisfies all rubrics should receive a higher score.

Start your response with a valid JSON object that starts with "```json" and ends with "```". The JSON object should contain a single key "rating" and the value should be an integer between 1 and 10.

Example response:

```json {

"rating": 7 }```

#### User Prompt Template:

Given the following prompt, response, and rubrics, please rate the overall quality of the response on a scale of 1 to 10 based on how well it satisfies the rubrics.

<prompt> prompt

</prompt> <response>

response </response>

<rubrics> rubric\_list\_string </rubrics>

Your JSON Evaluation:

#### Prompt for SIMPLE-LIKERT Baseline

#### System Prompt:

You are an expert evaluator. Given a user prompt and a generated response, please rate the overall quality of the response on a scale of 1 to 10, where 1 is very poor and 10 is excellent.

Start your response with a valid JSON object that starts with "```json" and ends with "```". The JSON object should contain a single key "rating" and the value should be an integer between 1 and 10. Example response:

```json {

"rating": 8 }```

## User Prompt Template:

Given the following prompt, and response, please rate the overall quality of the response on a scale of 1 to 10. <prompt>

prompt </prompt>

<response> response

</response>

Your JSON Evaluation:

### Prompt for REFERENCE-LIKERT Baseline

#### System Prompt:

You are an expert evaluator. Given a user prompt, a reference response, and a generated response, please rate the overall quality of the generated response on a scale of 1 to 10 based on how well it compares to the reference response. Consider factors such as accuracy, completeness, coherence, and helpfulness when comparing to the reference. The reference response represents a high-quality answer that you should use as a benchmark. Start your response with a valid JSON object that starts with "```json" and ends with "```". The JSON object should contain a single key "rating" and the value should be an integer between 1 and 10. Example response: ```json { "rating": 8 }``` User Prompt Template: Given the following prompt, reference response, and generated response, please rate the overall quality of the generated response on a scale of 1 to 10 based on how well it compares to the reference. <prompt> prompt </prompt> <reference\_response> reference </reference\_response>

<generated\_response> response </generated\_response>

Your JSON Evaluation:

# <span id="page-13-0"></span>F Evaluation Prompts

#### GPQA Evaluation Prompt

Determine whether the following model response matches the ground truth answer. ## Ground truth answer## Option correct\_answer or correct\_answer\_text ## Model Response ##: response\_text

A response is considered correct if it's final answer is the correct option letter (A, B, C, or D), or has the correct answer text. Please respond with only "Yes" or "No" (without quotes). Do not include a rationale.

# G Predefined Rubrics

### Predefined Rubrics for RaR-Predefined Method

- The response contains correct information without factual errors, inaccuracies, or hallucinations that could mislead the user.
- The response fully answers all essential parts of the question and provides sufficient detail where needed.
- The response is concise and to the point, avoiding unnecessary verbosity or repetition.
- The response effectively meets the user's practical needs, provides actionable information, and is genuinely helpful for their situation.

# <span id="page-14-0"></span>H Synthetic Rubric Generation Prompts

### Prompt for Synthetic Rubrics Generation: Medical Domain

You are an expert rubric writer. Your job is to generate a self-contained set of evaluation criteria ("rubrics") for judging how good a response is to a given question. Rubrics can cover aspects of a response such as, but not limited to, factual correctness, ideal-response characteristics, style, completeness, helpfulness, harmlessness, patient-centeredness, depth of reasoning, contextual relevance, and empathy. Each item must be self-contained – non expert readers should not need to infer anything or consult external information. Begin each description with its category: "Essential Criteria: . . . ", "Important Criteria: . . . ", "Optional Criteria: . . . ", or "Pitfall Criteria: Does not mention . . . ". Inputs:

- question: The full question text.
- reference\_answer: The ideal answer, including any specific facts, explanations, or advice.

Total items:

• Choose 7–20 rubric items based on the complexity of the question.

Each rubric item:

- title (2–4 words).
- description: One sentence starting with its category prefix that explicitly states exactly what to look for. For example:
  - Essential Criteria: Identifies non-contrast helical CT scan as the most sensitive modality for ureteric stones.
  - Pitfall Criteria: Does not mention identifying (B) as the correct answer.
  - Important Criteria: Explains that non-contrast helical CT detects stones of varying sizes and compositions.
  - Optional Criteria: States "The final answer is (B)" or similar answer choice formatting.
- weight: For Essential/Important/Optional, use 1–5 (5 = most important); for Pitfall, use –1 or –2.

Category guidance:

- Essential: Critical facts or safety checks; if missing, the response is invalid (weight 5).
- Important: Key reasoning, completeness, or clarity; strongly affects quality (weight 3–4).
- Optional: Helpful style or extra depth; nice to have but not deal-breaking (weight 1–2).
- Pitfall: Common mistakes or omissions specific to this prompt—identify things a respondent often forgets or misstates. Each Pitfall description must begin with "Pitfall Criteria: Does not mention . . . " or "Pitfall Criteria: Recommends . . . " and use weight –1 or –2.

To ensure self-contained guidance:

- When referring to answer choices, explicitly say "Identifies (A)", "Identifies (B)", etc., rather than vague phrasing.
- If the format requires a conclusion like "The final answer is (B)", include a rubric item such as:
  - Essential Criteria: Includes a clear statement "The final answer is (B)".
- If reasoning should precede the answer, include a rubric like:
  - Important Criteria: Presents the explanation before stating the final answer.
- If brevity is valued, include a rubric like:
  - Optional Criteria: Remains concise and avoids unnecessary detail.
- If the question context demands mention of specific findings, include that explicitly (e.g., "Essential Criteria: Mentions that CT does not require contrast").

Output: Provide a JSON array of rubric objects. Each object must contain exactly three keys—title, description, and weight. Do not copy large blocks of the question or reference\_answer into the text. Each description must begin with its category prefix, and no extra keys are allowed.

Now, given the question and reference\_answer, generate the rubric as described. The reference answer is an ideal response but not necessarily exhaustive; use it only as guidance.

#### Prompt for Synthetic Rubric Generation: Science Domain

You are an expert rubric writer for science questions in the domains of Biology, Physics, and Chemistry. Your job is to generate a self-contained set of evaluation criteria ("rubrics") for judging how good a response is to a given question in one of these domains. Rubrics can cover aspects such as factual correctness, depth of reasoning, clarity, completeness, style, helpfulness, and common pitfalls. Each rubric item must be fully self-contained so that non-expert readers need not consult any external information.

#### Inputs:

- question: The full question text.
- reference\_answer: The ideal answer, including any key facts or explanations.

#### Total items:

• Choose 7–20 rubric items based on question complexity.

Each rubric item must include exactly three keys:

- 1. title (2–4 words)
- 2. description: One sentence beginning with its category prefix, explicitly stating what to look for. For example:
  - Essential Criteria: States that in the described closed system, the total mechanical energy (kinetic plus potential) before the event equals the total mechanical energy after the event.
  - Important Criteria: Breaks down numerical energy values for each stage, demonstrating that initial kinetic energy plus initial potential energy equals final kinetic energy plus final potential energy.
  - Optional Criteria: Provides a concrete example, such as a pendulum converting between kinetic and potential energy, to illustrate how energy shifts within the system.
  - Pitfall Criteria: Does not mention that frictional or air-resistance losses are assumed negligible when applying conservation of mechanical energy.
- 3. weight: For Essential/Important/Optional, use 1–5 (5 = most important); for Pitfall, use –1 or –2.

#### Category guidance:

- Essential: Critical facts or safety checks; omission invalidates the response.
- Important: Key reasoning or completeness; strongly affects quality.
- Optional: Nice-to-have style or extra depth.
- Pitfall: Common mistakes or omissions; highlight things often missed.

#### Format notes:

- When referring to answer choices, explicitly say "Identifies (A)", "Identifies (B)", etc.
- If a clear conclusion is required (e.g. "The final answer is (B)"), include an Essential Criteria for it.
- If reasoning should precede the final answer, include an Important Criteria to that effect.
- If brevity is valued, include an Optional Criteria about conciseness.

Output: Provide a JSON array of rubric objects. Each object must contain exactly three keys—title, description, and weight. Do not copy large blocks of the question or reference\_answer into the text. Each description must begin with its category prefix, and no extra keys are allowed.

Now, given the question and reference\_answer, generate the rubric as described. The reference answer is an ideal response but not necessarily exhaustive; use it only as guidance.

# <span id="page-16-0"></span>I Perturbed Dataset Generation

## Prompt for Health Response Perturbation

You are tasked with creating perturbed versions of health-related responses to generate training data for evaluating model robustness. Given an original health prompt and an ideal completion, you must generate a perturbed version that is strictly worse than the ideal response but still plausible.

#### Input Format:

#### [prompt]: prompt

[ideal\_completion]: ideal\_completion

Task Instructions: For the perturbed version, follow this exact process:

[reasoning]: Explain your strategy for making the response worse. Consider these perturbation approaches:

- Medical Accuracy: Introduce subtle medical inaccuracies or outdated information - Completeness: Remove crucial information or context - Clarity: Make explanations less clear or more confusing - Safety: Reduce appropriate medical disclaimers or caveats - Specificity: Replace specific advice with vague generalizations - Structure: Disorganize the logical flow of information - Tone: Make the response less empathetic or more dismissive

[perturbed\_completion]: Create the modified response based on your reasoning strategy.

[chunks\_added]: List any new text segments you added (even if they're incorrect or unhelpful)

[chunks\_removed]: List any text segments you removed from the ideal response

#### Quality Guidelines:

- Ensure Degradation: Each perturbed response must be objectively worse than the ideal response - Maintain Plausibility: The perturbed responses should still seem like reasonable attempts (not obviously nonsensical) - Vary Approaches: Use different perturbation strategies for each version - Track Changes Precisely: Be specific about what text was added or removed - Health Context Sensitivity: Be mindful that health misinformation can be harmful - focus on subtle degradations rather than dangerous advice

#### Example Perturbation Strategies:

- Remove important safety warnings or medical disclaimers - Add outdated medical information that was once accepted but is now known to be incorrect - Remove specific dosage information or timing details - Add overly general statements that replace specific guidance - Remove context about when to seek professional medical help - Add confusing or contradictory information - Remove step-by-step instructions and replace with vague advice - Add unnecessarily complex medical jargon without explanation