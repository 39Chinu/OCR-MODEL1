# RLZ-r and LZ-End-r: Enhancing Move-r

Patrick Dinklage Johannes Fischer Lukas Nalbach Jan Zumbrink

July 24, 2025

In pattern matching on strings, a locate query asks for an enumeration of all the occurrences of a given pattern in a given text. The r-index [Gagie et al., 2018] is a recently presented compressed self index that stores the text and auxiliary information in compressed space. With some modifications, locate queries can be answered in optimal time [Nishimoto & Tabei, 2021], which has recently been proven relevant in practice in the form of Move-r [Bertram et al., 2024]. However, there remains the practical bottleneck of evaluating function Φ for every occurrence to report. This motivates enhancing the index by a compressed representation of the suffix array featuring efficient random access, trading off space for faster answering of locate queries [Puglisi & Zhukova, 2021].

In this work, we build upon this idea considering two suitable compression schemes: Relative Lempel-Ziv [Kuruppu et al., 2010], improving the work by Puglisi and Zhukova, and LZ-End [Kreft & Navarro, 2010], introducing a different trade-off where compression is better than for Relative Lempel-Ziv at the cost of slower access times. We enhance both the r-index and Move-r by the compressed suffix arrays and evaluate locate query performance in an experiment.

We show that locate queries can be sped up considerably in both the r-index and Move-r, especially if the queried pattern has many occurrences. The choice between two different compression schemes offers new trade-offs regarding index size versus query performance.

## Disclaimer

This is the full version of the paper of the same title to be published with the String Processing and Information Retrieval (SPIRE) conference in 2025 (CITATION PENDING). It contains additional details that had to be omitted from the conference paper due to space constraints.

## 1. Introduction

Searching for occurrences of a pattern in a text or a collection of texts is a ubiquitous problem. A common use case is to query different patterns against the same text, e.g., picture different users searching for different terms on (a snapshot of) the internet or DNA reads being matched in a genomic database. This scenario is typically tackled by building an index on the text, a data structure that allows for efficient pattern matching queries. Since the text can be prohibitively large to be indexed plainly, we are very much interested in compressed indexes. Arguably an important milestone in this area was the invention of the r-index by Gagie et al. [\[8\]](#page-13-0) that can be stored in space O (r), where r is the number of runs in the text's Burrows-Wheeler transform – a well-accepted measure of compressibility. Augmented by the move data structure by Nishimoto et al. [\[20\]](#page-14-0), pattern matching queries can be answered in optimal time. Bertram et al. [\[2\]](#page-13-1) recently implemented this and presented Move-r, achieving a very good practical time/space trade-off.

We differentiate between two types of queries: while count queries tell us how often a pattern occurs in the text, locate queries ask for an enumeration of all positions at which the pattern occurs. For this, in the r-index, we need to evaluate a function Φ for every occurrence, which turns out to be the main performance bottleneck in practice. In independent work (Move-r was not around yet), Puglisi and Zhukova [\[23,](#page-15-0) [24\]](#page-15-1) considered storing a compressed representation of the suffix array alongside the r-index that features efficient random access. For locate queries, we can now directly decode the relevant portion of the suffix array instead of evaluating Φ for every step. This resulted in a new trade-off where locate queries could be answered much faster, at the cost of having to store a compressed representation of the suffix array alongside the index.

Our contributions. We transfer the idea of [\[24\]](#page-15-1) and explore enhancing Move-r by a compressed representation of the suffix array with efficient random access, expecting this to be a practical trade-off for speeding up locate queries. For this, we consider two different compression schemes. First, like [\[24\]](#page-15-1), we consider Relative Lempel-Ziv, where we greatly improved the reference construction as well as the parsing procedure. While the source code of [\[24\]](#page-15-1) remains closed, we publish our reimplementation under an open source license. Second, we consider LZ-End [\[17\]](#page-14-1), a different Lempel-Ziv compression scheme that allows for efficient random access. Here, we give a competitive generalized and simplified algorithm to compute the LZ-End parsing of a string over an integer alphabet based on the (suboptimal) O (n lg lg n)-time algorithm of Kempa and Kosolobov [\[16\]](#page-14-2). We also improve Move-r itself by engineering internal rank and select data structure.

We implement different variations of the r-index and Move-r and show trade-offs between index size and query performance in our experiments.

## 2. Preliminaries

Let Σ ⊆ N be an integer alphabet and T ∈ Σ <sup>n</sup> a text over Σ of length n. In this work, we are interested in pattern matching queries asking for occurrences of a given pattern P ∈ Σ <sup>m</sup> of length m in T. Particularly, we are interested in the queries (a) count, asking for the number occ of occurrences of P in T, and (2) locate, asking for an enumeration of the starting positions of the occurrences. For some i ∈ [1, n], we denote by T[i] the i-th character in T. Given additionally j ∈ [i, n], we denote by T[i .. j] and T[i, j] the substring T[i] T[i + 1] · · · T[j − 1] T[j], juxtaposition meaning concatenation. The aforementioned queries are formally defined as locate(T, P) = {i ∈ [1, n − m] | T[i .. i + m − 1] = P} and count(T, P) = |locate(T, P)|. We argue about running times in the word RAM model, where we can do operations on words of length ω = Θ (lg n) bits in constant time. Unless explicitly stated otherwise, logarithms are given as base-two.

#### 2.1. Lempel-Ziv Parsings and Random Access

Lempel-Ziv (LZ) parsings factorize a text T into z ≤ n phrases f1, . . . , f<sup>z</sup> such that their concatenation f<sup>1</sup> · · · f<sup>z</sup> = T. They form a family of dictionary compressors, with arguably the most popular representative being Lempel-Ziv 77 (LZ77) [\[29\]](#page-15-2). There, we define the phrase f<sup>i</sup> (for i ∈ [1, z]) as either (1) a new symbol that does not occur in T[1 .. |f<sup>1</sup> · · · fi−1|], or (2) the longest prefix of T[|f<sup>1</sup> · · · fi−1| + 1 .. n] that has an occurrence in T starting at a position ≤ |f<sup>1</sup> · · · fi−1|. In the second case, we can encode the phrase as a reference to a previous occurrence, which can potentially be stored in less bits than storing the phrase explicitly. Even if we relax the definition of referencing phrases, it is typically z ≪ n if T is repetitive. Thus, LZ parsings are a popular choice for compressing T and are used in myriad everyday utilities such as gzip.

Random access. We are interested in efficient random access on T in its compressed form, i.e., we wish to extract a substring T[x .. x+ℓ] for some x ∈ [1, n] and ℓ ≥ 0 without decoding substantial portions of T. Regarding just the character T[x], it is contained in the phrase f<sup>i</sup> for i = min{i ∈ [1, z] | |f<sup>1</sup> · · · f<sup>i</sup> | ≥ x}. We say that phrase f<sup>i</sup> covers position x. We can store the set E = {|f<sup>1</sup> · · · f<sup>j</sup> | | j ∈ [1, z]} (the end positions of the phrases) in z⌈lg n⌉ bits (i.e., in space O (z)) and compute i in time O (lg z) using binary search, or use a static successor data structure to compute i in time O (lg lg(n/z)) [\[25\]](#page-15-3). [1](#page-2-0) An inherent disadvantage of the classic LZ77 scheme defined above, albeit achieving very good compression in practice, is that random access cannot be done efficiently. Phrases may refer to arbitrary prior positions in T, and thus to decode T[x], we may have to decode all phrases f1, . . . , f<sup>i</sup> up to (a prefix of) the phrase that covers x. We now look at two variants that resolve this issue at the cost of worse compression.

<span id="page-2-0"></span><sup>1</sup>Alternatively, we can build the characteristic bit vector B<sup>E</sup> of n bits where the j-th bit is set iff j ∈ E. Since exactly z bits are set in BE, we can build a data structure of size ⌈lg z m ⌉ + o (n) + O (lg lg z) bits that supports constant-time rank and select queries on B<sup>E</sup> [\[26\]](#page-15-4). With this, we can then also compute i in constant time. In this work, however, our aim is to focus on compressed space O (z).

LZ-End. Kreft and Navarro introduced the scheme LZ-End [\[17\]](#page-14-1). Here, each phrase fi is represented as a triple (j, ℓ, α), where j < i is the source phrase, ℓ ≥ 0 is the copy length and α ∈ Σ is a character such that f<sup>i</sup> = T[|f<sup>1</sup> · · · f<sup>j</sup> | − ℓ + 1 .. |f<sup>1</sup> · · · f<sup>j</sup> |] α for maximal possible ℓ and there is no k < i such that f<sup>i</sup> is a suffix of T[1 .. |f<sup>1</sup> · · · fk|]. We allow f<sup>0</sup> := ϵ as a valid source phrase such that the above is well-defined. Intuitively, f<sup>i</sup> extends the length-ℓ suffix of T[1 .. |f<sup>1</sup> · · · f<sup>j</sup> |] by a new character α and j is picked greedily such that ℓ is maximized.

Since each phrase adds exactly one character to a previously occurring substring, the end position of which is explicitly stated in the encoding triple, we can decode T[x .. x+ℓ] in time O (h + ℓ) once we know the phrase that covers position x + ℓ. Here, h is the length of the longest phrase. This gives us total random access time O (lg lg(n/z) + h + ℓ). When computing the parsing, we can artificially constrain h to obtain parameterized random access time.

Relative Lempel-Ziv. Kuruppu et al. proposed a variant of Lempel-Ziv parsings where we do not refer to earlier parts of T itself, but instead to a given reference R ∈ Σ ∗ [\[18\]](#page-14-3). This is useful especially in scenarios where we want to store a collection of texts that are highly similar (e.g., genomic sequences from the same species). Formally, the phrase f<sup>i</sup> is the longest prefix of T[|f<sup>1</sup> · · · fi−1| + 1 .. n] that occurs in R, or a single character that does not occur in R. This scheme is referred to as Relative Lempel-Ziv (RLZ).

To decode T[x .. ℓ], we can directly access the substring in R that the phrase covering x refers to, which can be done in total time O (lg lg(n/z) + ℓ). The compression depends on how well R represents T, and R must be stored alongside the compressed form of T in order to be able to decode T.

#### 2.2. Suffix Arrays, Burrows-Wheeler Transform and Compression

In the suffix array A of T, we store the starting positions of the suffixes of T in their lexicographical order [\[19\]](#page-14-4). This ordering causes suffixes that begin with equal prefixes to be grouped in consecutive intervals. A text book algorithm to answer count queries in time O (m lg n) finds the interval [b, e] ⊆ [1, n] of A that contains all (and only the) suffixes of T beginning with P, the query time stemming from binary searches for b and e, respectively. To answer locate, we simply need to enumerate A[b .. e]. We can store A in n⌈lg n⌉ bits of space and construct it in time O (n) [\[21\]](#page-14-5).

The Burrows-Wheeler transform (BWT) of T is a reversible transform of T defined as L[i] := T[A[i] − 1] (or L[i] := T[n] if A[i] = 1) [\[4\]](#page-13-2). The BWT of repetitive texts tends to contain long equal-letter runs, which can be exploited by run-length compression. We denote by r the number of these runs.

Compressed Differential Suffix Arrays. In practice, storing A plainly is prohibitive for large T. Even though it is a permutation over [1, n] and thus not inherently compressible, different ways to compress A have been shown (we refer to [\[12\]](#page-14-6) for an overview). In this work, we focus on compressing the differential suffix array A<sup>d</sup> ∈ Z n , where A<sup>d</sup> [1] := A[1] and A<sup>d</sup> [i] := A[i] − A[i − 1] for i ∈ [2, n].

Gonz´alez et al. first exploited the interesting property that the number of distinct values in A<sup>d</sup> is bounded by the number r of BWT runs and thus, essentially, repetitiveness in T implies repetitiveness in A<sup>d</sup> [\[11\]](#page-13-3). In this work, we are interested in the approach by Puglisi and Zhukova [\[24\]](#page-15-1), who instead considered RLZ to compress A<sup>d</sup> . They describe a strategy to extract R from A<sup>d</sup> by selecting segments based on the frequencies of representative substrings, and show that this outperforms using a random sample of A<sup>d</sup> (for which bounds on the expected compression have been shown [\[9\]](#page-13-4)). We denote by zˆ<sup>R</sup> the number of RLZ phrases of A<sup>d</sup> computed this way.

For random access on A, we want to avoid having to compute A[x] = P<sup>x</sup> <sup>i</sup>=1 A<sup>d</sup> [i] for some x ∈ [1, n] in time O (n). Rather, we create a sample A′ that contains a subsequence of A. Let y be the greatest sampled position ≤ x, then we can compute A[x] = A′ [y] + P<sup>x</sup> <sup>i</sup>=y+1 A<sup>d</sup> [i] in time O (δ), where δ is the maximum distance between any position and the previous sample.

For example, in [\[24\]](#page-15-1), we take a sample of A for every RLZ phrase. This gives us δ < h and using a static successor data structure of size O (ˆzR) (since |A′ | = zˆR), random access is possible in time O (lg lg(n/zˆR) + h), where h is the length of the longest RLZ phrase.

### 2.3. (Move-)r-Index

The r-index is a recent advancement in compressed data structures for pattern matching [\[8\]](#page-13-0) that is also highly relevant in practice. It is a self-index that encodes the BWT of T and auxiliary data structures in O (r) space. Using the move data structure of [\[20\]](#page-14-0), we obtain optimal O (m lg log<sup>ω</sup> σ) time for count and optimal additional time O (occ) for locate queries if |Σ| = O (polylog(n)).

## 3. LZ-End Compression of Suffix Arrays

Following the idea of [\[24\]](#page-15-1) to apply LZ compression on the differential suffix array, we explore its compression using LZ-End. To give an intuition as to why this may be fruitful, LZ-End (1) allows for efficient random access on the compressed input and (2) achieves competitive compression in practice. We first show the following for compressing any integer sequence A.

<span id="page-4-0"></span>Theorem 1. Let A ∈ [1, n] n be an integer sequence. In time and space O (n), we can construct a data structure of size O (ˆzend) such that for x ∈ [1, n] and ℓ ≥ 0, we can reconstruct A[x .. x + ℓ] in time O (lg lg(n/zˆend) + h + ℓ), where zˆend is the number of LZ-End phrases of the differential representation A<sup>d</sup> of A and h the length of the longest phrase.

Proof. The differential representation A<sup>d</sup> of A can be computed in time and space O (n) and by [\[16\]](#page-14-2), the same holds for the LZ-End parsing of A<sup>d</sup> . We represent the triples defining the parsing as three arrays:

1. the array src, where the i-th entry contains the number ∈ [1, i − 1] of the source phrase that f<sup>i</sup> refers to,

- 2. the array end, where the i-th entry contains the position |f<sup>1</sup> · · · f<sup>i</sup> | ∈ [1, n] at which phrase f<sup>i</sup> ends in A<sup>d</sup> , and
- 3. the array ext, where the i-th entry contains the value ∈ [−n, n] from A<sup>d</sup> that extends the suffix of A<sup>d</sup> [1 .. |f<sup>1</sup> · · · fsrc[i] |].

Each array can be stored in space O (ˆzend). We also build a static successor data structure over end that allows for successor queries in time O (lg lg(n/zˆend)), which we can do in time and space O (ˆzend). Finally, in time at most O (ˆzend), we sample the zˆend values from A at the positions stored in end in a new array A′ and store them also in space O (ˆzend).

Given x ∈ [1, n] and ℓ ≥ 0, we decode A[x .. x + ℓ] as follows: we first extract the range A<sup>d</sup> [x .. x + ℓ] from the LZ-End parsing in time O (lg lg(n/zˆend) + h + ℓ) using the extraction algorithm from [\[17\]](#page-14-1) (the length of a phrase f<sup>i</sup> can trivially be computed in constant time as |f<sup>i</sup> | = end[i] − end[i − 1]). Using the successor data structure, we can find in time O (lg lg(n/zˆend)) the position of a relevant sample of A that is stored in A′ . Then, in time at most O (h + ℓ), we accumulate the relevant differential values from Ad [x .. x + ℓ] to reconstruct A[x .. x + ℓ].

Corollary 1. Let T ∈ Σ n be a string of length n. In time and space O (n), we can construct a data structure of size O (ˆzend) such that for x ∈ [1, n] and ℓ ≥ 0, we can compute the suffix array interval A[x .. x + ℓ] in time O (lg lg(n/zˆend) + h + ℓ), where zˆend is the number of LZ-End phrases of the differential representation A<sup>d</sup> of the suffix array A of T and h is the length of the longest phrase.

### <span id="page-5-0"></span>3.1. Practical LZ-End Parsing

To implement the computation of LZ-end parsings, we adopt and modify the algorithm by Kempa and Kosolobov [\[16\]](#page-14-2) that does so in time O (n lg lg n) in a left-to-right scan of the text T of length n (a linear-time algorithm exists [\[16\]](#page-14-2), but we conjecture it to be hardly practical). An its core lies a dynamic predecessor/successor data structure M that marks the lexicographic ranks of suffixes of the reverse input ←− T at which already computed phrases end. In the following, we briefly describe our modifications and refer to Appendix [A](#page-16-0) for details.

We make M associative, so that at each marked suffix, we also store the number of the phrase that ends at the suffix. This removes a level of indirection and even allows us to completely discard the suffix array after initialization. Second, instead of temporarily unmarking and marking back phrases in M (which is done to rule out finding a copy source phrase that may be eliminated by a merge), we reduce the overall workload on M by performing additional predecessor/successor query only if absolutely necessary, and only ever unmark a position in M upon extension or merging of phrases. Third, since the parsing is computed processing <sup>T</sup> left to right but suffixes of ←− T are considered, we save arithmetic computations by using a variant A←<sup>1</sup> of the inverse suffix array of T that is defined as A←<sup>1</sup> [A[n − i − 1]] := i. Then, it is A←<sup>1</sup> [i] = A−<sup>1</sup> [n − i] and we read A←<sup>1</sup> left to right as we process T. Finally, our implementation of the algorithm is written in a

way that Σ may be an arbitrary integer alphabet such that, e.g., we can compute the parsing for a differential suffix array.

We set the maximum phrase length to h := 213, giving us the best access performance in preliminary experiments. Furthermore, we store the array end of end positions plainly using z⌈lg n⌉ bits and use a simple O (lg z)-time binary search with no auxiliary data structure to find the phrase covering a position in question. Preliminary experiments have shown that despite its simplicity, this approach is the fastest given the relatively low number z.

In Appendix [A,](#page-16-0) we present results of experiments showing that our implementation of LZ-End is competitive with the lz-end-toolkit and faster on general (non-highly repetitive) inputs.

## <span id="page-6-0"></span>4. Improved RLZ Compression of Suffix Arrays

Puglisi and Zhukova [\[24\]](#page-15-1) considered compressing the differential suffix array using Relative Lempel-Ziv (henceforth referred to as RLZSA). However, their source code remains closed. With the aim of reproducing their results for further research, we reimplemented RLZSA as described there and in Zhukova's doctoral thesis [\[28\]](#page-15-5) to the best of our capabilities. In this process, we found several clues as to how to improve upon their work. We summarize our improvements here and refer to Appendix [B](#page-18-0) for an in-depth description of the individual steps.

First, we shrink the overall representation by separating data pertaining to literal or copying RLZ phrases. This allows us to drop the requirement that each copying phrase needs to be preceded by a literal phrase (also improving RLZ compression). Using the new representation, we can reduce the time for randomly accessing a suffix array interval A[b .. e] from O (|e − b| + h + lg(z/a) + a) to O (|e − b| + lg(na/z) + a), (see Appendix [B.2](#page-21-0) and [B.3\)](#page-21-1), where a ≥ 1 is an integer sampling parameter and h is the length of the longest RLZ phrase.

We then proceed to improve upon the construction of RLZSA. By using Big-BWT [\[3\]](#page-13-5), we can make the construction of A<sup>d</sup> semi-external, reducing the memory usage from O (n) to O (|PFP|), where PFP is a prefix free parsing of T. By allowing the selection of arbitrary segments from A<sup>d</sup> (instead of partitioning A<sup>d</sup> and only allowing aligned segments) and by setting the considered k-mer length to k := 1, we can reduce the time and space required for reference construction from O (n) to O r <sup>1</sup>−ϵn ϵ and O (r), respectively, where ϵ ∈ [0, 1] is a parameter (see Appendix [B.4\)](#page-22-0). The new segment selection strategy also improves the reference quality, leading to better compression (as shown later in Table [1\)](#page-9-0).

Finally, we speed up and reduce the memory usage for computing the RLZ parsing of Ad for the computed reference R by replacing the FM-index by Move-r over ←− R using an optimized rank/select data structure for large alphabets from Appendix [C.2.](#page-26-0)

We set the size of the RLZ reference |R| := min(5.2r, n/3), which gave us the best results overall in preliminary experiments. RLZ phrases are limited to maximum length h := 216, which allows storing their length in 16-bit integers.

## 5. Applications to the (Move)-r-index

The main bottleneck when answering locate queries using the r-index in practice are the applications of the function Φ required to enumerate occurrences.

There have been at least two different approaches to resolve this, both of which have been shown to be relevant in practice: Puglisi and Zhukova store the RLZ-compressed differential suffix array next to the r-index, which allows for up to two orders of magnitude faster locate queries (with many occurrences) at the cost of using 2–13 times as much memory [\[24\]](#page-15-1). Bertram et al., on the other hand, implement the move data structure by [\[20\]](#page-14-0), speeding up queries (both count and locate) by an order of magnitude while only doubling the required space [\[2\]](#page-13-1).

We propose variations and combinations of the above and evaluate them in our experiments (Section [6\)](#page-7-0). Namely, we explore storing an LZ-End-compressed differential suffix array next to the r-index to find out whether we can obtain a trade-off similar to [\[24\]](#page-15-1). Furthermore, albeit much faster than in the original r-index, the Φ steps for enumerating occurrences remain a bottleneck for the locate queries of [\[2\]](#page-13-1), each causing cache misses. We thus also consider storing either compressed differential suffix array next to Move-r.

The r-index (as well as Move-r) already maintains a sampling A′ <sup>r</sup> of the suffix array at the boundary of every BWT run, i.e., |A′ r | = r. This creates redundancy regarding the sampling A′ of suffix array values at LZ phrase end positions proposed by [\[24\]](#page-15-1) and in the proof of Theorem [1.](#page-4-0) For reconstructing a suffix array value using Ad, we can as well use A′ r . This worsens the worst-case access time to O (n), because there is no general bound on the length of a BWT run. In practice, however, the average length of a BWT run is reasonably short even for repetitive inputs (see, e.g., column ⌊n/r⌋ in Table [1\)](#page-9-0).

Alternatively, one could consider replacing the sampling A′ <sup>r</sup> by A′ in the r-index or Move-r. However, then, to retrieve the suffix array value at the end of a run, we must spend up to O (lg lg(n/zˆ) + h) extra time time for random access, which would worsen the performance of locate queries, conflicting with our motivations. It would also increase the index size in practice as empirically, it holds that z > r ˆ . Therefore, we do not further consider this sampling method.

## <span id="page-7-0"></span>6. Experiments

In our experiments, we evaluate the construction and locate query performance of the following variations of the r-index and Move-r:

- r-index the original r-index of [\[8\]](#page-13-0),
- r-rlz the r-index plus the RLZ-compressed differential suffix array,
- r-lzend the r-index plus the LZ-End-compressed differential suffix array,
- move-r the Move-r index of [\[2\]](#page-13-1) (with improved internal rank/select),
- move-r-rlz Move-r plus the RLZ-compressed differential suffix array, and

• move-r-lzend – Move-r plus the LZ-End-compressed differential suffix array.

Note that only move-r-rlz contains the improved RLZSA construction that we described in Section [4,](#page-6-0) whereas r-rlz is based on a reimplementation of [\[24\]](#page-15-1) described in Appendix [B.](#page-18-0) We do this to better argue about our improvements. However, we use Big-BWT [\[3\]](#page-13-5) for all variants to compute suffix arrays. As mentioned in the list above, we also applied improvements to Move-r itself by engineering a new rank/select data structure tailored specifically for its internal queries. We refer to Appendix [C](#page-25-0) for details.

We implemented all index variants in C++20 and make the source code publicly available[2](#page-8-0) . We compiled using the GCC 13.3.0 compiler with flags set for highest optimization (-march=native -DNDEBUG -Ofast).

Table [1](#page-9-0) lists the input texts that we considered in our experiments alongside relevant statistics. The texts einstein and english are part of the Pizza&Chili Corpus[3](#page-8-1) , whereas dewiki is a highly repetitive text manually constructed from German Wikipedia entries. From the National Center for Biotechnology Information[4](#page-8-2) (NCBI) database, we constructed chr19, consisting of concatenated human chromosome 19 haplotypes, and sars2, a collection of Sars-Cov-2 genomes. From all text files, we erased all zero bytes.

For each text, we generated two sets of query patterns (hence two lines per file in Table [1\)](#page-9-0) using our tool move-r-patterns (also included in our source code repository). The sets differ in the pattern length m, as well as the average number occ of occurrences in the respective text. We chose the patterns in the first set such that occ ≈ m. This implies that when locating those patterns, we measure a blend of backward-search and suffix array extraction. The performance of counting queries was measured against this set. The patterns in the second set were chosen such that occ ≈ 105m. When locating these, we measure mostly suffix array extraction, which is a particularly relevant measure for our experiments.

All experiments were done on a Ubuntu 24.04 system with two AMD EPYC 7452 CPUs (32/64x 2.35-3.35GHz, 2/16/128MB L1/2/3 cache) and 1TB of RAM (3200 MT/s DDR4).

#### 6.1. Construction Performance

We first look at the construction of the competing indexes. Figure [1](#page-10-0) shows the construction throughput as well as the peak memory usage during construction.

To no surprise, compressing the differential suffix array dominates the time and space needed for construction (comparing r-index and move-r to the variants storing a compressed suffix array). Regarding the two different compression schemes, we see that LZ-End (move-r-lzend and r-lzend) is relatively slow to compute overall, but competitive with r-rlz regarding both time and space.

Our improved RLZSA construction from Section [4](#page-6-0) (move-r-rlz), however, clearly outperforms the other variants that compress the suffix array: it is faster by a factor of up

<span id="page-8-0"></span><sup>2</sup>Our source code: <https://github.com/LukasNalbach/Move-r>.

<span id="page-8-1"></span><sup>3</sup>Pizza&Chili Corpus: <https://pizzachili.dcc.uchile.cl/>

<span id="page-8-2"></span><sup>4</sup>NCBI: <https://www.ncbi.nlm.nih.gov/>

<span id="page-9-0"></span>Table 1: The input files for our experiments. For each input, we give the size n, the size |Σ| of the alphabet and the compression ratios n/r, n/zˆ<sup>R</sup> and n/zˆend (higher values mean more repetitive). Here, zˆ<sup>R</sup> is the number of RLZ phrases of A<sup>d</sup> following the construction of [\[24\]](#page-15-1), whereas zˆR′ refers to our improved construction from Section [4.](#page-6-0) As in Section [3.1,](#page-5-0) zˆend denotes the number of LZ-End phrases of A<sup>d</sup> . By N, we denote the number of queried patterns, by m the pattern length and by occ the average number of occurrences of the patterns. Per input, the first line indicates N, m and occ for m ≈ occ, the second line for m ≪ occ.

| text     | n [GB] | Σ   | ⌊n/r⌋ | ⌊n/zˆR⌋ | ⌊n/zˆR′⌋ | ⌊n/zˆend⌋ | N       | m      | occ     |
|----------|--------|-----|-------|---------|----------|-----------|---------|--------|---------|
| einstein | 0.47   | 140 | 1,611 | 118     | 183      | 1,081     | 100,000 | 800    | 736     |
|          |        |     |       |         |          |           | 10,000  | 7      | 72,644  |
| sars2    | 10.00  | 80  | 548   | 60      | 61       | 336       | 3,000   | 2,700  | 2,745   |
|          |        |     |       |         |          |           | 100     | 24     | 178,948 |
| dewiki   | 10.00  | 207 | 377   | 122     | 146      | 306       | 100,000 | 300    | 323     |
|          |        |     |       |         |          |           | 1,000   | 9      | 76,372  |
| chr19    | 10.00  | 53  | 46    | 12      | 25       | 34        | 1,000   | 25,000 | 19,531  |
|          |        |     |       |         |          |           | 1,000   | 100    | 107,991 |
| english  | 2.21   | 240 | 3     | 2       | 4        | 3         | 500,000 | 35     | 37      |
|          |        |     |       |         |          |           | 300     | 7      | 91,964  |

to ten (einstein) and the required space is sometimes even lower than for just computing the r-index. It also clearly outperforms the construction of our reimplementation of RLZSA (r-rlz).

#### 6.2. Locate Query Performance

We now look at locate queries for the two pattern sets described above (one query per pattern). Figure [2](#page-11-0) shows the query throughput as well as the size of the considered indexes. For reference, we also give the throughput of count queries, which does not involve any compressed suffix arrays (because we only report the size of the corresponding suffix array interval, not its contents).

We can assert that the performance of move-r is somewhat improved over [\[2\]](#page-13-1) (the experiments there were done on the same machine). The trade-off compared to r-index remains the same: we require roughly twice the amount of space, but queries are considerably faster overall.

As expected, enhancing the r-index by compressed suffix arrays (r-rlz and r-lzend) considerably improves the performance of locate queries for patterns with many occurrences. This confirms the results of [\[24\]](#page-15-1). We see how r-rlz achieves overall higher throughputs than r-lzend (by a factor of 4 for m ≪ occ). This is expected, as random access on RLZ-compressed data incurs only one cache miss per phrase, as opposed to up to h cache misses for LZ-End. However, we see that LZ-End achieves better compression,

<span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)

Figure 1: Construction time versus peak memory usage (in bytes per input character) of our implemented index data structures for the given inputs. Memory usage is given on a logarithmic scale in order to highlight the marginal differences between r-index, move-r and move-r-rlz. Data points for r-lzend and move-r-lzend do, in fact, overlap nearly precisely.

which is also confirmed in Table [1](#page-9-0) when comparing columns ⌊n/zˆR⌋ and ⌊n/zˆend⌋, making it a trade-off.

When enhancing Move-r with compressed suffix arrays (move-r-rlz and move-r-lzend), the picture differs a bit. Here, using LZ-End (move-r-lzend) can sometimes even slow down locate queries (e.g., on einstein and chr19). Using RLZ (rlzsa), on the other hand, improves query performance by a great deal particularly for frequent patterns m ≪ occ (e.g., by a factor of over 16 for sars). Again, however, LZ-End yields much better compression than RLZ in most cases (now comparing ⌊n/zˆR'⌋ and ⌊n/zˆend⌋ in Table [1\)](#page-9-0). Interestingly however, on english, the improved RLZSA construction (move-r-rlz) achieves better compression than LZ-End (move-r-lzend), which is a topic for further research.

Overall, our improved RLZSA (move-r-rlz) achieves better compression than that of [\[24\]](#page-15-1) (r-rlz). This is particularly evident for einstein and chr19, where move-r-rlz is smaller than r-rlz despite storing more information (e.g., compare move-r against r-index).

<span id="page-11-0"></span>![](_page_11_Figure_0.jpeg)

Figure 2: Size (in bits per input character) versus locate query throughput (queries per millisecond) of our implemented index data structures given medium (m ≈ occ) or short (m ≪ occ)patterns for the given inputs. For reference, we also give the count query throughput of the base index data structures, r-index and move-r for medium patterns. 

## 7. Conclusions and Future Work

We enhanced the recent r-index as well as Move-r by compressed suffix arrays with efficient random access to speed up locate queries. For this, we explored two different compression schemes: Relative Lempel-Ziv and LZ-End. The experiments show that the idea works, confirming and expanding upon the results of [\[24\]](#page-15-1). We can achieve different trade-offs regarding construction performance, index size and query performance by choosing different combinations of index and compressed suffix arrays. For both compression schemes, we gave new strategies and algorithms that improve upon their predecessors.

In future research, enhancing the subsampled r-index by Cobas et al. [\[5\]](#page-13-6) may be considered. We also saw that reference construction for Relative Lempel-Ziv is still an interesting topic of research beyond [\[9,](#page-13-4) [24\]](#page-15-1). By improving upon the segment selection strategy of [\[24\]](#page-15-1), we were able to improve the quality of the reference and thus compression.

## References

- <span id="page-13-8"></span>[1] Michael A. Bender and Martin Farach-Colton. The LCA problem revisited. In 4th Latin American Theoretical Informatics Symposium (LATIN), volume 1776 of Lecture Notes in Computer Science, pages 88–94. Springer, 2000. [doi:10.1007/](https://doi.org/10.1007/10719839_9) [10719839\\\_9](https://doi.org/10.1007/10719839_9).
- <span id="page-13-1"></span>[2] Nico Bertram, Johannes Fischer, and Lukas Nalbach. Move-r: Optimizing the rindex. In 22nd International Symposium on Experimental Algorithms (SEA), volume 301 of LIPIcs, pages 1:1–1:19. Dagstuhl, 2024. [doi:10.4230/LIPICS.SEA.2024.1](https://doi.org/10.4230/LIPICS.SEA.2024.1).
- <span id="page-13-5"></span>[3] Christina Boucher, Travis Gagie, Alan Kuhnle, Ben Langmead, Giovanni Manzini, and Taher Mun. Prefix-free parsing for building big bwts. Algorithms Mol. Biol., 14(1):13:1–13:15, 2019. [doi:10.1186/S13015-019-0148-5](https://doi.org/10.1186/S13015-019-0148-5).
- <span id="page-13-2"></span>[4] Michael Burrows and David Wheeler. A block-sorting lossless data compression algorithm. Technical Report 124, Digital Equipment Corporation, 1994.
- <span id="page-13-6"></span>[5] Dustin Cobas, Travis Gagie, and Gonzalo Navarro. A fast and small subsampled r-index. In 32nd Annual Symposium on Combinatorial Pattern Matching (CPM), volume 191 of LIPIcs, pages 13:1–13:16. Dagstuhl, 2021. [doi:10.4230/LIPICS.CPM.](https://doi.org/10.4230/LIPICS.CPM.2021.13) [2021.13](https://doi.org/10.4230/LIPICS.CPM.2021.13).
- <span id="page-13-9"></span>[6] Patrick Dinklage, Johannes Fischer, and Alexander Herlez. Engineering predecessor data structures for dynamic integer sets. In 19th International Symposium on Experimental Algorithms (SEA), volume 190 of LIPIcs, pages 7:1–7:19. Dagstuhl, 2021. [doi:10.4230/LIPIcs.SEA.2021.7](https://doi.org/10.4230/LIPIcs.SEA.2021.7).
- <span id="page-13-7"></span>[7] Johannes Fischer and Volker Heun. Space-efficient preprocessing schemes for range minimum queries on static arrays. SIAM J. Comput., 40(2):465–492, 2011. [doi:](https://doi.org/10.1137/090779759) [10.1137/090779759](https://doi.org/10.1137/090779759).
- <span id="page-13-0"></span>[8] Travis Gagie, Gonzalo Navarro, and Nicola Prezza. Optimal-time text indexing in bwt-runs bounded space. In 29th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1459–1477. SIAM, 2018. [doi:10.1137/1.9781611975031.96](https://doi.org/10.1137/1.9781611975031.96).
- <span id="page-13-4"></span>[9] Travis Gagie, Simon J. Puglisi, and Daniel Valenzuela. Analyzing relative lempelziv reference construction. In 23rd International Symposium on String Processing and Information Retrieval (SPIRE), volume 9954, pages 160–165. Springer, 2016. [doi:10.1007/978-3-319-46049-9\\_16](https://doi.org/10.1007/978-3-319-46049-9_16).
- <span id="page-13-10"></span>[10] Simon Gog, Timo Beller, Alistair Moffat, and Matthias Petri. From theory to practice: Plug and play with succinct data structures. In 13th International Symposium on Experimental Algorithms (SEA), volume 8504 of Lecture Notes in Computer Science, pages 326–337. Springer.
- <span id="page-13-3"></span>[11] Rodrigo Gonz´alez, Gonzalo Navarro, and H´ector Ferrada. Locally compressed suffix arrays. ACM J. Exp. Algorithmics, 19(1), 2014. [doi:10.1145/2594408](https://doi.org/10.1145/2594408).

- <span id="page-14-6"></span>[12] Roberto Grossi. A quick tour on suffix arrays and compressed suffix arrays. Theor. Comput. Sci., 412(27):2964–2973, 2011. [doi:10.1016/J.TCS.2010.12.036](https://doi.org/10.1016/J.TCS.2010.12.036).
- <span id="page-14-10"></span>[13] Roberto Grossi, Ankur Gupta, and Jeffrey Scott Vitter. High-order entropycompressed text indexes. In 14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 841–850. SIAM, 2003.
- <span id="page-14-8"></span>[14] Toru Kasai, Gunho Lee, Hiroki Arimura, Setsuo Arikawa, and Kunsoo Park. Lineartime longest-common-prefix computation in suffix arrays and its applications. In 12th Annual Symposium on Combinatorial Pattern Matching (CPM), volume 2089 of Lecture Notes in Computer Science, pages 181–192. Springer, 2001. [doi:10.1007/](https://doi.org/10.1007/3-540-48194-X_17) [3-540-48194-X\\_17](https://doi.org/10.1007/3-540-48194-X_17).
- <span id="page-14-7"></span>[15] Dominik Kempa and Dmitry Kosolobov. Lz-end parsing in compressed space. In 2017 Data Compression Conference (DCC), pages 350–359. IEEE, 2017. [doi:](https://doi.org/10.1109/DCC.2017.73) [10.1109/DCC.2017.73](https://doi.org/10.1109/DCC.2017.73).
- <span id="page-14-2"></span>[16] Dominik Kempa and Dmitry Kosolobov. Lz-end parsing in linear time. In 25th European Symposium on Algorithms (ESA), volume 87 of LIPIcs, pages 53:1–53:14. Dagstuhl, 2017. URL: <https://doi.org/10.4230/LIPIcs.ESA.2017.53>, [doi:10.](https://doi.org/10.4230/LIPICS.ESA.2017.53) [4230/LIPICS.ESA.2017.53](https://doi.org/10.4230/LIPICS.ESA.2017.53).
- <span id="page-14-1"></span>[17] Sebastian Kreft and Gonzalo Navarro. Lz77-like compression with fast random access. In 2010 Data Compression Conference (DCC), pages 239–248. IEEE, 2010. [doi:10.1109/DCC.2010.29](https://doi.org/10.1109/DCC.2010.29).
- <span id="page-14-3"></span>[18] Shanika Kuruppu, Simon J. Puglisi, and Justin Zobel. Relative lempel-ziv compression of genomes for large-scale storage and retrieval. In 17th International Symposium on String Processing and Information Retrieval (SPIRE), pages 201–206. Springer, 2010. [doi:10.1007/978-3-642-16321-0\\_20](https://doi.org/10.1007/978-3-642-16321-0_20).
- <span id="page-14-4"></span>[19] Udi Manber and Eugene W. Myers. Suffix arrays: A new method for on-line string searches. SIAM J. Comput., 22(5):935–948, 1993.
- <span id="page-14-0"></span>[20] Takaaki Nishimoto and Yasuo Tabei. Optimal-time queries on bwt-runs compressed indexes. In 48th International Colloquium on Automata, Languages, and Programming (ICALP), volume 198 of LIPIcs, pages 101:1–101:15. Dagstuhl, 2021. [doi:10.4230/LIPICS.ICALP.2021.101](https://doi.org/10.4230/LIPICS.ICALP.2021.101).
- <span id="page-14-5"></span>[21] Ge Nong, Sen Zhang, and Wai Hong Chan. Two efficient algorithms for linear time suffix array construction. IEEE Trans. Computers, 60(10):1471–1484, 2011. [doi:10.1109/TC.2010.188](https://doi.org/10.1109/TC.2010.188).
- <span id="page-14-9"></span>[22] Daisuke Okanohara and Kunihiko Sadakane. Practical entropy-compressed rank/select dictionary. In 9th Workshop on Algorithm Engineering and Experiments (ALENEX). SIAM, 2007. [doi:10.1137/1.9781611972870.6](https://doi.org/10.1137/1.9781611972870.6).

- <span id="page-15-0"></span>[23] Simon J. Puglisi and Bella Zhukova. Relative lempel-ziv compression of suffix arrays. In 27th International Symposium on String Processing and Information Retrieval (SPIRE), volume 12303 of Lecture Notes in Computer Science, pages 89–96. Springer, 2020. [doi:10.1007/978-3-030-59212-7\\_7](https://doi.org/10.1007/978-3-030-59212-7_7).
- <span id="page-15-1"></span>[24] Simon J. Puglisi and Bella Zhukova. Smaller rlz-compressed suffix arrays. In 2021 Data Compression Conference (DCC), pages 213–222. IEEE, 2021. [doi:](https://doi.org/10.1109/DCC50243.2021.00029) [10.1109/DCC50243.2021.00029](https://doi.org/10.1109/DCC50243.2021.00029).
- <span id="page-15-3"></span>[25] Mihai Pˇatra¸scu and Mikkel Thorup. Time-space trade-offs for predecessor search. In 31st Annual ACM Symposium on Theory of Computing (STOC), pages 232–240. ACM, 2006. [doi:10.1145/1132516.1132551](https://doi.org/10.1145/1132516.1132551).
- <span id="page-15-4"></span>[26] Rajeev Raman, Venkatesh Raman, and S. Srinivasa Rao. Succinct indexable dictionaries with applications to encoding k-ary trees and multisets. In 13th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 233–242. SIAM, 2002.
- <span id="page-15-6"></span>[27] Dan E. Willard. Log-logarithmic worst-case range queries are possible in space theta(n). Inform. Process. Lett., 17(2):81–84, 1983. [doi:10.1016/0020-0190\(83\)](https://doi.org/10.1016/0020-0190(83)90075-3) [90075-3](https://doi.org/10.1016/0020-0190(83)90075-3).
- <span id="page-15-5"></span>[28] Bella Zhukova. New space-time trade-offs for pattern matching with compressed indexes. PhD thesis, University of Helsinki, Finland, 2024. URL: [http://hdl.](http://hdl.handle.net/10138/570140) [handle.net/10138/570140](http://hdl.handle.net/10138/570140).
- <span id="page-15-2"></span>[29] Jacob Ziv and Abraham Lempel. A universal algorithm for sequential data compression. IEEE Trans. Inform. Theory, 23(3):337–343, 1977. [doi:10.1109/TIT.1977.](https://doi.org/10.1109/TIT.1977.1055714) [1055714](https://doi.org/10.1109/TIT.1977.1055714).

## <span id="page-16-0"></span>A. Computing the LZ-End Parsing

We look at computing the LZ-End parsing for a given text T ∈ Σ ∗ in practice. The algorithm given originally by Kreft and Navarro [\[17\]](#page-14-1) runs in time O (nh(lg |Σ| + lg lg n)) using FM-index machinery. Kempa and Kosolobov [\[16\]](#page-14-2) greatly improve this and state an algorithm that runs in time O (n), but we conjecture it to be hardly practical.

We focus on their surprisingly simple and practically competitive (albeit suboptimal) O (n lg lg n)-time algorithm, which they implemented in the lz-end-toolkit [5](#page-16-1) accompanying [\[15\]](#page-14-7). However, we find that the description of this algorithm comes either too short (in [\[16\]](#page-14-2)) or gets somewhat lost in the details of surrounding work (in [\[15\]](#page-14-7)). Therefore, we choose to give a concise but comprehensible description here and apply a few further practical improvements. The following observation is the main tool for their algorithm.

<span id="page-16-2"></span>Lemma 1. If f<sup>1</sup> · · · f<sup>z</sup> is the LZ-End parsing of a string T ∈ Σ ∗ , then, for any character α ∈ Σ, the last phrase in the LZ-End parsing of T α is (1) fz−1fzα or (2) fzα or (3) α.

This allows us to compute the LZ-End parsing in a left-to-right scan of T where in each step, we only have to consider to either (1) merge the two most recent phrases, (2) extend the most recent phrase or (3) begin a new phrase consisting of a single character.

Suppose that we already parsed the prefix T[1 .. i − 1] = f<sup>1</sup> · · · f<sup>z</sup> for some position i > 1. In the next step, we compute the LZ-End parsing of T[1 . . . i], i.e., we append T[i]. We look for a phrase f<sup>p</sup> such that a suffix X of T[1 .. i − 1] of maximum possible length is also a suffix of T[1 .. |f<sup>1</sup> · · · fp|]. Then, we greedily decide which of the three aforementioned cases applies. If |X| ≥ |fz−1| + |fz|, it means that the new phrase covers at least the two most recent phrases and we can merge them to (p, |fz−1| + |fz| + 1, T[i]). Otherwise, if |X| ≥ |fz|, we can extend the most recent phrase to (p, |fz| + 1, T[i]). If neither applies, we begin a new phrase (0, 1, T[i]). The core of the problem is clearly finding f<sup>p</sup> and the corresponding suffix X. We employ the following data structures over the reverse input ←− T :

- 1. the inverse suffix array A−<sup>1</sup> , which we can compute in linear time and space using [\[21\]](#page-14-5) and subsequent inversion,
- 2. the LCP array H that can be computed in linear time and space [\[14\]](#page-14-8), where H[i] = lce(T[A[i − 1] .. n], T[A[i] .. n) for i > 1 is the length of the longest common extension (lce) between two lexicographically neighbouring suffixes,
- 3. a data structure that allows for constant-time range minimum queries (rmq) over H, which we can compute in linear time and space [\[7\]](#page-13-7) and
- 4. an associative dynamic predecessor/successor data structure M that is initially empty.

In M, we mark the end positions of already computed LZ-End phrases in lex-space, i.e., a position i ≥ 1 is marked iff there is a phrase f<sup>p</sup> such that A[i] = |f<sup>1</sup> · · · fp|. We

<span id="page-16-1"></span><sup>5</sup> lz-end-toolkit: <https://github.com/dominikkempa/lz-end-toolkit>

<span id="page-17-0"></span>![](_page_17_Figure_0.jpeg)

Figure 3: Search for source phrase candidates in lex-space. Positions that mark the ending locations in M of already computed LZ-End phrases are indicated by the circles. We search for a predecessor starting from i ′ − 1 and a successor starting from i ′ + 1, giving us the locations j ′ L and j ′ R that mark the candidates p<sup>L</sup> and pR, respectively. In case p<sup>L</sup> = z − 1 or p<sup>R</sup> = z − 1, we do another predecessor or successor query starting from j ′ <sup>L</sup> − 1 or j ′ <sup>R</sup> + 1, respectively, to find a candidate for merging.

then use M to lookup the phrase number p using position i as the query key. We note that A is used only conceptually and is only required for the construction of A−<sup>1</sup> and H; it may be discarded afterwards to save space.

In the situation described earlier, we already parsed T[1 .. i − 1] = f<sup>1</sup> · · · f<sup>z</sup> and want to find the longest possible suffix X that is a suffix of T[1 .. |f<sup>1</sup> · · · fp|] for some phrase fp. In M, we search for the predecessor p<sup>L</sup> and the successor pR, respectively, starting from position i ′ = A−<sup>1</sup> [n−i]. The phrases fp<sup>L</sup> and fp<sup>R</sup> are candidates for our sought phrase f<sup>p</sup> because the strings ←−−−−−−−−−−−− T[1 .. |f<sup>1</sup> · · · fp<sup>L</sup> |] and ←−−−−−−−−−−−− T[1 .. |f<sup>1</sup> · · · fp<sup>R</sup> |] are lexicographically closest to ←−−−−−−−− T[1 .. i − 1]. We greedily select p := p<sup>L</sup> or p := p<sup>R</sup> by whichever shares a longer common prefix, which we can compute via a range minimum query over H, respectively.

Some care has to be taken regarding the selection of p: we need p ̸= z (we cannot copy from the last phrase because we extend it) and for merging, we also need p ≠ z − 1 (we cannot copy from a phrase that we merge away). We ensure p ̸= z (which is marked at position i ′ in M because it is |f<sup>1</sup> · · · fz| = i) by offsetting predecessor searches to start from position i ′ − 1 and successor searches to start from position i ′ + 1. To ensure p ̸= z − 1 for merges, we have to check whether p<sup>L</sup> = z − 1 or p<sup>R</sup> = z − 1 and if either is the case, compute the next predecessor or successor, respectively, conceptually skipping phrase fz−<sup>1</sup> in M.

The candidate search is visualized in Figure [3.](#page-17-0) To get overall time O (n lg lg n) to compute the LZ-End parsing, we can implement M using a y-fast trie [\[27\]](#page-15-6) such that each update and query can be done in time O (lg lg n). It is easy to see that the required data structures require space O (n). For further reference, we give the pseudocode for the parsing algorithm in Algorithm [1,](#page-19-0) which is deliberately close to the actual C++ implementation.

<span id="page-18-2"></span>

| Input           | n             | z          | z/n    | lz-end-toolkit | Algorithm 1 |
|-----------------|---------------|------------|--------|----------------|-------------|
| cere            | 461,286,644   | 1,863,246  | 0.40%  | 455            | 582         |
| dblp.xml        | 296,135,874   | 10,244,979 | 3.46%  | 283            | 251         |
| dna             | 403,927,746   | 26,939,573 | 6.67%  | 602            | 379         |
| einstein.en.txt | 467,626,544   | 104,087    | 0.02%  | 454            | 728         |
| english.1024MB  | 1,073,741,824 | 68,034,586 | 6.34%  | 2,119          | 1,160       |
| pitches         | 55,832,855    | 5,675,142  | 10.16% | 27             | 25          |
| proteins        | 1,184,051,855 | 77,369,007 | 6.53%  | 1,651          | 1185        |
| sources         | 210,866,607   | 12,750,341 | 6.05%  | 158            | 135         |

Table 2: Benchmark results showing the parsing times, in seconds, of lz-end-toolkit and Algorithm [1](#page-19-0) (shortest underlined). For each input, we also list the length n, the number z of LZ-End phrases and the ratio z/n as a simple compressibility measure.

### A.1. Experiments

In a small experiment on the same setup as that described in Section [6,](#page-7-0) we compress different files from the Pizza&Chili corpus and compare the performance of our implementation [6](#page-18-1) of Algorithm [1](#page-19-0) against the in-memory implementation featured in the lz-end-toolkit by [\[15\]](#page-14-7). Here, the maximum phrase length remains unbounded (h := ∞). The results are given in Table [2.](#page-18-2)

On most inputs, our implementation is much faster (up to nearly twice as fast on english.1024MB) than the lz-end-toolkit, indicating that the lazy evaluation of predecessor and successor queries as well as the removed layer of indirection via the suffix array A can be very beneficial.

It stands out, however, that the lz-end-toolkit is faster than Algorithm [1](#page-19-0) on highly repetitive inputs (namely cere and einstein.en.txt). The reason is that it is implicitly tuned for this case: the temporary removal of phrase fz−<sup>1</sup> from M is beneficial for the case that merges occur frequently.

We can easily tune Algorithm [1](#page-19-0) for this case as well by re-arranging the candidate search to first look for merges, including a preliminary removal of fz−<sup>1</sup> from M to take advantage of. However, pointing at the results given in Table [2,](#page-18-2) we conjecture that this slows down the algorithm in the general case.

# <span id="page-18-0"></span>B. Engineering RLZSA

Computing the RLZ-compressed suffix array (RLZSA) according to [\[24\]](#page-15-1) requires time and space O (n). More precisely, it requires five integer arrays of length n in RAM. This results in a much higher memory consumption than that of the r-index or Move-r,

<span id="page-18-1"></span><sup>6</sup>We use libsais for computing the suffix and LCP arrays, the data structure of [\[1\]](#page-13-8) for range minimum queries, and a B-tree of [\[6\]](#page-13-9) as an ordered dictionary to implement M.

<sup>1</sup> Function LZ-End(T): <sup>2</sup> <sup>A</sup> <sup>←</sup> suffix array of ←− <sup>T</sup> , <sup>H</sup> <sup>←</sup> LCP array of ←− T (with rmq support) <sup>3</sup> A′ ← array of length n <sup>4</sup> for i ← 0 to n − 1 do A′ [n − A[i] − 1] = i <sup>5</sup> discard <sup>A</sup> and ←− T <sup>6</sup> f<sup>0</sup> ← (0, 0, ε), f<sup>1</sup> ← (0, 1, T[0]), z ← 1, M ← ∅ <sup>7</sup> for i ← 1 to n − 1 do <sup>8</sup> i ′ ← A′ [i − 1] // suffix array neighbourhood of i − 1 in ←− T <sup>9</sup> p<sup>1</sup> ← ⊥, p<sup>2</sup> ← ⊥ /\* find candidates \*/ <sup>10</sup> FindCopySource(LexSmallerPhrase) <sup>11</sup> if p<sup>1</sup> = ⊥ or p<sup>2</sup> = ⊥ then <sup>12</sup> FindCopySource(LexGreaterPhrase) /\* case distinction according to Lemma [1](#page-16-2) \*/ <sup>13</sup> if p<sup>2</sup> ̸= ⊥ then /\* merge phrases fz−<sup>1</sup> and f<sup>z</sup> \*/ <sup>14</sup> M ← M \ {(A′ [i − |fz| − 1], ∗)} // unmark phrase fz−<sup>1</sup> <sup>15</sup> fz−<sup>1</sup> ← (p2, |fz| + |fz−1| + 1, T[i]) <sup>16</sup> z ← z − 1 <sup>17</sup> else if p<sup>1</sup> ̸= ⊥ then /\* extend phrase f<sup>z</sup> \*/ <sup>18</sup> f<sup>z</sup> ← (p1, |fz| + 1, T[i]) <sup>19</sup> else /\* begin new phrase \*/ <sup>20</sup> M ← M ∪ {i ′ , z} // lazily mark phrase f<sup>z</sup> <sup>21</sup> fz+1 ← (0, 1, T[i]) <sup>22</sup> z ← z + 1 <sup>23</sup> return (f1, . . . , fz) <sup>24</sup> Function FindCopySource(f): <sup>25</sup> (j ′ , p, ℓ) ← f(i ′ ) <sup>26</sup> if ℓ ≥ |fz| then <sup>27</sup> p<sup>1</sup> ← p <sup>28</sup> if i > |fz| then <sup>29</sup> if p = z − 1 then (j ′ , p, ℓL) ← f(j ′ ) <sup>30</sup> if ℓ ≥ |fz| + |fz−1| then p<sup>2</sup> ← p <sup>31</sup> Function LexSmallerPhrase(i ′ ): <sup>32</sup> if M contains the predecessor j ′ ≤ i ′ − 1 with j ′ 7→ p then <sup>33</sup> return (j ′ , p, H[rmq(j ′ + 1, i′ )]) <sup>34</sup> else return (0, 0, 0) <sup>35</sup> Function LexGreaterPhrase(i ′ ): <sup>36</sup> if M contains the successor j ′ ≥ i ′ + 1 with j ′ 7→ p then <sup>37</sup> return (j ′ , p, H[rmq(i ′ + 1, j′ )]) <sup>38</sup> else return (0, 0, 0)

<span id="page-19-0"></span>Algorithm 1: Algorithm to compute the LZ-End parsing for a string T ∈ Σ n . Note that here, we use zero-based indexing to more closely resemble the implementation. For i ∈ [0, n − 1] and p ∈ [0, z], we say that i 7→ p iff position i is marked in M and phrase f<sup>p</sup> ends at the corresponding location.

which need only O (|PFP|) space, where PFP is a prefix-free parsing of the input T with |PFP| ≪ n. This motivates our optimized construction algorithm, which requires only O (r) space and O r <sup>1</sup>−ϵn ϵ time for a parameter ϵ ∈ [0, 1] (see Appendix [B.4\)](#page-22-0), making RLZSA practical for indexing large texts.

In order to reason about our optimized implementation, we first briefly summarize the original implementation from [\[24\]](#page-15-1) in Appendix [B.1.](#page-20-0) Then, we discuss our adjusted set of RLZSA index data structures, our new reference construction algorithm and practical optimizations for the RLZ parsing algorithm.

<span id="page-20-1"></span>Definition 1. Given a reference R ∈ Σ ∗ , we call ⟨s1, l1⟩,⟨s2, l2⟩, . . . ,⟨sz, lz⟩ a Relative Lempel-Ziv (RLZ) parsing of T w.r.t. R if P<sup>z</sup> <sup>i</sup>=1 max(1, li) = n and T[p<sup>i</sup> , p<sup>i</sup> + li) = R[s<sup>i</sup> , s<sup>i</sup> +li) is the longest prefix of Tp<sup>i</sup> that occurs in R, if it exists, or ⟨s<sup>i</sup> , li⟩ = ⟨T[p<sup>i</sup> ], 0⟩, else, for each i ∈ [1, z] and p<sup>i</sup> = 1 + Pi−<sup>1</sup> <sup>j</sup>=1 max(1, l<sup>j</sup> ).

Definition 2. Let S and x be sequences and let k ≥ 1 be an integer. Then, Kk(S) denotes the set of k-mers that occur in S and #S(x) denotes the number of occurrences of x in S.

#### <span id="page-20-0"></span>B.1. Summary of the original RLZSA Implementation

The RLZSA index described in [\[24\]](#page-15-1) consists of the r-index – without a data structure for computing Φ and without suffix array samples – and the RLZSA, represented using the following arrays:

- R the reference R stored in O (|R| lg n) bits,
- S[1 .. z] = [s1, . . . , sz] the phrase source positions in R or literal phrases, respectively, stored in O (z lg n) bits,
- PL[1 .. z] = [l1, . . . , lz] the phrase lengths, stored also in O (z lg n) bits and
- PS[1 .. ⌊z/a⌋] samples of phrase starting positions where PS[i] = pai, stored in O (z/a lg n) bits.

Here, a ≥ 1 is an integer sampling parameter. We set a := 64 as described by the authors. To reduce space usage in practice, the copy phrase length has been limited to 2<sup>16</sup> . R is constructed by dividing A<sup>d</sup> into consecutive segments S1, . . . , Sn′ of size s, scoring those, and iteratively adding a segment that maximizes the score until R has reached its target size. A segment's score rises with the frequencies in A<sup>d</sup> of the new k-mers that it adds to R. More formally, the score of segment S<sup>i</sup> = A<sup>d</sup> [is .. (i + 1)s) is

$$
f(S_i) = \sum_{x \in \mathcal{K}_k(S_i) \setminus \cup_{j \in C} \mathcal{K}_k(S_j)} \sqrt{\#A^d(x)},
$$

where C ⊆ [1, n′ ] is the set containing the indices of already chosen segments.

The reference sizes in [\[24\]](#page-15-1) have been tuned manually for each input. Regarding the RLZ parsing, it is modified such that every referencing phrase is preceded by a literal phrase. This simplifies the query procedure and reduces query time in the case that the start of the interval to extract lies at the end of a long series of long copy phrases.

### <span id="page-21-0"></span>B.2. Index Data Structures

We begin by discussing our choice of index data structures. It consits of the arrays

- R the reference R stored in O (|R| lg n) bits,
- PT[1 .. z] phrase types, where PT[i] := 1 ⇔ l<sup>i</sup> = 0 stored as a bit vector with constant-time rank/select support in z + o (z) bits,
- LP[1 .. z<sup>l</sup> ] – literal phrases, where LP[i] := s<sup>j</sup> with j = PT.select1(i), stored in O (z<sup>l</sup> lg n) bits (where z<sup>l</sup> is the number of literal phrases),
- SR[1 .. zc] phrase source positions in R, where SR[i] := s<sup>j</sup> with j = PT.select0(i) stored in O (z<sup>c</sup> lg |R|) bits (where z<sup>c</sup> is the number of copy phrases),
- CPL[1 .. zc] copy phrase lengths, where CPL[i] := l<sup>j</sup> with j = PT.select0(i) stored in O (z<sup>c</sup> lg(n/z)) bits) and
- SCP[1 .. ⌊zc/a⌋] samples of copy phrase starting positions in T, where SCP[i] := p<sup>j</sup> with j = PT.select0(ai), stored as an s-array [\[22,](#page-14-9) [10\]](#page-13-10) using (zc/a)lg(na/zc)+2zc/a+ o (zc/a) bits.

Here, a ≥ 1 is an integer sampling parameter. In practice, we use a = 4. Since |R| ≪ n in practice, storing ⌈lg n⌉ bits per value in S is wasteful. Therefore, we split S up into two arrays SR and LP and the bit vector PT. Then, we can store SR with ⌈lg |R|⌉ bits per value. As in [\[24\]](#page-15-1), we limit the copy phrase length to 2<sup>16</sup> such that we can store CPL using 16 bits per value. Finally, we replace copy phrases of length one with literal phrases. This reduces the number of cache misses caused by lookups in R and reduces the index size, because one value in SR and CPL, respectively, and 1/a values in SCP are replaced by one value in LP.

Additionally, we store MLF , L ′ from Move-r [\[2\]](#page-13-1) and RSL′ from Appendix [C](#page-25-0) to compute the suffix array interval of a pattern. Finally, we store the array SA<sup>s</sup> [1..r′ ], where SA<sup>s</sup> [i] = A[MLF.p[i]] and maintain z(i) and ˆb ′ z(i) during the backward search phase of a locate query, where z(i) = A[b<sup>i</sup> ] is defined analogously to [\[2,](#page-13-1) Definition 12]. Then, we can compute A[b] = SA<sup>s</sup> [ ˆb ′ <sup>z</sup>(1)] − z(1) in constant time after the backward search. This eliminates the need for the rule that each copy phrase has to be preceded by a literal phrase, as we do not have to decode the region between the suffix array interval and the last literal phrase before it. Additionally, this reduces the overall number of phrases by a factor up to two.

#### <span id="page-21-1"></span>B.3. Queries

We now show how to answer queries using our data structures. Storing SR, LP and PT instead of S makes the query procedure more complicated, because we now have to also maintain the indices xcp and xlp of the current copy- and literal phrases, respectively, i.e, x is the index of the phrase containing b + 1, xcp is the index of the last copy-phrase starting at or before b + 1 and xlp is the index of the last literal phrase at or before b + 1.

<span id="page-22-1"></span>![](_page_22_Figure_0.jpeg)

Figure 4: Illustration of a the initialization phase of an RLZSA query. "C" and "L" indicate that the interval in A<sup>d</sup> is a copy/literal phrase.

Lastly, we need the starting position p<sup>x</sup> of the phrase containing b + 1.

To compute those values, we at first compute a lower bound x ′ cp ← a · SCP.rank1(b) for xcp (see Figure [4\)](#page-22-1). This takes time O (lg(na/zc)) [\[10\]](#page-13-10). x ′ ← PT.select0(x ′ cp) gives us the phrase index of the x ′ cp-th copy phrase. Then, we compute its starting position px′ ← SCP.select1(x ′ cp/a) in constant time [\[10\]](#page-13-10). Using x ′ , x ′ cp and px′, we can then traverse RLZSA to the right until the x ′ -th phrase contains b + 1. More formally, we compute x, xlp and p<sup>x</sup> by the equation in Figure [4.](#page-22-1) Finally, xlp ← x − xcp gives us the current literal phrase index. This takes overall time O (a) if we use PT.select<sup>0</sup> queries to skip blocks of consecutive literal phrases in O (1) time. Decoding A(b, e] using x, xcp, xlp and p<sup>x</sup> is similar and takes optimal time O (|[b, e]|).

#### <span id="page-22-0"></span>B.4. Index Construction

In the following, we describe the construction of RLZSA. After constructing A, MLF , L ′ , RSL′ and SA<sup>s</sup> in O (n + r lg r) time, O (n) external space and O (|PFP|) space in the RAM using Big-BWT [\[3\]](#page-13-5), we will only access A in external memory. Our algorithm for constructing R follows a similar method as the algorithm presented in [\[24\]](#page-15-1), but reduces the running time from O (n) to O r <sup>1</sup>−ϵn ϵ for a parameter ϵ ∈ [0, 1]. The space is also reduced from O (n) to O (r). Instead of splitting A<sup>d</sup> into consecutive segments of size s, we consider arbitrary segments of size s and set the k-mer length to k = 1, because when setting the segment size optimally (s = 3072), the number of phrases in RLZSA only rises when increasing k beyond 1. Choosing k = 1 also simplifies the computation of

all 1-mer frequencies. We show how this can be done efficiently in the following section. Then, we describe the construction of the reference.

### B.4.1. Computing the frequencies of all values in A<sup>d</sup>

To see how we can compute the frequencies of all values (1-mers) in A<sup>d</sup> , we need the following lemma from [\[8\]](#page-13-0).

Definition 3. Let l1, . . . , l<sup>r</sup> be the starting positions of the runs in L, and let lr+1 = n+ 1. Let Φ be a function such that Φ(A[i]) = A[(i − 1) mod n].

Lemma 2 ([\[8\]](#page-13-0), Lemma 3.5). Let {u1, u2, . . . , ur+1} = {A[l1], A[l2], . . . , A[lr], n + 1} and u<sup>1</sup> < u<sup>2</sup> < · · · < ur+1 = n + 1. Then Φ(i) = Φ(ux) + (i − ux) for u<sup>x</sup> ≤ i < ux+1.

We now show the following.

<span id="page-23-0"></span>Theorem 2. (i) Given I<sup>Φ</sup> = (u1, Φ(u1)), . . . ,(ur, Φ(ur)), we can compute the frequencies of all values in A<sup>d</sup> in O (r) expected time and space, and (ii) there are ≤ r + 1 distinct values in A<sup>d</sup> .

Proof. We compute a hash map H # A<sup>d</sup> that maps ⟨A<sup>d</sup> [i] → #A<sup>d</sup> (A<sup>d</sup> [i])⟩, for i ∈ [1, n]. We start by inserting ⟨A[1] → 1⟩ into H # A<sup>d</sup> . Then, we iterate with x from 1 to r. Each value i ∈ [ux, ux+1) is mapped to Φ(i) = Φ(ux) + (i − ux) by [\[8,](#page-13-0) Lemma 3.5]. Hence Ad [A−<sup>1</sup> [i]] = i − Φ(i) = u<sup>x</sup> − Φ(ux) for i ̸= A[1]. Let v = u<sup>x</sup> − Φ(ux) and f = ux+1 − ux, if x ̸= r, and f = ux+1 − u<sup>x</sup> − 1, else (this avoids counting A[1] − A[n]). Now, we check, whether there is a mapping ⟨v → f ′ ⟩ ∈ H # A<sup>d</sup> . If so, then we increment f ′ by f. Else, we insert ⟨v → f⟩ into H # A<sup>d</sup> . Since the intervals [ux, ux+1) are disjoint, we consider i = A[j] in exactly one iteration, for each j ∈ [2, n]. Hence, this algorithm correctly computes H # A<sup>d</sup> .

Note that the number of mappings in H # A<sup>d</sup> is equal to the number of distinct values in A<sup>d</sup> . This number is at most ≤ r + 1, because we add at most one mapping for each one of the r intervals [ux, ux+1), and separately handling A<sup>d</sup> [1] = A[1] adds at most one extra value. Thus, we have shown (i) and (ii).

#### B.4.2. Reference Construction

Our algorithm uses H # A<sup>d</sup> from Theorem [2](#page-23-0) to iteratively choose segments. We maintain a balanced search tree T<sup>s</sup> = {⟨b1, e1⟩, . . . , ⟨b<sup>N</sup> , e<sup>N</sup> ⟩} with 1 < b<sup>1</sup> < e<sup>1</sup> < · · · < b<sup>N</sup> < e<sup>N</sup> < n to represent the current state of R = A<sup>d</sup> [b<sup>1</sup> .. e1]A<sup>d</sup> [b<sup>2</sup> .. e2] . . . A<sup>d</sup> [b<sup>N</sup> .. e<sup>N</sup> ]. We also maintain that H # <sup>A</sup><sup>d</sup> maps ⟨A<sup>d</sup> [i] → #A<sup>d</sup> (A<sup>d</sup> [i])⟩, if A<sup>d</sup> [i] ∈/ R, and ⟨A<sup>d</sup> [i] → 0⟩, else, for i ∈ [1, n]. We set t<sup>R</sup> = O (r) as the target size for R.

Scoring segments. In each iteration, we consider M = O ((n/r) ϵ ) (with ϵ ∈ [0, 1]) random candidate segments [l1, r1], . . . , [lM, rM] ⊆ [1, n] of fixed length s. Given a candidate segment [l<sup>i</sup> , r<sup>i</sup> ], we at first shorten it from the left and/or the right (using a

successor search over Ts), such that it does not intersect the already chosen segments. More precisely, we instead consider the segment [l ′ i , r′ i ] = [l<sup>i</sup> , r<sup>i</sup> ] \ ∪j∈[1,M] [b<sup>j</sup> , e<sup>j</sup> ]. This segment must be connected, because |[b<sup>j</sup> , e<sup>j</sup> ]| ≥ s ∀j ∈ [1, M]. This takes time O (lg N) = O (lg(tR/s)). If [l ′ i , r′ i ] ̸= ∅, then we compute its score

$$
f([l'_i, r'_i]) = \left(\sum_{x \in \mathcal{K}_1(A^d[l'_i \dots r'_i]) \backslash \mathcal{K}_1(R)} \sqrt{\#A^d(x)}\right) / |[l'_i, r'_i]|
$$
  
= 
$$
\left(\sum_{x \in \mathcal{K}_1(A^d[l'_i \dots r'_i])} \sqrt{H_{A^d}^{\#}[x]}\right) / |[l'_i, r'_i]|.
$$

This can be done in expected time O (|[l ′ i .. r′ i ]|) = O (s) by scanning over A<sup>d</sup> [l ′ i .. r′ i ] once and maintaining a temporary hashtable storing the already considered values in Ad [l ′ i .. r′ i ]. Thus, scoring all segments takes O (M(s + lg(tR/s))) expected time.

Adding the best segment. Let [l ′ m, r′ <sup>m</sup>] be a segment that maximizes the score. We update H # A<sup>d</sup> to map ⟨A<sup>d</sup> [j] → 0⟩ for each j ∈ [l ′ m, r′ <sup>m</sup>] in O (s) expected time. To reduce the number of segments stored in T<sup>s</sup> and thereby also memory consumption, we merge [l ′ m, r′ <sup>m</sup>] with already chosen directly adjacent segments. More precisely, if there exists a pair ⟨bx, ex⟩ ∈ T<sup>s</sup> with e<sup>x</sup> + 1 = l ′ <sup>m</sup>, then we remove ⟨bx, ex⟩ from T<sup>s</sup> and set l ′ <sup>m</sup> ← bx. Similarly, if there exists a pair ⟨by, ey⟩ ∈ T<sup>s</sup> with b<sup>y</sup> − 1 = r ′ <sup>m</sup>, then we remove ⟨by, ey⟩ from T<sup>s</sup> and set r ′ <sup>m</sup> ← ey. Finally, we insert ⟨l ′ m, r′ <sup>m</sup>⟩ into T<sup>s</sup> . We stop as soon as |R| ≥ (1 − ϵ ′ )tR, where ϵ ′ ∈ [0, 1]. The search in T<sup>s</sup> takes time O (lg(tR/s)) time. Thus, adding one segment takes overall time O (s + lg(tR/s)).

Running time and memory consumption. If we assume that the expected length of the newly added segment [l ′ m, r′ <sup>m</sup>] (before merging) is Θ (s), then this algorithm takes overall expected time O ((tR/s) · M · (s + lg(tR/s))) = O (r · (n/r) ϵ ) = O r <sup>1</sup>−ϵn ϵ (for lg(tR/s) = O (s)) and space O (r + tR/s) = O (r). In practice, we set t<sup>R</sup> := min(5.2r, n/3), s := 3072, M := 5(n/r) ϵ , ϵ = 0.45 and ϵ ′ = 1/20.

Post processing. As a post-processing step, we close short gaps between long adjacent segments. We maintain T<sup>s</sup> and additionally a balanced search tree T<sup>g</sup> initialized with T<sup>g</sup> = {⟨g1, s1⟩, . . . ,⟨gN−1, sN−1⟩}, where initially g<sup>i</sup> = b<sup>i</sup> and s<sup>i</sup> = |[b<sup>i</sup> , ei+1]|/|(e<sup>i</sup> , bi+1)| hold for i ∈ [1, N −1]. Each pair ⟨b<sup>i</sup> , si⟩ represents the gap (e<sup>i</sup> , bi+1) between the segments [bi , e<sup>i</sup> ] and [bi+1, ei+1], and its score s<sup>i</sup> is the length |[b<sup>i</sup> , ei+1]| of the connected segment resulting from closing the gap relative to the cost |(e<sup>i</sup> , bi+1)| for closing it, i.e, the length of the gap. In Tg, the pairs are ordered by their scores.

As long as T<sup>g</sup> ̸= ∅, we iteratively consider the pair ⟨b<sup>i</sup> , si⟩ with the highest score, and remove it from Tg. We check whether we can close the gap (e<sup>i</sup> , bi+1) it represents without exceeding t<sup>R</sup> (using a successor search over Ts). If |R| + |(e<sup>i</sup> , bi+1)| ≤ tR, then we close the gap by merging [b<sup>i</sup> , e<sup>i</sup> ] and [bi+1, ei+1] into [b<sup>i</sup> , ei+1] in T<sup>s</sup> and possibly update the

scores and starting positions of the gaps (ei−1, bi) and (ei+1, bi+2) in T<sup>g</sup> (if they exist, respectively) using searches over Tg. Since we consider and search for a constant number of gaps and segments per iteration, and each search over T<sup>s</sup> and T<sup>g</sup> takes time O (lg(tR/s)) time, the post-processing takes overall time O ((tR/s) lg(tR/s)) = O (r lg r).

Finally, we build R by iterating once over T<sup>s</sup> in time O (r).

#### <span id="page-25-1"></span>B.4.3. Computing the RLZ Parsing

To compute the RLZ parsing of <sup>A</sup><sup>d</sup> w.r.t. <sup>R</sup>, we build the Move-<sup>r</sup> index [\[2\]](#page-13-1) for ←− R. However, since we only need to compute one occurrence in R per RLZ phrase, we do not construct M<sup>Φ</sup> and SAΦ. Instead, we store the array SA′ s [1 .. r′ ] [\[2\]](#page-13-1). Then, we can compute exactly one occurrence SA′ s [ˆe ′ <sup>y</sup>(1)] − y(1) per locate query.

Suppose we have computed the parsing up to phrase i − 1 and want to compute the i-th phrase. Recall from Definition [1](#page-20-1) that we compute the longest prefix of A<sup>d</sup> [pi .. n] that occurs in R with a backward search using Move-r over ←− R. Let j ∈ [1, n − i + 1] be the minimum length such that A<sup>d</sup> [pi .. p<sup>i</sup> + j) has exactly one occurrence o in R if it exists. Then s<sup>i</sup> = o will produce a valid RLZ parsing, hence we can abort the backward search after j steps, compute s<sup>i</sup> , and instead scan for l<sup>i</sup> in R, i.e, increment j until p<sup>i</sup> + j = n ∨ s<sup>i</sup> + j = |R| ∨ A<sup>d</sup> [p<sup>i</sup> + j] ̸= R[s<sup>i</sup> + j] in order to get j = l<sup>i</sup> .

## <span id="page-25-0"></span>C. Engineering Rank/Select Data Structures for Move-r

In Move-r, we need a data structure to answer a particular combination of rank and select queries on a string T ∈ Σ n in constant time using at most O (n) words of space. Given a character c ∈ Σ and a position i ∈ [1, n], we want to either (1) compute select(T, c,rank(T, c, i) + 1) or (2) compute select(T, c,rank(T, c, i)). The former can be considered a successor query, while the latter resembles a predecessor query for occurrences of a character.

While this is a classic use case for wavelet trees [\[13\]](#page-14-10), we make use of the fact that the queries only ever happen in a specific combination. In the following, we present two simple, but practically efficient data structures that improve upon [\[2\]](#page-13-1).

Let σ = |Σ| be the size of the alphabet. Our first data structure is aimed at the case where σ is small (σ = O (1)), while the second one is aimed at large alphabets (σ = n <sup>O</sup>(1)). The latter is designed particularly for the construction of the RLZ parsing for RLZSA (see Appendix [B.4.3\)](#page-25-1) because there, we use Move-r over ←− R featuring a large alphabet.

#### C.1. Small Alphabets

We first consider small alphabets, i.e., we assume σ = O (1). This scenario allows us to afford precomputing the answers for all possible query characters c for a subset (sampling) of positions. More precisely, given an integer sampling parameter s ≥ 1, we store two two-dimensional arrays X[1 .. ⌊n/B⌋][1 .. σ] and Y [1 .. ⌊n/B⌋][1 .. σ] with B = ⌈σs⌉ as follows:

$$
X[b][c] := \begin{cases} \text{select}(T, c, \text{rank}(T, c, bB) + 1) & \text{if } c \text{ occurs in } T[bB \dots n] \\ \infty & \text{otherwise} \end{cases}
$$
\n
$$
Y[b][c] := \begin{cases} \text{select}(T, c, \text{rank}(T, c, bB)) & \text{if } c \text{ occurs in } T[1 \dots bB - 1] \\ -\infty & \text{otherwise} \end{cases}
$$

This data structure can be stored in space O (σn/B) = O (n/s) and can be constructed in time O (n + σn/B) = O ((1 + 1/s)n) time by scanning over T once in both directions.

Given a query of the first type (successor) with position i and character c, we start by computing the first block b = ⌈i/B⌉ starting after i and set p := X[b][c]. Now, we scan over the preceding block in reverse and look for an occurrence of c, i.e., for j from b · B to i, we set p := j if T[j] = c. Finally, we return p. The second type of query (predecessor) can be answered similarly using Y . This takes overall time O (B) = O (σs) = O (1) and requires O (n) words of space since σ and s are constants. In practice, we set s := 4.

We note that by using this data structure to implement RSL′ in Move-r, we get optimal time O (m) for count and O (m + occ) for locate queries in overall O (r) words of space.

#### <span id="page-26-0"></span>C.2. Large Alphabets

We now consider large alphabets, i.e., σ = n <sup>O</sup>(1). The previously shown data structure is not suitable because answering queries takes time O (σs). However, since the alphabet is large, we can exploit the fact that most characters occur infrequently to speed up queries. Particularly, if a character has only few occurrences, then storing their positions in ascending order and scanning over (or performing binary search on) them for querying is sufficiently fast in practice. For frequent characters, using an s-array [\[22,](#page-14-9) [10\]](#page-13-10) to marking the occurrences of in T is faster. Our rank/select data structure combines both of these methods to achieve good performance in every case. It consists of the arrays

- C[1 .. σ + 1] with C[c] := |{T[i] < c |i ∈ [1, n]}|,
- O[1 .. n] with O[C[c] + j] := i such that T[i] is the j-th occurrence of c ∈ Σ and
- S[1 .. σ] where S[c] is an s-array marking the occurrences of c in T, if #T(c) > 512.

requiring O (n(lg n + H0(T))) bits of space, where H0(T) denotes the zeroth-order entropy of T. The space is dominated by the s-arrays that are also used in [\[2,](#page-13-1) Appendix B] and is asymptotically no larger than a Huffman-shaped wavelet tree unless H0(T) = o (lg n).

We can answer select(T, c, i) = O[C[c] + i] in constant time. To answer rank(T, c, i), we at first consider the number o = C[c + 1] − C[c] of occurrences of c in T. If o > 512, we use the s-array to compute rank(T, c, i) = S[c].rank1(i) in time O (lg(n/#T(c))) [\[10\]](#page-13-10). Otherwise, we compute x := min{j ∈ [C[c], C[c + 1])| O[j] > i} if it exists (using binary search if o > 16 or by linearly scanning otherwise). If x does not exist, then we output o. Otherwise, we output x − C[c]. This takes constant time because o ≤ 512 = O (1).