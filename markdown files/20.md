# Joint Asymmetric Loss for Learning with Noisy Labels

<span id="page-0-0"></span>Jialiang Wang Xianming Liu\* Xiong Zhou Gangfeng Hu Deming Zhai Junjun Jiang Harbin Institute of Technology

> Xiangyang Ji Tsinghua University

## Abstract

*Learning with noisy labels is a crucial task for training accurate deep neural networks. To mitigate label noise, prior studies have proposed various robust loss functions, particularly symmetric losses. Nevertheless, symmetric losses usually suffer from the underfitting issue due to the overly strict constraint. To address this problem, the Active Passive Loss (APL) jointly optimizes an active and a passive loss to mutually enhance the overall fitting ability. Within APL, symmetric losses have been successfully extended, yielding advanced robust loss functions. Despite these advancements, emerging theoretical analyses indicate that asymmetric losses, a new class of robust loss functions, possess superior properties compared to symmetric losses. However, existing asymmetric losses are not compatible with advanced optimization frameworks such as APL, limiting their potential and applicability. Motivated by this theoretical gap and the prospect of asymmetric losses, we extend the asymmetric loss to the more complex passive loss scenario and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We rigorously establish the necessary and sufficient condition under which AMSE satisfies the asymmetric condition. By substituting the traditional symmetric passive loss in APL with our proposed AMSE, we introduce a novel robust loss framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate the effectiveness of our method in mitigating label noise. Code available at:* [https://github.com/cswjl/](https://github.com/cswjl/joint-asymmetric-loss) [joint-asymmetric-loss](https://github.com/cswjl/joint-asymmetric-loss)

## 1. Introduction

Deep neural networks (DNNs) have demonstrated outstanding performance in a wide range of machine learning tasks [\[8,](#page-8-0) [15\]](#page-8-1). However, the prevalence of noisy labels in realworld datasets remains a significant challenge, often arising from human carelessness or a lack of domain expertise [\[8\]](#page-8-0).

![](_page_0_Figure_8.jpeg)

Figure 1. Visualizations of 2D t-SNE [\[21\]](#page-8-2) embeddings of learned representations on the CIFAR-10 test set, from models trained with 0.4 symmetric noise. The representations learned by the proposed JAL method are with more separated and clearly bound margin.

Applying supervised learning methods directly to noisy labeled data typically degrades model performance [\[1\]](#page-8-3). Furthermore, the ability to generalize from noisy supervision is crucial for aligning large language models [\[3\]](#page-8-4). As a result, developing noise-tolerant learning techniques has become a critical and increasingly studied problem within weakly supervised learning. Among various approaches proposed in the literature, designing robust loss functions has gained particular popularity due to its simplicity and broad applicability [\[7,](#page-8-5) [18,](#page-8-6) [31,](#page-9-0) [33\]](#page-9-1).

Previous works [\[7,](#page-8-5) [19,](#page-8-7) [22\]](#page-8-8) theoretically proved that symmetric loss functions are inherently tolerant to label noise

<sup>\*</sup>Corresponding author

<span id="page-1-0"></span>under some moderate assumptions. However, the fitting ability of symmetric loss functions is constrained by the overly strict symmetric condition [\[33\]](#page-9-1). Symmetric loss functions such as Mean Absolute Error (MAE) [\[7\]](#page-8-5) have proven challenging to optimize. To address this underfitting issue, inspired by complementary learning [\[11\]](#page-8-9), Ma et al. [\[18\]](#page-8-6) proposed the Active Passive Loss (APL) framework. They categorize loss functions into two types: 1)"Active loss", which only explicitly maximizes the probability of the labeled class, and 2) "Passive loss", which also explicitly minimizes the probabilities of other classes. APL simultaneously employs an active loss and a passive loss to enhance each other's optimization processes, improving overall fitting performance. By incorporating symmetric losses within the APL framework, several advanced robust loss functions have been developed [\[18,](#page-8-6) [30\]](#page-9-2).

Recently, Zhou et al. [\[33,](#page-9-1) [35\]](#page-9-3) proposed a novel class of robust loss functions called Asymmetric Loss Functions (ALFs). Their theoretical analysis shows that ALFs offer noise-tolerance to label noise under a more relaxed condition compared to symmetric loss functions. However, existing asymmetric loss functions, such as Asymmetric Unhinged Loss (AUL) , are all active losses, as achieving the asymmetric condition for passive losses remains a challenging problem. Unfortunately, our explorations indicate that these existing asymmetric loss functions are not compatible with the APL framework. The absence of a theoretical foundation for asymmetric loss functions in the passive loss scenario makes them unsuitable for the APL framework, thereby limiting their potential and practical applications.

In this paper, we extend asymmetric losses to the passive loss scenario, which is more challenging to analyze. We propose a new asymmetric passive loss function, called *Asymmetric Mean Square Error* (AMSE). Our proposed AMSE is both simple and theoretically sound, and we rigorously establish the necessary and sufficient condition for it to satisfy the asymmetric condition. By replacing the traditional symmetric loss in APL with our proposed AMSE, we introduce a new framework called *Joint Asymmetric Loss* (JAL). Our JAL enhances the traditional APL framework while preserving the complete noise-tolerance. Our key contributions are highlighted as follows:

- We extend asymmetric losses to the more challenging passive loss scenario and propose a novel asymmetric loss function, *Asymmetric Mean Square Error* (AMSE). Additionally, we rigorously establish the necessary and sufficient conditions for AMSE to satisfy the asymmetry condition.
- By incorporating the proposed AMSE into the APL framework, we introduce a novel approach called *Joint Asymmetric Loss* (JAL), which ensures robustness and enhances sufficient learning.

• We conducted comprehensive ablation and comparison experiments. The extensive results highlight the superiority of our method.

# 2. Related Work

Learning with noisy labels, or called noise-tolerant learning, aims to train a robust model in the presence of noisy labels. Our paper concentrates on one prevalent research avenue: designing robust loss functions.

Ghosh et al. [\[7\]](#page-8-5), Manwani and Sastry [\[19\]](#page-8-7), Van Rooyen et al. [\[22\]](#page-8-8) theoretically demonstrated that a loss function would be inherently tolerant to label noise as long as it satisfies the symmetric condition. However, symmetric loss functions are difficult to optimize due to the over-strictmetric condition, such as Mean Absolute Error (MAE). This drawback motivates some works to combine the robust MAE with the well-fitting Cross Entropy (CE). Examples of such mixture loss functions include Generalized Cross Entropy (GCE) [\[32\]](#page-9-4), Symmetric Cross Entropy (SCE) [\[24\]](#page-8-10), Taylor Cross Entropy (Taylor-CE) [\[6\]](#page-8-11), and Jensen-Shannon Divergence Loss (JS) [\[5\]](#page-8-12). These mixture loss functions often select an intermediate value between the gradients of CE and MAE, representing a trade-off between fitting ability and robustness. Sparse Regularization (SR) [\[34\]](#page-9-5) and ϵ-Softmax [\[23\]](#page-8-13) approximate one-hot vectors to achieve a relaxed symmetric condition. Inspired by the complementary learning (NLNL [\[11\]](#page-8-9) and JNPL [\[12\]](#page-8-14)), Active Passive Loss (APL) [\[18\]](#page-8-6) and Active Negative Loss (ANL) [\[30\]](#page-9-2), use two different symmetric losses simultaneously to improve the fitting ability. Recently, Zhou et al. [\[33,](#page-9-1) [35\]](#page-9-3) proposed a new family of robust loss functions for clean-label-dominant noise, namely asymmetric loss functions (ALFs). ALFs demonstrated better performance compared to symmetric loss functions. Wei et al. [\[27\]](#page-9-6) proposed a new label smoothing method, called Negative Label Smoothing (Negative-LS), improving robustness when learning with noisy labels. In addition, PHuber-CE [\[20\]](#page-8-15) and LogitClip (LC) [\[25\]](#page-8-16) mitigate the memorization of noisy labels by clamping the gradient and logit, respectively.

## 3. Preliminary

Problem Definition. Considering a classification problem, we denote X ⊂ R d as the sample space and Y = [K] = {1, 2, ..., K} as the label space, where K is the number of classes. In the supervised scenario, a labeled dataset S = {(xn, yn)} N <sup>n</sup>=1 is typically available for training classifiers, where (xn, yn) are i.i.d draws from an underlying distribution D over X × Y. The classifier f : X → P is a model with a softmax layer that maps the sample space X to the probability simplex P, where P = p ∈ [0, 1]<sup>K</sup> | 1 <sup>⊤</sup>p = 1 . The predicted label is then given by yˆ = arg max<sup>k</sup> f(x)k. Moreover, let <span id="page-2-1"></span>L : P × Y → R represent the classification loss function L(f(x), ey), where e<sup>y</sup> is the one-hot vector with its y-th element set to 1. In this paper, we consider the loss functional, L(u, v) = P<sup>K</sup> <sup>k</sup>=1 ℓ(uk, vk) with a basic loss function ℓ, where u<sup>k</sup> is the k-th element of the vector u. For the sake of brevity, we abbreviate L(f(x), ek) as L(f(x), k) in the following.

Label Noise Model. In the context of learning with noisy labels, we have access to a noisy training set S˜ = {(xn, y˜n)} N <sup>n</sup>=1 instead of its clean counterpart, \protect \mathcal {S}. For a given sample {\mathbf {x}}, the noise corruption process is characterized by the flipping of the true label y into the observed label \protect \tilde {y} with a conditional probability as follows:

$$
\tilde{y} = \begin{cases} y & \text{with probability} \quad \eta_{\mathbf{x}, y} = 1 - \eta_{\mathbf{x}} \\ k, k \in [K], k \neq y & \text{with probability} \quad \eta_{\mathbf{x}, k} \end{cases} (1)
$$

P where the overall noise rate for {\mathbf {x}} is given by η<sup>x</sup> = k̸=y ηx,k.

Following previous works [\[7,](#page-8-5) [18,](#page-8-6) [28,](#page-9-7) [30\]](#page-9-2), we primarily focus on three prevalent types of label noise: 1) Symmetric Noise: ηx,y = 1 − η and ηx,k̸=<sup>y</sup> = η K−1 , where noise rate η<sup>x</sup> = η is a constant for any instance. 2) Asymmetric Noise: ηx,y = 1 − η<sup>y</sup> and P k̸=y ηx,k = ηy, where η<sup>x</sup> = η<sup>y</sup> denotes the noise rate for the instance of y-th class. 3) Instance-Dependent Noise: P ηx,y = 1 − η<sup>x</sup> and k̸=y ηx,k = ηx, where η<sup>x</sup> denotes the noise rate for the instance x. Herein, for asymmetric and instance-dependent noise, ηx,i is not necessarily equal to ηx,j for i ̸= j.

Risk Minimization and Noise-Tolerant Learning. In the case of clean labels, the expected risk [\[2\]](#page-8-17) for a given loss function L and prediction function f is defined as RL(f) = E(x,y)∼D[L(f(x), y)]. The goal of supervised learning is to find the expectation risk minimizer: f <sup>∗</sup> ∈ arg minf∈F RL(f). However, in the presence of noisy labels, we instead minimize the noisy risk, given by

$$
\mathcal{R}_L^{\eta}(f) = \mathbb{E}_{\mathcal{D}}[(1 - \eta_{\mathbf{x}})L(f(\mathbf{x}), y) + \sum_{k \neq y} \eta_{\mathbf{x},k}L(f(\mathbf{x}), k)],
$$
\n(2)

where the term \DOTSB \sum@ \slimits@ \_{k \neq y} \eta \_{\rvx ,k} L(f(\rvx ), k) represents the noisy component, which often poses challenges in training deep neural networks (DNNs). As discussed in [\[7\]](#page-8-5), a loss function L is said to be *noise-tolerant* if the global minimizer of the noisy risk, f ∗ <sup>η</sup> ∈ arg min<sup>f</sup> R η L (f), also minimizes the clean risk, i.e., f ∗ <sup>η</sup> ∈ arg min<sup>f</sup> RL(f).

## 4. Methodology

In this section, we first introduce the Active Passive Loss (APL) [\[18\]](#page-8-6) and Asymmetric Loss Functions (ALFs) [\[33,](#page-9-1) [35\]](#page-9-3), which are relevant to our work. We then present the proposed Asymmetric Mean Square Error (AMSE) and Joint Asymmetric Loss (JAL), followed by a rigorous theoretical analysis.

#### 4.1. Active Passive Loss

Previous works [\[7,](#page-8-5) [22\]](#page-8-8) theoretically proved that a loss function is noise-tolerant to symmetric and asymmetric label noise under some mild assumptions if it is symmetric.

Definition 4.1 (Symmetric Condition) *A loss function* L *is symmetric if it satisfies*

$$
\sum_{k=1}^{K} L(f(\mathbf{x}), k) = C,\tag{3}
$$

*where* C *is a constant and* k ∈ [K] *is the label corresponding to each class.*

Based on this, Ma et al. [\[18\]](#page-8-6) proposed the normalized loss functions, which normalize a loss function by:

$$
L_{\text{norm}} = \frac{L(f(\mathbf{x}), y)}{\sum_{k=1}^{K} L(f(\mathbf{x}), k)}.
$$
 (4)

This simple normalization operation can make any loss function symmetric, since we always have P<sup>K</sup> <sup>k</sup>=1 Lnorm(f(x), k) = 1. By normalizing Cross Entropy (CE) and Focal Loss (FL) [\[17\]](#page-8-18), Ma et al. [\[18\]](#page-8-6) proposed Normalized Cross Entropy (NCE) and Normalized Focal Loss (NFL). However, similar to symmetirc MAE, both NCE and NFL are challenging to optimize due to the overly strict symmetric condition. To address this issue, Ma et al. [\[18\]](#page-8-6) characterize existing loss functions into two types: *Active* and *Passive*. For a loss L(f(x), y) = P<sup>K</sup> <sup>k</sup>=1 ℓ(f(x)k, ek), where f(x)<sup>k</sup> is the k-th element of the prediction vector f(x) = p(·|x) and e<sup>k</sup> is the k-th element of the label e<sup>y</sup> (e.g., for CE loss, we have L(f(x), y) = P<sup>K</sup> <sup>k</sup>=1 −e<sup>k</sup> log f(x)k), we have the following definitions [\[18,](#page-8-6) [30\]](#page-9-2):

Definition 4.2 (Active Loss Function) L*active is an active loss function if* ∀(x, y) ∈ D, ∀k ̸= y, ℓ(f(x)k, ek) = 0*.*

Definition 4.3 (Passive Loss Function) L*passive is a passive loss function if* ∀(x, y) ∈ D, ∃k ̸= y, ℓ(f(x), ek) ̸= 0*.*

According to definitions, active loss functions only explicitly maximize classifier's output probability at the class position specified by the label y. In contrast, passive loss functions also explicitly minimize the probability at least one other class positions. The active loss functions include CE, FL, NCE/NFL [\[18\]](#page-8-6), while the passive loss functions include MAE, and NNCE/NNFL [\[30\]](#page-9-2) [1](#page-2-0) .

<span id="page-2-0"></span><sup>1</sup>The active and passive definitions and the type of loss functions reference [\[18,](#page-8-6) [30\]](#page-9-2).

<span id="page-3-2"></span>To address the underfitting issue of symmetric losses, Ma et al. [\[18\]](#page-8-6) proposed the Active Passive Loss (APL):

$$
L_{\text{APL}} = \alpha \cdot L_{\text{active}} + \beta \cdot L_{\text{passive}},\tag{5}
$$

where α, β > 0 are parameters. By combining the two different symmetric loss functions, APL can improve the fitting ability under the premise of ensuring robustness. Through combining active NCE/NFL and passive MAE, Ma et al. [\[18\]](#page-8-6) get one of the state-of-the-art methods.

Additionally, Ye et al. [\[30\]](#page-9-2) proposed new passive symmetric loss functions, known as Normalized Negative Loss Functions (NNCE/NNFL). By replacing the MAE in APL with NNCE/NNFL, they proposed a new method, named Active Negative Loss (ANL). However, both APL [\[18\]](#page-8-6) and ANL [\[30\]](#page-9-2) are limited to symmetric loss functions within the APL framework. To date, no research has explored the potential benefits of incorporating higher-performing asymmetric loss functions [\[33,](#page-9-1) [35\]](#page-9-3) into the APL framework.

#### 4.2. Asymmetric Loss Functions

Recently, Zhou et al. [\[33,](#page-9-1) [35\]](#page-9-3) proposed a new class of robust loss functions, called asymmetric loss functions.

Definition 4.4 (Asymmetric Condition) *On the given weights* w1, . . . , w<sup>K</sup> ≥ 0*, where* ∃t ∈ [K]*, s.t.,* w<sup>t</sup> > maxi̸=<sup>t</sup> w<sup>i</sup> *, a loss function* L *is called asymmetric if* L *satisfies*

$$
\arg\min_{f(\mathbf{x})} \sum_{k=1}^{K} w_k L(f(\mathbf{x}), k) = \arg\min_{f(\mathbf{x})} L(f(\mathbf{x}), t), \quad (6)
$$

*where we always have* arg minf(x) L(f(x), t) = et*.*

Zhou et al. [\[33,](#page-9-1) [35\]](#page-9-3) proved that asymmetric loss functions are noise-tolerant for clean-label-dominant noise, i.e., 1 − η<sup>x</sup> > maxk̸=<sup>y</sup> ηx,k, ∀x. However, existing asymmetric loss functions, such as Asymmetric Generalized Cross Entropy (AGCE) [\[33,](#page-9-1) [35\]](#page-9-3), are all active losses. This is because implementing the asymmetric condition in passive losses remains a challenging problem.

Irreplaceable of NCE/NFL. Although no passive asymmetric loss has been designed, can we replace the active NCE/NFL in the APL framework with an active asymmetric loss? To further explore this question, we conducted a series of experiments using active AGCE combined with passive MAE, as shown in Table [1.](#page-3-0) The results indicate that although AGCE+MAE adheres to the APL framework, it fails to achieve the desired effect. This suggests that simply replacing NCE with an asymmetric loss function within the APL framework does not lead to strong performance. Currently, all robust loss functions based on the APL framework rely on NCE or its variant, NFL, as active losses, highlighting their crucial role in implementing the APL framework. Therefore, the key challenge is to design an effective

<span id="page-3-0"></span>Table 1. Last epoch test accuracies (%) of different methods on CIFAR-10 with symmetric (η ∈ [0.4, 0.8]) and asymmetric (η ∈ [0.2, 0.4]) label noise. The results "mean±std" are reported over 3 random trials and the best results are in bold. † RCE actually equals a scaled MAE [\[24\]](#page-8-10). In order to be consistent with the original APL paper [\[18\]](#page-8-6), we still write RCE here.

|          |            | Symmetric  | Asymmetric |            |  |  |
|----------|------------|------------|------------|------------|--|--|
| CIFAR-10 | 0.4        | 0.8        | 0.2        | 0.4        |  |  |
| MAE      | 82.03±3.63 | 44.45±6.49 | 77.20±4.45 | 57.86±1.23 |  |  |
| NCE      | 69.37±0.22 | 41.20±1.25 | 72.20±0.38 | 65.33±0.40 |  |  |
| AGCE     | 83.39±0.17 | 44.42±0.74 | 86.67±0.14 | 60.91±0.20 |  |  |
| AGCE+MAE | 85.25±0.12 | 44.61±5.72 | 78.28±4.67 | 57.80±2.53 |  |  |
| NCE+RCE† | 85.89±0.31 | 54.99±2.13 | 88.62±0.29 | 77.94±0.21 |  |  |

passive asymmetric loss function that can be effectively integrated with NCE/NFL to further enhance the APL framework.

#### 4.3. Joint Asymmetric Loss

In this paper, we extend the asymmetric loss function to a more complex passive loss scenario and propose the Asymmetric Mean Square Error (AMSE), a new asymmetric and passive loss function. Then, we embed the proposed AMSE into the APL framework to build a better performance framework, which we call Joint Asymmetric Loss (JAL).

First, we introduce the proposed AMSE.

#### Asymmetric Mean Square Error (AMSE):

$$
L_{\text{AMSE}}(f(\mathbf{x}), y) = \frac{1}{K} ||a \cdot \mathbf{e}_y - f(\mathbf{x})||_2^2
$$
  
= 
$$
\sum_{k=1}^{K} \frac{1}{K} |a \cdot e_k - f(\mathbf{x})_k|^2,
$$
 (7)

where a ≥ 1 is a hyperparameter. AMSE is an extension of the MSE loss. If a = 1, this is the vanilla MSE loss.

In the following, we build the sufficient and necessary condition for AMSE to realize the asymmetric condition.

<span id="page-3-1"></span>Theorem 4.1 *On the given weights* w1, . . . , wK*, where* w<sup>m</sup> > wn*, and* w<sup>n</sup> = maxi̸=<sup>m</sup> w<sup>i</sup> *. The loss function* L(f(x), y) = <sup>1</sup> K ∥a · e<sup>y</sup> − f(x)∥ q <sup>q</sup> = P<sup>K</sup> k=1 1 K |a · e<sup>k</sup> − f(x)k| q *, where* q > 0 *and* a ≥ 1 *are parameters, is asymmetric if and only if* <sup>w</sup><sup>m</sup> w<sup>n</sup> ≥ a <sup>q</sup>−1+ P i̸=m wi wn (a−1)q−<sup>1</sup> · <sup>I</sup>(q > 1) + I(q ≤ 1)*.*

*Proof.* For the sake of brevity, we abbreviate f(x)<sup>k</sup> as f<sup>k</sup> in the proof.

If L(f(x), k) is asymmetric, for w<sup>m</sup> > w<sup>n</sup> ≥ 0, we have P<sup>K</sup> <sup>k</sup>=1 wkL(f(x), k) ≥ P<sup>K</sup> <sup>k</sup>=1 wkL(f ′ (x), k) ≥ P<sup>K</sup> <sup>k</sup>=1 wkL(em, k) always holds, where f ′ <sup>i</sup> = f<sup>i</sup> for i = m, n and f ′ <sup>i</sup> = 0 for i ̸= m, n. That is

wm[(a − fm) <sup>q</sup> + f q <sup>n</sup>] + wn[(a − fn) <sup>q</sup> + f q <sup>m</sup>] + P <sup>i</sup≯=m,n wi(a <sup>q</sup> + f q <sup>m</sup> + f q <sup>n</sup>) ≥ wm(a − 1)<sup>q</sup> + wn(a <sup>q</sup> + 1) + P <sup>i</sup≯=m,n wi(a <sup>q</sup> + 1).

<span id="page-4-1"></span>For wn = 0, the The inequality is trivial. For w<sup>n</sup> > 0, we have <sup>w</sup><sup>m</sup> wn ≥

$$
a^{q} + 1 - (a - f_{n})^{q} - f_{m}^{q} + \sum_{i \neq m,n} \frac{w_{i}}{w_{n}} (1 - f_{m}^{q} - f_{n}^{q})
$$
  
\nsup  
\n
$$
f_{m}, f_{n} \geq 0 \qquad (a - f_{m})^{q} + f_{n}^{q} - (a - 1)^{q}
$$
  
\n
$$
a^{q} + 1 - (a - 1 + x)^{q} - x^{q} + \sum_{i \neq m,n} \frac{w_{i}}{w_{n}} [1 - x^{q} - (1 - x)^{q}]
$$
  
\n
$$
\sup_{0 \leq x \leq 1} \frac{a - x^{q} + (1 - x)^{q} - (a - 1)^{q}}{(a - x)^{q} + (1 - x)^{q} - (a - 1)^{q}}
$$
  
\n
$$
\stackrel{\triangle}{=} \sup_{0 \leq x \leq 1} h(x).
$$

For 0 < q ≤ 1 and 0 ≤ x ≤ 1, because a <sup>q</sup> ≤ (a−1+x) <sup>q</sup> +(1−x) q and 1+(a−1)<sup>q</sup> ≤ x <sup>q</sup>+(a−x) q , we have <sup>a</sup> <sup>q</sup>+1−(a−1+x) <sup>q</sup>−x q (a−x) q+(1−x) <sup>q</sup>−(a−1)<sup>q</sup> ≤ 1. Since P i̸=m,n wi wn [1 − x <sup>q</sup> − (1 − x) q ] ≤ 0, we have sup 0≤x≤1 h(x) = 1.

For q > 1, we have that sup0≤x≤<sup>1</sup> h(x) is equal to

$$
\sup_{x \le \xi \le 1} \frac{(a-1+\xi)^{q-1} + \xi^{q-1} + \sum_{i \ne m,n} \frac{w_i}{w_n} [\xi^{q-1} - (1-\xi)^{q-1}]}{(a-\xi)^{q-1} + (1-\xi)^{q-1}}
$$
$$
\stackrel{\triangle}{=} \sup_{x \le \xi \le 1} \rho(\xi) = \lim_{\xi \to 1} \rho(\xi) = \frac{a^{q-1} + \sum_{i \ne m} \frac{w_i}{w_n}}{(a-1)^{q-1}},
$$

where the first line follows from Cauchy's Mean Value Theorem.

On the other hand, if <sup>w</sup><sup>m</sup> wn ≥ a <sup>q</sup>−1+ P i̸=m wi wn (a−1)q−1 ·I(q > 1)+I(q ≤ 1). We reset f ′<sup>m</sup> = f<sup>m</sup> + fn, f′ <sup>n</sup> = 0, and f ′ <sup>i</sup> = f<sup>i</sup> for i ̸= m, n. We abbreviate f<sup>m</sup> + f<sup>k</sup> as fm&<sup>k</sup> for concision . Then for any k ̸= m, we have

$$
\frac{w_m}{w_k} \ge \frac{a^{q-1} + \sum_{i \ne m} \frac{w_i}{w_k}}{(a-1)^{q-1}} \cdot \mathbb{I}(q > 1) + \mathbb{I}(q \le 1) \Leftrightarrow \frac{w_m}{w_k} \ge \sup_{\substack{f_m, f_k \ge 0 \\ f_m \le k \le 1}} \frac{a^q + (f_{m\&k})^q - (a - f_k)^q - f_m^q + \sum_{i \ne m, k} \frac{w_i}{w_k} [(f_{m\&k})^q - f_m^q - f_k^q]}{(a - f_m)^q + f_k^q - (a - f_{m\&k})^q}
$$
\n
$$
\Rightarrow \sum_{k=1}^K w_k L(f(\mathbf{x}), k) \ge \sum_{k=1}^K w_k L(f'(\mathbf{x}), k),
$$

According to Lemma 1 in [\[33\]](#page-9-1), L is asymmetric. *End Proof.*

As shown in Theorem [4.1,](#page-3-1) we consider not only the case where q = 2, but also other cases. To maintain consistency with MSE and simplify the loss function, we only use q = 2 in the main paper. The analysis of different values of q can be found in the supplementary materials. Theorem [4.1](#page-3-1) demonstrates that by adjusting a parameter a, AMSE, which is a passive loss, can satisfy the asymmetric condition and subsequently become noise-tolerant. For example, considering a 10-class dataset with 0.8 symmetric noise, we require <sup>w</sup><sup>m</sup> w<sup>n</sup> = 0.2 <sup>0</sup>.8/<sup>9</sup> ≥ a+9 a−1 , i.e., a ≥ 9.

Parameter and Performance Analysis for AMSE. To demonstrate the superiority of the proposed AMSE, we compare it with the latest state-of-the-art passive loss, NNCE [\[30\]](#page-9-2), on CIFAR-10. Our analysis suggests that for CIFAR-10 with 0.8 symmetric noise, a should be ≥ 9. Therefore, we selected a ∈ [10, 20, 30, 40] for our experiments, as shown in Figure [4.](#page-10-0) As illustrated, larger values of a impose tighter constraints, making a = 20, 30, 40 more

![](_page_4_Figure_11.jpeg)

Figure 2. Test accuracies on CIFAR-10 with 0.8 symmetric and 0.4 asymmetric noise.

<span id="page-4-0"></span>Table 2. Last epoch test accuracies (%) of different methods on CIFAR-10 with symmetric (η ∈ [0.4, 0.8]) and asymmetric (η ∈ [0.2, 0.4]) label noise. The results "mean±std" are reported over 3 random trials and the best results are in bold.

|          | Symmetric  |            | Asymmetric |            |  |
|----------|------------|------------|------------|------------|--|
| CIFAR-10 | 0.4        | 0.8        | 0.2        | 0.4        |  |
| NCE      | 69.37±0.22 | 41.20±1.25 | 72.20±0.38 | 65.33±0.40 |  |
| AMSE     | 87.54±0.26 | 64.97±0.87 | 83.88±5.07 | 58.07±2.21 |  |
| JAL-CE   | 87.53±0.10 | 65.43±0.99 | 89.11±0.38 | 79.54±0.34 |  |

robust than a = 10 under 0.8 symmetric noise. However, excessively strict constraints may reduce the model's fitting ability, particularly under asymmetric noise. Therefore, selecting a moderate a is recommended to achieve both robust and sufficient learning. Overall, AMSE significantly outperforms NNCE in both symmetric and asymmetric noise, further demonstrating its effectiveness.

Joint Asymmetric Loss. We now integrate the proposed AMSE into the APL framework to enhance its performance, resulting in a novel approach called Joint Asymmetric Loss (JAL). Specifically, we introduce two joint asymmetric losses, as described in the following.

Base on Cross Entropy (CE), we have JAL-CE:

$$
L_{\text{JAL-CE}} = \alpha \cdot L_{\text{NCE}} + \beta \cdot L_{\text{AMSE}}.\tag{8}
$$

Base on Focal Loss (FL), we have JAL-FL:

$$
L_{\text{JAL-FL}} = \alpha \cdot L_{\text{NFL}} + \beta \cdot L_{\text{AMSE}}.\tag{9}
$$

We can easily prove that JAL remains noise-tolerant. Zhou et al. [\[33\]](#page-9-1) demonstrated that symmetric loss functions are completely asymmetric and that the combination of asymmetric loss functions remains asymmetric. Since NCE/NFL are symmetric (and therefore also asymmetric), and we have already proven that AMSE is asymmetric, it follows that JAL is also asymmetric and thus noise-tolerant.

<span id="page-5-1"></span><span id="page-5-0"></span>Table 3. Last epoch test accuracies (%) of different methods on CIFAR-10 and CIFAR-100 with clean, symmetric (η ∈ [0.2, 0.4, 0.6, 0.8]), and asymmetric (η ∈ [0.1, 0.2, 0.3, 0.4]) label noise. The results (mean±std) are reported over 3 random trials and the top-2 best results are in bold.

|           |            | Symmetric  |            |            |            | Asymmetric |            |            |            |
|-----------|------------|------------|------------|------------|------------|------------|------------|------------|------------|
| CIFAR-10  | Clean      | 0.2        | 0.4        | 0.6        | 0.8        | 0.1        | 0.2        | 0.3        | 0.4        |
| CE        | 90.50±0.22 | 75.21±0.39 | 58.05±0.53 | 38.80±0.45 | 19.74±0.40 | 86.85±0.15 | 83.05±0.35 | 78.37±0.61 | 73.85±0.07 |
| FL        | 89.70±0.24 | 74.50±0.18 | 58.23±0.40 | 38.69±0.06 | 19.47±0.74 | 86.64±0.12 | 83.08±0.07 | 79.34±0.30 | 74.68±0.31 |
| GCE       | 89.36±0.19 | 89.36±0.19 | 82.19±0.84 | 68.01±0.40 | 46.61±0.39 | 88.41±0.20 | 85.72±0.22 | 79.49±0.20 | 73.36±0.53 |
| SCE       | 91.51±0.24 | 87.65±0.36 | 79.73±0.29 | 61.79±0.72 | 28.01±0.92 | 89.54±0.33 | 85.94±0.38 | 80.50±0.09 | 74.33±0.56 |
| NCE       | 75.48±0.37 | 73.22±0.35 | 69.37±0.22 | 62.47±0.85 | 41.20±1.25 | 74.11±0.24 | 72.20±0.38 | 70.14±0.27 | 65.33±0.40 |
| NCE+RCE   | 90.80±0.06 | 88.93±0.04 | 85.89±0.31 | 79.89±0.25 | 54.99±2.13 | 90.04±0.17 | 88.62±0.29 | 85.07±0.27 | 77.94±0.21 |
| NCE+AUL   | 91.17±0.18 | 89.00±0.58 | 86.05±0.30 | 79.22±0.22 | 56.24±0.94 | 90.06±0.16 | 88.19±0.07 | 84.83±0.47 | 77.60±0.16 |
| NCE+AGCE  | 91.01±0.20 | 88.91±0.38 | 86.16±0.38 | 79.93±0.33 | 43.82±1.91 | 90.29±0.05 | 88.49±0.28 | 85.21±0.59 | 78.47±1.05 |
| CE+LC     | 90.09±0.13 | 83.87±0.27 | 70.36±0.23 | 46.53±0.29 | 19.74±1.77 | 87.74±0.23 | 83.16±0.33 | 78.48±0.25 | 73.32±0.78 |
| ANL-CE    | 91.74±0.18 | 89.68±0.29 | 87.16±0.16 | 81.28±0.63 | 62.28±1.10 | 90.66±0.16 | 89.09±0.21 | 85.49±0.49 | 77.99±0.40 |
| ANL-FL    | 91.58±0.19 | 89.93±0.03 | 86.94±0.03 | 81.10±0.30 | 61.89±2.25 | 90.72±0.20 | 89.29±0.02 | 85.80±0.38 | 77.89±0.28 |
| LT-APL    | -          | 89.42±0.13 | 86.82±0.18 | 80.93±0.30 | 40.87±1.57 | -          | 89.28±0.24 | 86.29±0.36 | 79.99±0.58 |
| JAL-CE    | 91.63±0.21 | 89.95±0.22 | 87.53±0.10 | 82.03±0.18 | 65.43±0.99 | 90.70±0.21 | 89.11±0.38 | 86.38±0.14 | 79.54±0.34 |
| JAL-FL    | 91.56±0.25 | 89.99±0.11 | 87.43±0.29 | 82.09±0.08 | 64.84±1.13 | 90.77±0.16 | 89.36±0.27 | 86.18±0.04 | 79.51±0.06 |
|           |            | Symmetric  |            |            | Asymmetric |            |            |            |            |
|           |            |            |            |            |            |            |            |            |            |
| CIFAR-100 | Clean      | 0.2        | 0.4        | 0.6        | 0.8        | 0.1        | 0.2        | 0.3        | 0.4        |
| CE        | 70.93±0.77 | 56.47±1.34 | 39.68±0.77 | 22.64±0.53 | 7.82±0.33  | 64.14±1.01 | 58.67±0.45 | 50.44±1.16 | 41.51±0.12 |
| FL        | 70.58±0.34 | 56.32±1.43 | 40.83±0.52 | 22.44±0.54 | 7.68±0.37  | 65.00±0.46 | 58.12±0.44 | 51.16±1.32 | 41.46±0.38 |
| GCE       | 61.73±1.30 | 60.58±2.51 | 57.35±0.91 | 46.15±1.10 | 20.33±0.31 | 62.01±1.11 | 59.19±1.36 | 53.35±0.65 | 40.92±0.21 |
| SCE       | 70.57±0.93 | 55.50±0.35 | 40.13±1.48 | 22.23±1.29 | 7.84±0.56  | 64.51±0.45 | 57.84±0.57 | 49.66±0.48 | 41.58±0.87 |
| NCE       | 29.95±0.56 | 25.43±0.91 | 20.26±0.25 | 14.66±1.04 | 8.82±0.47  | 27.16±1.01 | 26.67±0.73 | 23.83±0.29 | 20.83±1.08 |
| NCE+RCE   | 68.07±0.70 | 64.57±0.16 | 58.48±0.51 | 46.73±1.00 | 26.94±1.29 | 66.74±0.30 | 62.82±0.57 | 55.86±0.40 | 41.50±0.39 |
| NCE+AUL   | 69.95±0.33 | 65.45±0.49 | 56.37±0.12 | 38.68±0.75 | 12.95±0.37 | 66.41±0.15 | 57.39±0.34 | 48.20±0.19 | 38.41±0.52 |
| NCE+AGCE  | 69.05±0.36 | 65.61±0.27 | 59.40±0.34 | 47.66±0.49 | 26.14±0.01 | 66.96±0.45 | 64.08±0.44 | 57.17±0.33 | 44.62±1.04 |
| CE+LC     | 71.80±0.34 | 56.26±0.09 | 37.36±0.49 | 17.46±0.62 | 6.32±0.16  | 63.51±0.27 | 56.19±0.30 | 48.07±0.38 | 39.64±0.14 |
| ANL-CE    | 70.26±0.15 | 66.93±0.09 | 61.58±0.33 | 52.09±0.58 | 28.01±1.06 | 68.60±0.41 | 65.96±0.18 | 60.57±0.07 | 45.73±0.74 |
| ANL-FL    | 70.11±0.27 | 67.03±0.46 | 61.89±0.25 | 51.58±0.33 | 28.81±0.74 | 68.67±0.21 | 66.12±0.39 | 60.03±0.48 | 46.20±0.45 |
| LT-APL    | -          | 63.29±0.49 | 54.70±1.73 | 40.52±1.65 | 22.63±0.78 | -          | 62.59±1.31 | 56.90±1.29 | 44.05±1.32 |

Robust and Sufficient learning of JAL. To evaluate the effectiveness of our proposed JAL framework in improving performance, we conducted ablation experiments on CIFAR-10 using NCE, AMSE (a = 30), and JAL-CE (α = 1, β = 1, a = 30), as shown in Table [2.](#page-4-0) The results indicate that under symmetric noise, JAL-CE performs similarly to AMSE, with both achieving strong performance. This highlight the effectiveness of the the AMSE component. In addition, JAL framework can effectively alleviate parameter sensitivity to noise rates and types. Under asymmetric noise, NCE and AMSE exhibit signs of underfitting, whereas JAL-CE maintains a strong fitting ability. These findings demonstrate that JAL offers both robustness and superior fitting ability in label noise scenarios.

## 5. Experiments

In this section, we provide extensive experiments to evaluate the effectiveness of our method on various datasets, including CIFAR-10/CIFAR-100 [\[13\]](#page-8-19), CIFAR-10N/CIFAR-100N [\[26\]](#page-9-8), WebVision [\[16\]](#page-8-20), ILSVRC12 [\[4\]](#page-8-21), and Clothing1M [\[29\]](#page-9-9). Detailed experiment settings can be found in the supplementary materials.

#### 5.1. Evaluation on Benchmark Datasets

Baselines. We experiment with various state-of-the-art methods, including Cross Entropy (CE); Focal Loss (FL)

| Loss     |            | CIFAR-10 IDN |            | CIFAR-100 IDN |            |            |  |
|----------|------------|--------------|------------|---------------|------------|------------|--|
|          | 0.2        | 0.4          | 0.6        | 0.2           | 0.4        | 0.6        |  |
| CE       | 75.38±0.19 | 57.63±0.27   | 37.97±0.36 | 57.02±0.54    | 40.91±2.05 | 24.49±0.86 |  |
| GCE      | 86.66±0.14 | 79.99±0.23   | 51.90±0.13 | 61.43±2.24    | 57.07±1.04 | 42.40±0.52 |  |
| SCE      | 86.65±0.27 | 74.54±0.34   | 49.83±0.40 | 56.32±0.27    | 39.82±1.43 | 23.19±0.87 |  |
| NCE+RCE  | 89.06±0.26 | 85.11±0.28   | 71.27±0.66 | 64.33±0.46    | 57.53±0.84 | 40.36±0.35 |  |
| NCE+AGCE | 88.95±0.07 | 85.30±0.23   | 71.49±0.34 | 65.18±0.17    | 57.89±0.57 | 43.04±0.29 |  |
| CE+LC    | 82.77±0.09 | 68.06±0.22   | 43.60±0.39 | 55.93±0.39    | 37.74±0.63 | 18.68±0.50 |  |
| ANL-CE   | 89.71±0.35 | 85.74±0.15   | 69.83±0.38 | 66.89±0.53    | 60.88±0.35 | 48.12±0.48 |  |
| ANL-FL   | 89.68±0.21 | 85.97±0.16   | 70.70±0.30 | 67.17±0.11    | 61.07±0.38 | 46.77±0.80 |  |
| JAL-CE   | 90.01±0.12 | 86.46±0.15   | 75.62±0.18 | 67.51±0.29    | 63.24±0.16 | 51.69±0.68 |  |
| JAL-FL   | 89.90±0.14 | 86.78±0.17   | 75.02±0.48 | 67.77±0.38    | 63.56±0.18 | 51.69±0.59 |  |

<span id="page-6-2"></span><span id="page-6-0"></span>Table 4. Last epoch test accuracies (%) of different methods on CIFAR-10 and CIFAR-100 with instance-dependent noise (IDN) (η ∈ [0.2, 0.4, 0.6]). The results "mean±std" are reported over 3 random trials and the top-2 best results are in bold.

<span id="page-6-1"></span>Table 5. Last epoch test accuracies (%) of different methods on CIFAR-10N and CIFAR-100N human-annotated noise [\[26\]](#page-9-8). The results "mean±std" are reported over 3 random trials and the top-2 best results are in bold.

|          |            | CIFAR-100                      |            |            |
|----------|------------|--------------------------------|------------|------------|
| Loss     |            | CIFAR-10<br>Aggregate Random 1 | Worst      | Noisy      |
| CE       | 85.09±0.30 | 79.09±0.28                     | 61.43±0.52 | 48.63±0.53 |
| GCE      | 87.39±0.09 | 85.98±0.42                     | 77.77±0.59 | 50.97±0.60 |
| SCE      | 88.48±0.26 | 85.65±0.30                     | 73.65±0.29 | 48.52±0.11 |
| NCE+RCE  | 89.17±0.28 | 87.62±0.34                     | 79.74±0.09 | 54.27±0.09 |
| NCE+AGCE | 89.27±0.28 | 87.92±0.02                     | 79.91±0.37 | 55.96±0.20 |
| CE+LC    | 86.60±0.40 | 83.51±0.13                     | 70.11±0.10 | 47.76±0.29 |
| ANL-CE   | 89.66±0.12 | 88.68±0.13                     | 80.23±0.28 | 56.37±0.42 |
| ANL-FL   | 89.81±0.08 | 88.57±0.18                     | 80.56±0.23 | 57.09±0.40 |
| JAL-CE   | 89.94±0.20 | 88.85±0.23                     | 81.33±0.34 | 59.54±0.12 |
| JAL-FL   | 90.06±0.22 | 88.71±0.30                     | 81.25±0.10 | 59.38±0.24 |

[\[17\]](#page-8-18), Generalized Cross Entropy (GCE) [\[32\]](#page-9-4); Symmetric Cross Entropy (SCE) [\[24\]](#page-8-10); Active Passive Loss (APL) [\[18\]](#page-8-6), including NCE and NCE+RCE [\[18\]](#page-8-6); Asymmetric Loss Functions (ALFs) [\[33,](#page-9-1) [35\]](#page-9-3), including NCE+AUL and NCE+AGCE; LogitClip (CE+LC) [\[25\]](#page-8-16); Active Negative Loss (ANL) [\[30\]](#page-9-2), including ANL-CE and ANL-FL. Student loss (LT-APL) [\[31\]](#page-9-0). We follow the same experimental settings in [\[18,](#page-8-6) [30,](#page-9-2) [33\]](#page-9-1): An 8-layer CNN [\[14\]](#page-8-22) is used for CIFAR-10 and a ResNet-34 [\[9\]](#page-8-23) for CIFAR-100.

Results. We evaluate the test accuracy of various methods under different types of label noise, including symmetric, asymmetric, instance-dependent, and human-annotated noise. The experimental results for symmetric and asymmetric noise are presented in Table [3.](#page-5-0) As shown, our proposed JAL-CE and JAL-FL demonstrate exceptional performance, consistently ranking among the top-2 in most scenarios. On CIFAR-10, under the most challenging 0.8 symmetric noise, JAL achieves an improvement in accuracy of about 3%. For the more complex CIFAR-100, our method significantly outperforms previous state-of-the-art methods in most cases. In particular, on CIFAR-100, our method improves accuracy by 10% under 0.4 asymmetric noise.

The experimental results for instance-dependent noise (IDN) are presented in Table [4.](#page-6-0) As can be seen, our JAL-CE and JAL-FL consistently achieve top-2 performance across all cases, with a 3~4% increase in accuracy under 0.6 instance-dependent noise on both CIFAR-10 and CIFAR-100 compared to previous state-of-the-art methods, such as NCE+RCE, NCE+AGCE, and ANL.

Furthermore, we conduct experiments on humanannotated label noise using the CIFAR-10N and CIFAR-100N datasets [\[26\]](#page-9-8), as shown in Table [5.](#page-6-1) As can be seen, our method achieves top-2 performance across all humanannotated cases, especially for the most difficult CIFAR-10 worst and CIFAR-100 noisy cases, highlighting the excellent performance of our method in practical scenarios. These results demonstrate that our method significantly surpasses the latest benchmarks.

Comparison with Previous Asymmetric Loss Functions. Previous asymmetric loss functions have also been combined with NCE to enhance performance, such as NCE+AUL and NCE+AGCE. However, since both NCE and earlier asymmetric loss functions act as active losses, they do not form the APL framework. As a result, only limited improvements can be gained, which explains why our JAL method outperforms previous asymmetric loss approaches.

Histogram Visualization. To further assess the robustness of our JAL method compared to vanilla CE, we visualize the prediction probability distributions on the training set for models trained on CIFAR-10 with 0.4 symmetric noise, as illustrated in Figure [3.](#page-7-0) The results reveal that while CE

<span id="page-7-2"></span><span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)

Figure 3. Histograms of the distribution of samples with different prediction probabilities in the training set for CIFAR-10 with 0.4 symmetric noise.

<span id="page-7-1"></span>Table 6. Last epoch test accuracies (%) of different methods on ILSVRC12, WebVision, and Clothing1M. The top-2 best results are in bold.

| Loss<br>CE          | GCE   | SCE   | NCE+RCE | NCE+AGCE | ANL-CE | ANL-FL | JAL-CE | JAL-FL |
|---------------------|-------|-------|---------|----------|--------|--------|--------|--------|
| WebVision<br>66.28  | 61.84 | 65.16 | 66.96   | 67.16    | 67.36  | 67.76  | 69.84  | 69.20  |
| ILSVRC12<br>60.68   | 60.32 | 61.00 | 63.96   | 64.36    | 65.60  | 64.84  | 66.64  | 66.00  |
| Clothing1M<br>67.93 | 68.46 | 67.71 | 69.24   | 67.90    | 69.75  | 69.90  | 70.31  | 70.11  |

initially fits clean labels in the early training stages, it progressively overfits to noisy labels as training continues. In contrast, JAL demonstrates superior robustness by predominantly focusing on clean labels while effectively avoiding fitting to noisy labels throughout all the training process.

#### 5.2. Evaluation on Real-World Datasets

We perform experiments on large-scale real-world datasets, including WebVision [\[16\]](#page-8-20), ILSVRC12 [\[4\]](#page-8-21), and Clothing1M [\[29\]](#page-9-9). For WebVision, we follow the mini setting in [\[10\]](#page-8-24) that takes the first 50 classes in the google image subset. We train a ResNet-50 [\[9\]](#page-8-23) and evaluate the trained network on the same 50 classes of ILSVRC12 and WebVision validation set. For Clothing1M, we use a ResNet-50 pre-trained on ImageNet similar to [\[29\]](#page-9-9). We train the model on the noisy training set with a million samples and subsequently evaluate it on the test set.

Results. In Table [6,](#page-7-1) we present the test accuracies achieved by various robust loss functions on ILSVRC12, WebVision, and Clothing1M. Notably, our JAL-CE and JAL-FL outperform other state-of-the-art methods, achieving the highest accuracy across all real-world datasets. These results highlight the robustness and effectiveness of JAL in practical applications.

# 6. Conclusion

In this paper, we expand the research of asymmetric loss functions, and realize a more complex passive asymmetric loss function. Specifically, we introduce the *Asymmetric Mean Square Error* (AMSE), the first passive asymmetric loss function. We rigorously establish the necessary and sufficient condition for AMSE to satisfy the asymmetric condition. By replacing the traditional passive symmetric loss in APL with our AMSE, we propose the *Joint Asymmetric Loss* (JAL), a novel robust loss framework with better fitting ability. Our theoretically guaranteed method has shown positive results in mitigating label noise. We hope AMSE and JAL will be useful with other methods and tasks that involve noise-tolerant learning.

## Acknowledgements

This work was supported in part by National Natural Science Foundation of China under Grants 632B2031 and 92270116, in part by National Key Research and Development Program of China under Grant 2023YFC2509100, and in part by HIT-XNJKKGYDJ2024014.

## References

- <span id="page-8-3"></span>[1] Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In *International conference on machine learning*, pages 233– 242. PMLR, 2017. [1](#page-0-0)
- <span id="page-8-17"></span>[2] Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds. *Journal of the American Statistical Association*, 101(473):138–156, 2006. [3](#page-2-1)
- <span id="page-8-4"></span>[3] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. *arXiv preprint arXiv:2312.09390*, 2023. [1](#page-0-0)
- <span id="page-8-21"></span>[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE Conference on Computer Vision and Pattern Recognition*, pages 248–255, 2009. [6,](#page-5-1) [8](#page-7-2)
- <span id="page-8-12"></span>[5] Erik Englesson and Hossein Azizpour. Generalized jensenshannon divergence loss for learning with noisy labels. *Advances in Neural Information Processing Systems*, 34: 30284–30297, 2021. [2](#page-1-0)
- <span id="page-8-11"></span>[6] Lei Feng, Senlin Shu, Zhuoyi Lin, Fengmao Lv, Li Li, and Bo An. Can cross entropy loss be robust to label noise? In *Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence*, pages 2206–2212, 2021. [2](#page-1-0)
- <span id="page-8-5"></span>[7] Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise for deep neural networks. In *Proceedings of the AAAI conference on artificial intelligence*, 2017. [1,](#page-0-0) [2,](#page-1-0) [3](#page-2-1)
- <span id="page-8-0"></span>[8] Bo Han, Quanming Yao, Tongliang Liu, Gang Niu, Ivor W Tsang, James T Kwok, and Masashi Sugiyama. A survey of label-noise representation learning: Past, present and future. *arXiv preprint arXiv:2011.04406*, 2020. [1](#page-0-0)
- <span id="page-8-23"></span>[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770–778, 2016. [7,](#page-6-2) [8,](#page-7-2) [12](#page-11-0)
- <span id="page-8-24"></span>[10] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In *International conference on machine learning*, pages 2304–2313. PMLR, 2018. [8,](#page-7-2) [12](#page-11-0)
- <span id="page-8-9"></span>[11] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In *Proceedings of*

*the IEEE/CVF international conference on computer vision*, pages 101–110, 2019. [2](#page-1-0)

- <span id="page-8-14"></span>[12] Youngdong Kim, Juseung Yun, Hyounguk Shon, and Junmo Kim. Joint negative and positive learning for noisy labels. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 9442–9451, 2021. [2](#page-1-0)
- <span id="page-8-19"></span>[13] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [6](#page-5-1)
- <span id="page-8-22"></span>[14] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. *Neural computation*, 1(4):541–551, 1989. [7,](#page-6-2) [12](#page-11-0)
- <span id="page-8-1"></span>[15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. *nature*, 521(7553):436–444, 2015. [1](#page-0-0)
- <span id="page-8-20"></span>[16] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning and understanding from web data. *arXiv preprint arXiv:1708.02862*, 2017. [6,](#page-5-1) [8](#page-7-2)
- <span id="page-8-18"></span>[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In *Proceedings of the IEEE international conference on computer vision*, pages 2980–2988, 2017. [3,](#page-2-1) [7](#page-6-2)
- <span id="page-8-6"></span>[18] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In *International conference on machine learning*, pages 6543–6553. PMLR, 2020. [1,](#page-0-0) [2,](#page-1-0) [3,](#page-2-1) [4,](#page-3-2) [7,](#page-6-2) [11,](#page-10-1) [12](#page-11-0)
- <span id="page-8-7"></span>[19] Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. *IEEE transactions on cybernetics*, 43(3): 1146–1151, 2013. [1,](#page-0-0) [2](#page-1-0)
- <span id="page-8-15"></span>[20] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient clipping mitigate label noise? In *International conference on learning representations*, 2020. [2](#page-1-0)
- <span id="page-8-2"></span>[21] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. *Journal of machine learning research*, 9 (11), 2008. [1](#page-0-0)
- <span id="page-8-8"></span>[22] Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label noise: The importance of being unhinged. *Advances in neural information processing systems*, 28, 2015. [1,](#page-0-0) [2,](#page-1-0) [3](#page-2-1)
- <span id="page-8-13"></span>[23] Jialiang Wang, Xiong Zhou, Deming Zhai, Junjun Jiang, Xiangyang Ji, and Xianming Liu. ϵ-softmax: Approximating one-hot vectors for mitigating label noise. In *The Thirtyeighth Annual Conference on Neural Information Processing Systems*, 2024. [2](#page-1-0)
- <span id="page-8-10"></span>[24] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 322–330, 2019. [2,](#page-1-0) [4,](#page-3-2) [7,](#page-6-2) [11](#page-10-1)
- <span id="page-8-16"></span>[25] Hongxin Wei, Huiping Zhuang, Renchunzi Xie, Lei Feng, Gang Niu, Bo An, and Yixuan Li. Mitigating memorization of noisy labels by clipping the model prediction. In *International Conference on Machine Learning*, pages 36868– 36886. PMLR, 2023. [2,](#page-1-0) [7](#page-6-2)

- <span id="page-9-8"></span>[26] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. In *International Conference on Learning Representations*, 2021. [6,](#page-5-1) [7](#page-6-2)
- <span id="page-9-6"></span>[27] Jiaheng Wei, Hangyu Liu, Tongliang Liu, Gang Niu, Masashi Sugiyama, and Yang Liu. To smooth or not? when label smoothing meets noisy labels. In *International Conference on Machine Learning*, pages 23589–23614. PMLR, 2022. [2](#page-1-0)
- <span id="page-9-7"></span>[28] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. *Advances in Neural Information Processing Systems*, 33:7597–7610, 2020. [3,](#page-2-1) [12](#page-11-0)
- <span id="page-9-9"></span>[29] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 2691–2699, 2015. [6,](#page-5-1) [8,](#page-7-2) [12](#page-11-0)
- <span id="page-9-2"></span>[30] Xichen Ye, Xiaoqiang Li, Songmin Dai, Tong Liu, Yan Sun, and Weiqin Tong. Active negative loss functions for learning with noisy labels. In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023. [2,](#page-1-0) [3,](#page-2-1) [4,](#page-3-2) [5,](#page-4-1) [7,](#page-6-2) [12](#page-11-0)
- <span id="page-9-0"></span>[31] Shuo Zhang, Jian-Qing Li, Hamido Fujita, Yu-Wen Li, Deng-Bao Wang, Ting-Ting Zhu, Min-Ling Zhang, and Cheng-Yu Liu. Student loss: Towards the probability assumption in inaccurate supervision. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 46(6):4460– 4475, 2024. [1,](#page-0-0) [7,](#page-6-2) [12](#page-11-0)
- <span id="page-9-4"></span>[32] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. *Advances in neural information processing systems*, 31, 2018. [2,](#page-1-0) [7](#page-6-2)
- <span id="page-9-1"></span>[33] Xiong Zhou, Xianming Liu, Junjun Jiang, Xin Gao, and Xiangyang Ji. Asymmetric loss functions for learning with noisy labels. In *International conference on machine learning*, pages 12846–12856. PMLR, 2021. [1,](#page-0-0) [2,](#page-1-0) [3,](#page-2-1) [4,](#page-3-2) [5,](#page-4-1) [7,](#page-6-2) [11,](#page-10-1) [12](#page-11-0)
- <span id="page-9-5"></span>[34] Xiong Zhou, Xianming Liu, Chenyang Wang, Deming Zhai, Junjun Jiang, and Xiangyang Ji. Learning with noisy labels via sparse regularization. In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 72–81, 2021. [2](#page-1-0)
- <span id="page-9-3"></span>[35] Xiong Zhou, Xianming Liu, Deming Zhai, Junjun Jiang, and Xiangyang Ji. Asymmetric loss functions for noise-tolerant learning: Theory and applications. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2023. [2,](#page-1-0) [3,](#page-2-1) [4,](#page-3-2) [7](#page-6-2)

# Joint Asymmetric Loss for Learning with Noisy Labels Supplementary Materials

## <span id="page-10-1"></span>A. More Results

More Ablation Experiments about AMSE. We present the ablation experiments for different q and a in Figure [4.](#page-10-0) As illustrated: 1) For q = 1, the asymmetric condition always holds. In this case, a is a constant with zero gradient, making different choices of a equivalent. The loss is difficult to optimize, similar to MAE. 2) For q = 2, the asymmetric condition holds when a ≥ 9. For the gradient, we have ∂L(f(x),y) ∂f(x)<sup>y</sup> = − 2 K (a−f(x)y), and a does not affect ∂L(f(x),y) ∂f(x)k̸=<sup>y</sup> . As a increases, the weight of high-confidence (clean) samples in the gradient increases, while the weight of low-confidence (noisy) samples decreases. This explains why a larger a leads to better robustness. 3) For q = 3, the condition holds when a ≥ 4.73. The performance of the loss is similar to q = 2, but it is more sensitive to the hyperparameter, as higher powers amplify the loss error. Therefore, using q = 2 is an appropriate choice.

<span id="page-10-0"></span>![](_page_10_Figure_3.jpeg)

Figure 4. Ablation experiments for AMSE on CIFAR-10 with 0.8 symmetric noise.

More Results for AGCE+MAE. For the experiment for AGCE+MAE, we use the same a = 6, q = 1.5 in [\[33\]](#page-9-1), and search for α, β ∈ [1, 10]. The complete results are presented in Table [7,](#page-10-2) while the results for α = 1, β = 1 are shown in the main paper.

|                           | Symmetric  |            | Asymmetric |            |
|---------------------------|------------|------------|------------|------------|
| CIFAR-10                  | 0.4        | 0.8        | 0.2        | 0.4        |
| MAE                       | 82.03±3.63 | 44.45±6.49 | 77.20±4.45 | 57.86±1.23 |
| NCE                       | 69.37±0.22 | 41.20±1.25 | 72.20±0.38 | 65.33±0.40 |
| AGCE                      | 83.39±0.17 | 44.42±0.74 | 86.67±0.14 | 60.91±0.20 |
| AGCE+MAE (α = 1, β = 1)   | 85.25±0.12 | 44.61±5.72 | 78.28±4.67 | 57.80±2.53 |
| AGCE+MAE (α = 1, β = 10)  | 85.86±0.11 | 39.44±0.71 | 77.64±3.75 | 56.50±0.41 |
| AGCE+MAE (α = 10, β = 1)  | 85.71±0.29 | 23.36±2.85 | 75.43±4.16 | 57.55±1.83 |
| AGCE+MAE (α = 10, β = 10) | 85.85±0.55 | 21.83±1.47 | 78.92±4.59 | 56.49±0.50 |
| NCE+RCE†                  | 85.89±0.31 | 54.99±2.13 | 88.62±0.29 | 77.94±0.21 |

<span id="page-10-2"></span>Table 7. Last epoch test accuracies (%) of different methods on CIFAR-10 with symmetric (η ∈ [0.4, 0.8]) and asymmetric (η ∈ [0.2, 0.4]) label noise. The results "mean±std" are reported over 3 random trials and the best results are in bold. † RCE actually equals a scaled MAE [\[24\]](#page-8-10). In order to be consistent with the original APL paper [\[18\]](#page-8-6), we still write RCE here.

# <span id="page-11-0"></span>B. Experiments

## B.1. Evaluation on Benchmark Datasets

Noise Generation. We follow the approach of the previous work [\[30\]](#page-9-2) to experiment with two types of synthetic label noise: symmetric noise and asymmetric noise. In the case of symmetric label noise, we intentionally corrupt the training labels by randomly flipping labels within each class to incorrect labels in other classes. As for asymmetric label noise, we flip the labels within a specific sets of classes: For CIFAR-10, the flips occur from TRUCK → AUTOMOBILE, BIRD → AIRPLANE, DEER → HORSE, and CAT ↔ DOG. For CIFAR-100, the 100 classes are grouped into 20 super-classes, each containing 5 sub-classes, and we flip the labels within the same super-class into the next. For instance-dependent noise, we follow the approach in PDN [\[28\]](#page-9-7) for generating label noise.

Experimental Setting. We follow the experimental settings in [\[18,](#page-8-6) [30,](#page-9-2) [33\]](#page-9-1): An 8-layer CNN is used for CIFAR-10 and a ResNet-34 [\[9,](#page-8-23) [14\]](#page-8-22) for CIFAR-100. The networks are trained for 120 and 200 epochs for CIFAR-10 and CIFAR-100 with batch size 128. We use the SGD optimizer with momentum 0.9 and L1 weight decay 5 × 10−<sup>5</sup> and 5 × 10−<sup>6</sup> for CIFAR-10 and CIFAR-100. The learning rate is set to 0.01 for CIFAR-10 and 0.1 for CIFAR-100 with cosine annealing. Typical data augmentations including random shift and horizontal flip are applied.

Parameters Setting. For baselines, we use the same parameter settings in [\[18,](#page-8-6) [30,](#page-9-2) [33\]](#page-9-1), which match their best parameters. The detailed parameters for JAL and baselines can be found in Table [8.](#page-11-1) For LT-APL [\[31\]](#page-9-0), we take results directly from the original paper. For our method, we follow a principled strategy for parameter tuning: the range of a can be initially estimated through theoretical guidance, and then selected from [5, 10, 20, 30] based on experimental results.

<span id="page-11-1"></span>

| Parameter             | CIFAR-10         | CIFAR-100         | WebVision         | Clothing1M        |
|-----------------------|------------------|-------------------|-------------------|-------------------|
| CE                    | -                | -                 | -                 | -                 |
| FL (γ)                | (0.5)            | (0.5)             | -                 | -                 |
| GCE (q)               | (0.9)            | (0.7)             | (0.7)             | (0.6)             |
| SCE (α, β, A)         | (0.1, 1, -4)     | (6, 1, -4)        | (10, 1, -4)       | (10, 1, -4)       |
| NCE                   | -                | -                 | -                 | -                 |
| NCE+RCE (α, β, A)     | (1, 1, -4)       | (10, 0.1, -4)     | (50, 0.1, -4)     | (10, 1, -4)       |
| NCE+AUL (α, β, a, p)  | (1, 3, 6.3, 1.5) | (10, 0.015, 6, 3) | -                 | -                 |
| NCE+AGCE (α, β, a, q) | (10, 4, 6, 1.5)  | (10, 0.1, 1.8, 3) | (50, 0.1, 2.5, 3) | (50, 0.1, 2.5, 3) |
| ANL-CE (α, β)         | (5, 5)           | (10, 1)           | (20, 1)           | (5, 0.1)          |
| ANL-FL (α, β, γ)      | (5, 5, 0.5)      | (10, 1, 0.5)      | (20, 1, 0.5)      | (5, 0.1, 0.5)     |
| JAL-CE (α, β, a)      | (1, 1, 30)       | (5, 1, 20)        | (50, 1, 30)       | (5, 0.1, 5)       |
| JAL-FL (α, β, a, γ)   | (1, 1, 30, 0.5)  | (5, 1, 20, 0.5)   | (50, 1, 30, 0.5)  | (5, 0.1, 5, 0.5)  |

Table 8. Parameter settings for different methods.

# B.2. Evaluation on Real-World Datasets

Experiment Setting for WebVision / ILSVRC12. For WebVision, we use the mini setting [\[10\]](#page-8-24), which includes the first 50 classes of the google image subset. We train a ResNet-50 using SGD for 250 epochs with initial learning rate 0.4, nesterov momentum 0.9 and weight decay 3 × 10<sup>−</sup><sup>5</sup> and batch size 256. The learning rate is multiplied by 0.97 after each epoch of training. All the images are resized to 224 × 224. Typical data augmentations including random shift, color jittering, and horizontal flip are applied. We train the model on Webvision and evaluate the trained model on the same 50 concepts on the corresponding WebVision and ILSVRC12 validation sets.

Experiment Setting for Clothing1M. For Clothing1M, we use ResNet-50 pre-trained on ImageNet similar to [\[29\]](#page-9-9). All the images are resized to 224 × 224. We use SGD with a momentum of 0.9, a weight decay of 1 × 10<sup>−</sup><sup>3</sup> , and batch size of 256. We train the network for 10 epochs with a learning rate of 5 × 10<sup>−</sup><sup>3</sup> and a decay of 0.1 at 5 epochs. Typical data augmentations including random shift and horizontal flip are applied.