# Compatibility of Max and Sum Objectives for Committee Selection and k-Facility Location

Yue Han and Elliot Anshelevich

Department of Computer Science, Rensselaer Polytechnic Institute

hany4@rpi.edu, eanshel@cs.rpi.edu

July 24, 2025

#### Abstract

We study a version of the metric facility location problem (or, equivalently, variants of the committee selection problem) in which we must choose k facilities in an arbitrary metric space to serve some set of clients C. We consider four different objectives, where each client i ∈ C attempts to minimize either the sum or the maximum of its distance to the chosen facilities, and where the overall objective either considers the sum or the maximum of the individual client costs. Rather than optimizing a single objective at a time, we study how compatible these objectives are with each other, and show the existence of solutions which are simultaneously close-to-optimum for any pair of the above objectives. Our results show that when choosing a set of facilities or a representative committee, it is often possible to form a solution which is good for several objectives at the same time, instead of sacrificing one desideratum to achieve another.

### 1 Introduction

Metric facility location problems and their variants form a classic and well-known area of research. In these problems, we are given a set of possible facility locations F and a set of n clients C in an arbitrary metric space (M, d). In the variants that we study in our work, the goal is to choose a set of k > 1 facilities A that optimizes some objective, usually making sure that the distance between clients and facilities is not too large. Such problems have been studied extensively in various areas such as operations research (see survey [\[12\]](#page-25-0)), approximation algorithms (see book [\[28\]](#page-26-0) and surveys [\[2,](#page-24-0) [24\]](#page-26-1)), and mechanism design (see survey [\[6\]](#page-25-1)) for decades. Metric facility location and its variants are general enough to capture many important settings. For example, consider when a city needs to choose a location to build a hospital from a set of potential locations, or a neighborhood wishes to choose multiple locations, each dedicated for one purpose (e.g. school, post office, library, grocery store). Facility location can also be thought of as a committee selection problem in a spacial voting setting [\[11,](#page-25-2)[21\]](#page-26-2), where each candidate is a potential facility location and each voter is a client; the goal is to choose k candidates to form a committee which is somehow representative of all the voters according to some objective.

The objective being optimized, i.e., the measure of what makes a committee or a set of facilities be "good", is a crucial component of facility location problems and their many versions. Many different objectives have been studied in the past (see Related Work). It is not clear, however, what the correct objective is for most settings. For example, should we care about utilitarian objectives (minimizing the average client cost), or egalitarian objectives (making sure that all clients are not too unhappy)? What determines if a committee or a set of locations is good from a client's perspective? Optimizing a single chosen objective can often make the cost of a solution in terms of other objectives be extremely bad, even if these other objectives are just as reasonable as the one we decided to optimize. In our work, instead of selecting a single specific objective, we would like to see if we are capable of finding a solution that would be simultaneously good for multiple objectives.

To see what kind of objectives we will optimize, first consider the individual cost for each client. Here note that instead of assigning each client to one facility, we are interested in modeling settings and applications where clients need to utilize all of the k facilities. For example, as mentioned above, consider the case where a neighborhood wants to build k different facilities to fulfill the needs of the residents in that area, e.g., a shopping center, a post office, and a grocery store. The residents will use all of these, but should they care more about not living too far away from any of these facilities, or about having a short average distance from their home to these facilities? In other words, is the goal to minimize the maximum distance from the client to all facilities (maxvariant [\[8,](#page-25-3) [19,](#page-26-3) [30\]](#page-26-4)), or is the goal to minimize the average (or total) distance from the client to all facilities (sum-variant [\[16,](#page-25-4) [19,](#page-26-3) [31\]](#page-26-5))? Similar objectives arise in spacial voting settings as well, where distance between a voter and a committee member represents ideological differences. Does a voter care about all of the committee members being ideologically similar to them (max-variant), or about the total ideological difference between them and the committee members (sum-variant)? While many other important objectives exist, such as the classical setting where each client is assigned to their closest facility, we focus on the max and sum variants in our work, and attempt to optimize both of them simultaneously.

After fixing the cost for each client, we also have many choices on how to calculate the overall cost of a committee or a set of facilities. Should we care about keeping the average of the individual costs as low as possible (a utilitarian measure), or should we keep the maximum cost of every individual low (an egalitarian measure)? With all the above individual costs, as well as different ways of combining them into an overall objective, this results in numerous possible objectives, with none necessarily "better" than the other. Because of this, we focus on optimizing multiple objectives simultaneously. While many other reasonable objectives exist, in our work we focus on the following four natural objectives:

<span id="page-1-0"></span>Definition 1.1. Let A be a set of k facilities and C be the set of clients in metric space (M, d). We define the following:

- Max-Max(A) = maxi∈C maxa∈<sup>A</sup> d(i, a)
- Max-Sum(A) = maxi∈C P a∈A d(i, a)
- Sum-Max(A) = P <sup>i</sup>∈C maxa∈<sup>A</sup> d(i, a)
- Sum-Sum(A) = P i∈C P a∈A d(i, a)

Our goal in this paper is to form solutions which are close-to-optimal for more than just a single objective. Formally, we want to form solutions which simultaneously approximate two objectives at the same time.

Definition 1.2. Simultaneous Approximation: Let c<sup>1</sup> and c<sup>2</sup> be two different objectives, and let A be the set of all possible solutions. Let O<sup>i</sup> be an optimum solution for objective ci, i.e., O<sup>i</sup> =

![](_page_2_Figure_0.jpeg)

<span id="page-2-0"></span>Figure 1: A summary of our results. Each line connecting two objectives includes a label showing how compatible we prove these two objectives to be. Specifically, if a line has a label [x, y], this means we show there always exists a solution which is a y-approximation to the optimum for both of these objectives simultaneously, as well as a lower bound showing no simultaneous approximation better than x is possible. Note that (∗) only holds when k ≥ 3 and is an approximation for the actual bounds, [(4 + <sup>√</sup> 7)/3 ≈ 2.22, 1 + p 5/3 ≈ 2.29].

arg minA∈A ci(A). We then define the approximation ratio of solution A with respect to objective c<sup>i</sup> as

$$
\alpha_{c_i}(A) = \frac{c_i(A)}{c_i(O_i)} \ge 1.
$$

Therefore, by choosing A, we would obtain a (αc<sup>1</sup> (A), αc<sup>2</sup> (A)) approximation for minimizing the two objectives. If we let α = max{αc<sup>1</sup> (A), αc<sup>2</sup> (A)}, this means that A is within a factor α for minimizing both of the objectives. Hence, we define α as the simultaneous approximation ratio of A for objectives c<sup>1</sup> and c2.

In other words, a solution A which simultaneously approximates two objectives within a factor α is simply a solution which is within a factor α of optimum for each objective, and thus a solution which is an α-approximation for each objective individually.

In our work, we call a pair of objectives α-compatible or just compatible if there always exists a solution which simultaneously approximates both objectives within some small constant α. This shows that it is possible to (approximately) optimize both objectives at once, and we do not have to sacrifice one objective in order to form a good solution for the other. Our goal, then, is to see if the four objectives listed above are compatible with each other, as well as quantify the upper and lower bounds of the simultaneous approximation ratio for each pair of them.

#### 1.1 Our Contributions

We begin by proving that for any pair of our objectives, there always exists a solution which is simultaneously a 3-approximation for both objectives. Moreover, this solution is simply the optimum solution for Sum-Sum. This simple result shows that all of the objectives are 3-compatible. We then proceed to improve the 3-simultaneous approximation ratio for individual pairs of objectives; our results are summarized in Figure [1.](#page-2-0)

To do this, we first show that results from [\[15\]](#page-25-5) which apply to placing a single facility (i.e., k = 1), extend without much difficulty to simultaneously approximating the pair (Sum-Sum, Max-Sum), as well as the pair (Sum-Max, Max-Max). This immediately implies that both pairs of objectives are (1 + <sup>√</sup> 2)-compatible. We then proceed with further improvements to these bounds, as shown in our main contributions:

- Sum-Sum and Max-Sum While the above approximation bound of 1 + <sup>√</sup> 2 ≈ 2.41 is tight for Sum-Max and Max-Max, we can show better bounds for simultaneously approximating Sum-Sum and Max-Sum when choosing more than 2 facilities. In fact, in Section [4.1](#page-9-0) we prove that these objectives are simultaneously approximable to within a factor of 1 + <sup>q</sup> 5 <sup>3</sup> ≈ 2.29, as long as k ≥ 3. This may be somewhat surprising, as usually things get worse and more complex as the number of facilities k becomes larger. For the questions we are asking, however, the reverse turns out to be true: when k is small, then only a few possible solutions may exist, and they may all be bad for at least one of the two objectives. When k is large, however, many solutions become possible, and we are able to always form a solution which is good for both objectives by carefully stitching together parts of the optimum solutions for each separate objective. The solution we form is not optimum for either, but is a good approximation for both.
- Max-Sum and Max-Max We then proceed to consider Max-Sum and Max-Max, Sum-Sum and Sum-Max, for which the results from [\[15\]](#page-25-5) can no longer be extended directly; the compatibility of these objectives has not been considered before. While we show both of these pairs of objectives are simultaneously approximable to within a factor of min(<sup>√</sup> k, 3), we are able to significantly improve this bound for the specific case of Max-Sum and Max-Max. We prove that these objectives are 2-compatible, showing that if we care about both of these costs, we are able to obtain a solution that is close-to-optimum for both. This result requires somewhat different techniques, and is presented in Section [4.3.](#page-19-0)

#### 1.2 Related Work

Many variants of facility location problems, as well as spacial voting problems, have been studied within many disciplines and are too much to survey here; see surveys [\[6,](#page-25-1)[12\]](#page-25-0). Single-facility location (i.e., k = 1) has been especially well-studied, while building multiple facilities has received somewhat less attention to due to the added complexity. When choosing the locations for multiple facilities, the most commonly considered cost for each client is the distance from the client to their closest facility [\[1,](#page-24-1) [5,](#page-25-6) [7,](#page-25-7) [18,](#page-26-6) [25,](#page-26-7) [26\]](#page-26-8). In our work, however, we are interested in settings where the client cares about the locations of all k facilities, not just the closest one. These settings arise when the facilities being built are heterogeneous [\[23\]](#page-26-9) (for example one is a post office, another is a grocery store, etc, so the client has to use them all). In social choice settings, this corresponds to the idea that the voter cares about the entire committee membership, not just the member which is most similar to them. Thus, we instead consider other types of client costs, specifically the maximum distance to a built facility (as in [\[8,](#page-25-3) [19,](#page-26-3) [30\]](#page-26-4)), and the total (or average) distance to the built facilities (as in [\[16,](#page-25-4) [19,](#page-26-3) [29,](#page-26-10) [31\]](#page-26-5)). Although we focus on the above two cost types, there are certainly other client cost functions that have been studied and are interesting. For example, [\[4\]](#page-25-8) studies q-social cost, where the cost of choosing a committee of size k for each voter is the distance from them to their q'th closest alternative in the committee, and [\[13\]](#page-25-9) considers the scenario where different clients could have different cost functions. For the overall objective combining the costs of the clients, max and sum are commonly used (see for example, [\[6–](#page-25-1)[8,](#page-25-3) [16,](#page-25-4) [19,](#page-26-3) [27\]](#page-26-11)), but are rarely considered together under the notion of simultaneous approximation. Note that while computing the optimum solution for some of these objectives is NP-Complete, in this work we are more focused on which objectives are compatible with each other in principle, leaving the question of poly-time simultaneous approximation for future work.

A significant amount of work also exists on optimizing several objectives at once for facility location. However, as described in [\[10,](#page-25-10)[12\]](#page-25-0), the most commonly studied way to do this is to convert multiple objectives into a single objective. For example, [\[20,](#page-26-12) [22\]](#page-26-13) combine two objectives together using a convex combination of them. Moreover, [\[1\]](#page-24-1) considers a slightly different measure that would achieve a (4, 8) approximation for k-Center and k-Median problems w.r.t the optimal solution of a convex combination of the two objectives.

Instead of such previous approaches, we attempt to approximate a pair of objectives simultaneously, so the solution formed is close-to-optimal for each of them at the same time. This notion of approximation has received far less attention (see the Related Work section in [\[15\]](#page-25-5) for a detailed discussion). While [\[15\]](#page-25-5) studied exactly this notion for exactly our setting, they only considered building one facility (k = 1); in this paper we greatly generalize their results to k > 1 facilities, as well as use new techniques for multiple facilities to form better approximation bounds. Although not the main focus of their work, [\[1\]](#page-24-1) considered this notion of approximation as well, and showed that if each client's cost is their distance to their closest facility, the simultaneous approximation ratio even for choosing two facilities for minimax and minisum can be arbitrarily large. While our results establish that many sum and max objectives are simultaneously compatible, the results from [\[1\]](#page-24-1) show that this is not true if we care about the distance to the closest facility instead. There is also other previous work, such as [\[14,](#page-25-11)[18\]](#page-26-6), which either considers the simultaneous approximation of max and sum objectives in our setting, or implies results that would fit under this model. However, such work only considers choosing a single facility, while our results hold for an arbitrary number of facilities k.

Another nice line of literature which studies facility location and committee selection exists in the area of mechanism design (see survey [\[6\]](#page-25-1)). Such literature is mostly interested in finding mechanisms such that no clients have the incentive to lie about their location or preferences, i.e., a truthful (or, strategy-proof) mechanism. In addition, much of this work is focused on choosing a single facility [\[7,](#page-25-7)[8,](#page-25-3)[16\]](#page-25-4), or two facilities [\[9,](#page-25-12)[19,](#page-26-3)[29\]](#page-26-10), and often only on a line, instead of in an arbitrary metric space. [\[27\]](#page-26-11) provides a 3-simultaneous approximation deterministic strategy-proof mechanism for minimax and minisum for choosing one facility on a line with limited location options, but also shows that no deterministic strategy-proof mechanism can do better than 3. The setting from [\[19\]](#page-26-3) may be the closest to ours in this area, which considers the two-facility location problem (k = 2) on a line and provides a (5,11) truthful deterministic mechanism for Max-Max and Max-Sum. Furthermore, 5 and 11 are tight bounds for the truthful approximation ratio for Max-Max and Max-Sum respectively. In addition, they also showed a 22 simultaneous approximation truthful deterministic mechanism for all of the four objectives from Definition [1.1.](#page-1-0) In this paper, we study the case where each client cares about all of the facilities in a general metric space without the consideration of strategy-proofness; our goal is to quantify what is possible to achieve when caring about multiple objectives, and which objectives are compatible with each other.

### <span id="page-5-4"></span>2 Extending Previous Results to Multiple Facilities (k > 1)

Consider the version of the facility location problem where we are given the set C of n client locations, and the multiset of possible facility locations F [1](#page-5-0) in a metric space (M, d). We want to place k facilities so that the placement would simultaneously be close to optimal for several of our objectives. Denote the set of k locations by A such that A ⊆ F, |A| = k. The set of objectives that we are interested in are chosen from objectives defined in Definition [1.1.](#page-1-0) In this section, however, it is convenient to think more generally, and consider an arbitrary function f : M × M<sup>k</sup> → R <sup>+</sup> ∪ {0} so that f(i, A) denotes the cost of client i for solution A. For such a fixed function f, we can also define the following.

<span id="page-5-5"></span>Definition 2.1. Let A ⊆ F, |A| = k. We define Max-f and Sum-f as follows:

- Max-f = maxi∈C(f(i, A)).
- Sum-f = P <sup>i</sup>∈C(f(i, A)).

Where f : M × M<sup>k</sup> → R <sup>+</sup> ∪ {0}.

In existing work, [\[15\]](#page-25-5) considered the case where k = 1, with the cost function being Max and Sum where f(i, A) = d(i, a), A = {a} ⊆ F, i ∈ C. Consider, however, a more general setting with k > 1, and suppose that the function f obeys the following inequality (which is essentially just the triangle inequality):

<span id="page-5-1"></span>
$$
f(i, A) \le f(i, B) + f(j, B) + f(j, A) \qquad \forall i, j \in \mathcal{C}, A \subseteq \mathcal{F}, B \subseteq \mathcal{F}
$$
 (1)

Note that some classic objectives such as k-median, for which f is the min function, do not obey this property. As we discuss in Section [2.1,](#page-6-0) however, many important objectives do satisfy this property. If f satisfies this inequality, consider a new metric (M′ , d′ ) where each client in C and each subset of k facilities in F are a point in M′ . We denote the set of client points by C ′ and the set of facility combinations by F ′ . Then, we can define a new metric d ′ as

- 1. d ′ (i, j) = d(i, j), i, j ∈ C′ 2. d ′ (i, A) = d ′ (A, i) = f(i, A), i ∈ C′ , A ∈ F′
- 3. d ′ (A, B) = mini∈C′(f(i, A) + f(i, B)), A, B ∈ F′

Since (M, d) is a metric, combined with Inequality [1,](#page-5-1) we can see that d ′ also obeys triangle inequality. Therefore, we can reduce the problem from choosing k facilities in metric (M, d) with clients C and facility locations F to choosing 1 facility in metric (M′ , d′ ) with clients C ′ and facility locations F ′ . Due to this reduction, results from [\[15\]](#page-25-5) immediately extend to choosing multiple facilities.[2](#page-5-2) In particular, the following results still hold:

<span id="page-5-3"></span>Theorem 2.1. (generalization of Theorem 3.1 from [\[15\]](#page-25-5)) Given the optimal facility location set O<sup>Σ</sup> that minimizes Sum-f and the optimal facility location set O<sup>M</sup> that minimizes Max-f, we have that αMax-<sup>f</sup> (OΣ) ≤ 1 <sup>α</sup>Sum-<sup>f</sup> (OM) + 2 as long as f obeys Inequality [1.](#page-5-1)

<span id="page-5-0"></span><sup>1</sup>Here we note that F is a multiset such that the multiplicity of a facility location in F is the number of times that location can be used/chosen.

<span id="page-5-2"></span><sup>2</sup> [\[15\]](#page-25-5) actually considers more than just Sum-f and Max-f. The extension we showed above also holds for all the other cost functions over all clients considered in [\[15\]](#page-25-5) (namely the l-centrum objectives, see Definition [A.1\)](#page-27-0).

Corollary 2.1.1. (generalization of Corollary 3.1.1 from [\[15\]](#page-25-5)) When f obeys Inequality [1,](#page-5-1) we have that

- 1. By choosing the optimal facility location set O<sup>Σ</sup> that minimizes Sum-f, we obtain a (3, 1) approximation for simultaneously minimizing Max-f and Sum-f.
- <span id="page-6-2"></span>2. There always exists a facility location set <sup>A</sup> ⊆ F such that choosing <sup>A</sup> would give a 1 + <sup>√</sup> 2 approximation both for minimizing Max-f and minimizing Sum-f. In fact, we would either get a (1, 1 + <sup>√</sup> 2) approximation by choosing <sup>O</sup><sup>M</sup> or a (1 + <sup>√</sup> 2, 1) approximation by choosing OΣ. In other words, at least one of αMax-<sup>f</sup> (OΣ) or αSum-<sup>f</sup> (OM) is always less than or equal to 1 + <sup>√</sup> 2.

#### <span id="page-6-0"></span>2.1 Examples of f

We will now look at some functions f that obey Inequality [1.](#page-5-1) Some examples of f include using the centroid and any norms of the set of chosen facilities, since these kinds of functions would map each possible facility set/committee (set of k facilities) into a new metric space, which means that f would obey Inequality [1.](#page-5-1) In addition, [\[4\]](#page-25-8) showed that the q-social cost of a committee of size k with q > k/2 also obeys Inequalit[y1.](#page-5-1) Note that for the four objectives that we consider (Max-Sum, Sum-Sum, Max-Max, and Sum-Max), we use the summation function and the maximum function as f. In fact, these two functions also obey Inequality [1,](#page-5-1) as we prove below, immediately implying that both (Max-Sum, Sum-Sum) and (Max-Max, Sum-Max) are (1 + <sup>√</sup> 2)-compatible.

<span id="page-6-1"></span>Lemma 2.2. Let

$$
f(i, A) = \sum_{a \in A} d(i, a) \qquad \forall i \in C, A \subseteq \mathcal{F},
$$

then we have that

$$
f(i, A) \le f(i, B) + f(j, B) + f(j, A) \qquad \forall i, j \in \mathcal{C}, A \subseteq \mathcal{F}, B \subseteq \mathcal{F}, |A| = |B| = k.
$$

Proof. For clarity, let A = {a1, a2, · · · , ak} ⊆ F, B = {b1, b2, · · · , bk} ⊆ F, i, j ∈ C, we then want to show that f obeys the above inequality. Note that all clients and facility locations are in some metric space (M, d), using triangle inequality we have

$$
f(i, A) = d(i, a_1) + d(i, a_2) + \dots + d(i, a_k)
$$
  
\n
$$
\leq d(i, b_1) + d(i, b_2) + \dots + d(i, b_k) + d(a_1, b_1) + d(a_2, b_2) + \dots + d(a_k, b_k)
$$
  
\n
$$
\leq f(i, B) + d(j, a_1) + d(j, a_2) + \dots + d(j, a_k) + d(j, b_1) + d(j, b_2) + \dots + d(j, b_k)
$$
  
\n
$$
= f(i, B) + f(j, B) + f(j, A).
$$

Since the choice of A, B, i, j are arbitrary, we can conclude that f indeed satisfies the statement.

<span id="page-6-3"></span>Lemma 2.3. Let

$$
f(i, A) = \max_{a \in A} d(i, a) \qquad \forall i \in C, A \subseteq \mathcal{F},
$$

then we have that

$$
f(i, A) \le f(i, B) + f(j, B) + f(j, A) \qquad \forall i, j \in \mathcal{C}, A \subseteq \mathcal{F}, B \subseteq \mathcal{F}.
$$

Proof. Let a <sup>∗</sup> ∈ A ⊆ F, b<sup>∗</sup> ∈ B ⊆ F, i, j ∈ C such that d(i, a<sup>∗</sup> ) = maxa∈<sup>A</sup> d(i, a), d(i, b<sup>∗</sup> ) = maxb∈<sup>B</sup> d(i, b). Note that we also have that d(j, b<sup>∗</sup> ) ≤ maxb∈<sup>B</sup> d(j, b), d(j, a<sup>∗</sup> ) ≤ maxa∈<sup>A</sup> d(j, a). Then, by triangle inequality we have

$$
f(i, A) = d(i, a^*)
$$
  
\n
$$
\leq d(i, b^*) + d(j, b^*) + d(j, a^*)
$$
  
\n
$$
\leq \max_{b \in B} d(i, b) + \max_{b \in B} d(j, b) + \max_{a \in A} d(j, a)
$$
  
\n
$$
= f(i, B) + f(j, B) + f(j, A).
$$

Since the choice of A, B, i, j are arbitrary, we can conclude that f indeed satisfies the statement.

Unfortunately, if the cost for each client i ∈ C is the distance from them to the closest facility in the chosen set A ⊆ F,

$$
f(i, A) = \min_{a \in A} d(i, a),
$$

then such cost functions do not obey Inequality [1.](#page-5-1) In addition, as we have discussed previously, the simultaneous approximation ratio for Max-Min and Sum-Min is unbounded.

#### 3 3-Compatibility of our objectives

We are interested in simultaneously approximating all pairs of the objectives defined in Definition [1.1.](#page-1-0) Before analyzing each pair more carefully, we first make the observation that the optimal solution of Sum-Sum is a 3 approximation for all the four objectives as defined in Definition [1.1.](#page-1-0) To show this, we will first consider Sum-Sum and Max-Sum. In fact, by Lemma [2.2](#page-6-1) we note that this is a special case of Corollary [2.1.1](#page-6-2) and can obtain the following result.

<span id="page-7-2"></span>Corollary 3.0.1. The optimal solution of Sum-Sum is a 3 approximation for Max-Sum.

To continue on to the other two objectives, we will first show a useful lemma. Let O be the optimal solution for Sum-Sum, A be the optimal solution for Sum-Max and B be the optimal solution for Max-Max. In addition, let o <sup>∗</sup> = argmaxo∈<sup>O</sup> P <sup>i</sup>∈C d(i, o), a <sup>∗</sup> = argmaxa∈<sup>A</sup> P <sup>i</sup>∈C d(i, a), b <sup>∗</sup> = argmaxb∈<sup>B</sup> P <sup>i</sup>∈C d(i, b). For each i ∈ C, let o<sup>i</sup> = argmaxo∈<sup>O</sup> d(i, o), a<sup>i</sup> = argmaxa∈<sup>A</sup> d(i, a), b<sup>i</sup> = argmaxb∈<sup>B</sup> d(i, b).

<span id="page-7-0"></span>Lemma 3.1. For any P ⊆ F, |P| = k, p<sup>∗</sup> = argmaxp∈<sup>P</sup> P <sup>i</sup>∈C d(i, p), we have that P <sup>i</sup>∈C d(i, o<sup>∗</sup> P ) ≤ <sup>i</sup>∈C d(i, p<sup>∗</sup> ).

Proof. First note that if P = O, then it is trivial that P <sup>i</sup>∈C d(i, o<sup>∗</sup> ) ≤ P <sup>i</sup>∈C d(i, p<sup>∗</sup> ). Therefore, we assume P P ̸= O. We will prove the above claim using contradiction. Assume otherwise, <sup>i</sup>∈C d(i, o<sup>∗</sup> ) > P <sup>i</sup>∈C d(i, p<sup>∗</sup> ). Note that since P ̸= O, |P| = |O| = k, by pigeonhole principle there must exist some p ∈ P such that p /∈ O. Recall that p <sup>∗</sup> = argmaxp∈<sup>P</sup> P <sup>i</sup>∈C d(i, p), this means that P <sup>i</sup>∈C d(i, o<sup>∗</sup> ) > P <sup>i</sup>∈C d(i, p<sup>∗</sup> ) ≥ P <sup>i</sup>∈C d(i, p). Now, let O′ = {p} ∪ O \ {o ∗ P }, we have that i∈C P o ′∈O′ d(i, o′ ) = P i∈C P o∈O d(i, o) − P <sup>i</sup>∈C d(i, o<sup>∗</sup> ) + P <sup>i</sup>∈C d(i, p) < P i∈C P o∈O d(i, o), which is a contradiction. Therefore, we can conclude that P <sup>i</sup>∈C d(i, o<sup>∗</sup> ) ≤ P <sup>i</sup>∈C d(i, p<sup>∗</sup> ).

Then, we can show the following results.

<span id="page-7-1"></span>Theorem 3.2. The optimal solution of Sum-Sum is a 3 approximation for Sum-Max.

Proof. By triangle inequality, we have that

$$
SUM-MAX(O) = \sum_{i \in C} d(i, o_i)
$$
  
\n
$$
\leq \sum_{i} (d(i, a_i) + d(a_i, o_i))
$$
  
\n
$$
\leq SUM-MAX(A) + \sum_{i} \frac{1}{n} \sum_{j} d(j, a_i) + \sum_{i} \frac{1}{n} \sum_{j} d(j, o_i)
$$
  
\n
$$
\leq SUM-MAX(A) + \sum_{i} \frac{1}{n} \sum_{j} d(j, a^*) + \sum_{i} \frac{1}{n} \sum_{j} d(j, o^*)
$$

The last inequality is true due to how a <sup>∗</sup> and o ∗ P are defined. Now, by Lemma [3.1,](#page-7-0) we have <sup>i</sup>∈C d(i, o<sup>∗</sup> ) ≤ P <sup>i</sup>∈C d(i, a<sup>∗</sup> ), therefore,

$$
Sum\text{-}MAX(O) \leq SUM\text{-}MAX(A) + \sum_{i} \frac{1}{n} \sum_{j} d(j, a^*) + \sum_{i} \frac{1}{n} \sum_{j} d(j, a^*)
$$
\n
$$
\leq SUM\text{-}MAX(A) + 2 \sum_{j} d(j, a^*)
$$
\n
$$
\leq SUM\text{-}MAX(A) + 2 \sum_{i} d(i, a_i)
$$
\n
$$
= 3 \cdot SUM\text{-}MAX(A)
$$

as desired.

<span id="page-8-0"></span>Theorem 3.3. The optimal solution of Sum-Sum is a 3 approximation for Max-Max.

Proof. Let i = argmaxj∈C d(j, o<sup>j</sup> ). Similiar to the proof of Theorem [3.2,](#page-7-1) by triangle inequality, we have that

$$
\begin{aligned} \text{MAX-MAX}(O) &= d(i, o_i) \\ &\le d(i, b_i) + d(b_i, o_i) \\ &\le \text{MAX-MAX}(B) + \frac{1}{n} \sum_j d(j, b_i) + \frac{1}{n} \sum_j d(j, o_i) \\ &\le \text{MAX-MAX}(B) + \frac{1}{n} \sum_j d(j, b^*) + \frac{1}{n} \sum_j d(j, o^*) \end{aligned}
$$

The last inequality is true due to how b <sup>∗</sup> and o ∗ P are defined. Now, by Lemma [3.1,](#page-7-0) we have <sup>i</sup>∈C d(i, o<sup>∗</sup> ) ≤ P <sup>i</sup>∈C d(i, b<sup>∗</sup> ), therefore,

$$
\begin{aligned} \text{MAX-MAX}(O) &\leq \text{MAX-MAX}(B) + \frac{1}{n} \sum_{j} d(j, b^*) + \frac{1}{n} \sum_{j} d(j, b^*) \\ &\leq \text{MAX-MAX}(B) + \frac{2}{n} \sum_{j} d(j, b^*) \\ &\leq \text{MAX-MAX}(B) + 2 \max_{j \in C} d(j, b_j) \\ &= 3 \cdot \text{MAX-MAX}(B) \end{aligned}
$$

as desired.

Finally, combining Corollary [3.0.1,](#page-7-2) Theorem [3.2,](#page-7-1) Theorem [3.3,](#page-8-0) we can see that

Theorem 3.4. The optimal solution of Sum-Sum is a 3 approximation for Max-Sum, Max-Max and Sum-Max.

The above result gives a solution that is a 3 approximation for all the combinations of the 4 objectives defined in Definition [1.1.](#page-1-0) In the next section, we will see if we can improve the simultaneous approximation ratio for some of the pairs of the 4 objectives.

#### 4 Improved Approximations for Pairs of Objectives

### <span id="page-9-0"></span>4.1 Sum-Sum and Max-Sum

Note that so far when choosing multiple facilities, to get a good approximation ratio for two objectives, we have been using the optimal solution for one of the objectives. However, can we get a better approximation if we choose a subset of one of the optimal solutions and a subset of the other optimal solution? To see what it would result in, we first consider Max-Sum and Sum-Sum, and establish the following helpful notation.

Definition 4.1. Suppose A, B are two non-empty sets in metric space (M, d), define f : M ×M → R <sup>+</sup> ∪ {0} as follows:

$$
f(A, B) = \sum_{a \in A} \sum_{b \in B} d(a, b).
$$

<span id="page-9-1"></span>Lemma 4.1. For any non-empty sets A, B, C in metric space (M, d), we have that

$$
f(A, B) \le \frac{|B|}{|C|} f(A, C) + \frac{|A|}{|C|} f(B, C).
$$

Proof. Consider any a ∈ A, b ∈ B, c ∈ C, since a, b, c are all in some metric space, we have that

$$
f(a,b) \le f(a,c) + f(c,b)
$$

Since the choice of c is arbitrary, we also have that

$$
f(a,b) \le \frac{1}{|C|} f(a,C) + \frac{1}{|C|} f(C,b).
$$

Then, we can see that

$$
f(A, B) = \sum_{a \in A} \left( \sum_{b \in B} f(a, b) \right)
$$
  
\n
$$
\leq \sum_{a \in A} \sum_{b \in B} (f(a, c) + f(c, b))
$$
  
\n
$$
\leq \sum_{a \in A} \sum_{b \in B} \left( \frac{1}{|C|} f(a, C) + \frac{1}{|C|} f(C, b) \right)
$$
  
\n
$$
= \sum_{a \in A} \left( \frac{|B|}{|C|} f(a, C) + \frac{1}{|C|} f(C, B) \right)
$$
  
\n
$$
= \frac{|B|}{|C|} f(A, C) + \frac{|A|}{|C|} f(B, C)
$$

as desired.

Let OΣΣ be the optimal solution for Sum-Sum, OM<sup>Σ</sup> be the optimal solution for Max-Sum, O = OM<sup>Σ</sup> ∩ OΣΣ, and k ′ = k − |O|, where k is the number of facilities we are choosing. In order to form a solution which simultaneously approximates both objectives, we will consider 3 cases, (i) k ′ > 1 and k ′ is even, (ii) k ′ > 1 and k ′ is odd, (iii) k ′ = 1. First, we consider case (i), assume k ′ > 1 is even. For any A ⊆ F \ O and |A| = k ′ , we reorder each a <sup>i</sup> ∈ A such that P <sup>a</sup>∈C d(a, a<sup>1</sup> ) ≤ P <sup>a</sup>∈C d(a, a<sup>2</sup> ) ≤ · · · ≤ P <sup>a</sup>∈C d(a, a<sup>k</sup> ′ ). Then, let QM<sup>Σ</sup> = {o 1 MΣ , o<sup>2</sup> MΣ , · · · , o k ′/2 <sup>M</sup><sup>Σ</sup> }, QΣΣ = {o 1 ΣΣ, o<sup>2</sup> ΣΣ, · · · , o k ′/2 ΣΣ }. Note that the above also means that QM<sup>Σ</sup> = argminA⊆OMΣ\O:|A|=<sup>k</sup> ′/2 P <sup>i</sup>∈C f(i, A), and QΣΣ = argminA⊆OΣΣ\O:|A|=<sup>k</sup> ′/2 P <sup>i</sup>∈C f(i, A). We can then show the following result.

<span id="page-10-0"></span>Theorem 4.2. For k ′ > 1 and even, there always exists a facility location set A ⊆ F such that choosing <sup>A</sup> would give a (5 +<sup>√</sup> 17)/4 approximation both for minimizing Max-Sum and minimizing Sum-Sum.

Proof. First, note that by Lemma [2.2](#page-6-1) and Theorem [2.1](#page-5-3) we have that

$$
\alpha_{\text{M}\Sigma}(O_{\Sigma\Sigma}) \leq \frac{1}{\alpha_{\Sigma\Sigma}(O_{\text{M}\Sigma})} + 2
$$

which implies that we can obtain a min(αΣΣ(OMΣ), 1/(αΣΣ(OMΣ) + 2)) approximation by choosing OM<sup>Σ</sup> or OΣΣ. Now, in addition to OM<sup>Σ</sup> and OΣΣ, we will look at solution A = O ∪ QM<sup>Σ</sup> ∪ QΣΣ. Let RM<sup>Σ</sup> = OM<sup>Σ</sup> \ (QM<sup>Σ</sup> ∪ O), RΣΣ = OΣΣ \ (QΣΣ ∪ O). Note that A is composed of OM<sup>Σ</sup> and OΣΣ with no duplicate facility locations, it is a valid solution. Then, we will analyze how good it is w.r.t. to Max-Sum(OMΣ). With triangle inequality, we have that

$$
\begin{aligned} \text{MAX-SUM}(A) &= \max_{i \in \mathcal{C}} f(i, A) \\ &= \max_{i \in \mathcal{C}} f(i, O \cup Q_{M\Sigma} \cup Q_{\Sigma\Sigma}) \\ &= \max_{i \in \mathcal{C}} \left( f(i, O) + f(i, Q_{M\Sigma}) + f(i, Q_{\Sigma\Sigma}) \right) \end{aligned}
$$

Here note that by Lemma [4.1,](#page-9-1) we have that f(i, QMΣ) ≤ f(i, RMΣ)+(2/k′ )f(RMΣ, QΣΣ), f(RMΣ, QΣΣ) ≤ (k ′/2)f(i, RMΣ) + (k ′/2)f(i, QΣΣ). Therefore, we can see that

$$
\begin{split} \text{MAX-SUM}(A) &\leq \max_{i \in \mathcal{C}} \left( f(i, O) + f(i, Q_{M\Sigma}) + f(i, R_{M\Sigma}) + \frac{2}{k'} \cdot f(R_{M\Sigma}, Q_{\Sigma\Sigma}) \right) \\ &\leq \text{MAX-SUM}(O_{M\Sigma}) + \frac{2}{k'} \cdot f(R_{M\Sigma}, Q_{\Sigma\Sigma}) \\ &\leq \text{MAX-SUM}(O_{M\Sigma}) + \frac{2}{k'} \cdot \frac{1}{n} \left( \frac{k'}{2} \cdot \sum_{i \in \mathcal{C}} f(i, R_{M\Sigma}) + \frac{k'}{2} \cdot \sum_{i \in \mathcal{C}} f(i, Q_{\Sigma\Sigma}) \right) \\ &\leq \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{n} \sum_{i \in \mathcal{C}} f(i, R_{M\Sigma}) + \frac{1}{n} \sum_{i \in \mathcal{C}} f(i, Q_{\Sigma\Sigma}) \end{split}
$$

Note that since RM<sup>Σ</sup> ⊂ OMΣ, so we can see that

$$
\sum_{i \in C} f(i, R_{M\Sigma}) \leq \sum_{i \in C} f(i, O_{M\Sigma})
$$
  
$$
\leq n \cdot \max_{i \in C} f(i, O_{M\Sigma})
$$
  
$$
\leq n \cdot \text{MAX-SUM}(O_{M\Sigma})
$$

Now, recall that we have QΣΣ = argminA⊆OΣΣ\O:|A|=<sup>k</sup> ′/2 P <sup>i</sup>∈C f(i, A), this means that P i∈C P f(i, QΣΣ) ≤ <sup>i</sup>∈C f(i, RΣΣ). We can then see that

$$
\sum_{i \in C} f(i, Q_{\Sigma \Sigma}) = \frac{1}{2} \left( \sum_{i \in C} f(i, Q_{\Sigma \Sigma}) + \sum_{i \in C} f(i, Q_{\Sigma \Sigma}) \right)
$$
  
\n
$$
\leq \frac{1}{2} \left( \sum_{i \in C} f(i, Q_{\Sigma \Sigma}) + \sum_{i \in C} d(i, R_{\Sigma \Sigma}) \right)
$$
  
\n
$$
= \frac{1}{2} \sum_{i \in C} f(i, Q_{\Sigma \Sigma} \setminus O)
$$
  
\n
$$
\leq \frac{1}{2} \sum_{i \in C} f(i, Q_{\Sigma \Sigma})
$$
  
\n
$$
\leq \frac{1}{2} \text{SUM-SUM}(O_{\Sigma \Sigma})
$$

Now, since we have that P <sup>i</sup>∈C f(i, QΣΣ) ≤ 1 2 Sum-Sum(OΣΣ) and P <sup>i</sup>∈C <sup>f</sup>(i, RMΣ) <sup>≤</sup> <sup>n</sup>·Max-Sum(OMΣ), we can derive that

$$
\begin{split} \text{MAX-SUM}(A) &\leq \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{n} \sum_{i \in \mathcal{C}} f(i, R_{M\Sigma}) + \frac{1}{n} \sum_{i \in \mathcal{C}} f(i, Q_{\Sigma\Sigma}) \\ &\leq \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{n} \cdot n \cdot \text{MAX-SUM}(O_{M\Sigma})) + \frac{1}{2n} \text{SUM-SUM}(O_{\Sigma\Sigma}) \\ &= 2 \cdot \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{2n} \cdot \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})} \text{SUM-SUM}(O_{M\Sigma}) \\ &\leq 2 \cdot \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{2n} \cdot \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})} \cdot \frac{n}{1} \text{MAX-SUM}(O_{M\Sigma}) \\ &= \left(2 + \frac{1}{2\alpha_{\Sigma\Sigma}(O_{M\Sigma})}\right) \text{MAX-SUM}(O_{M\Sigma}) \end{split}
$$

Note that we are calculating the approximation ratio for both Max-Sum and Sum-Sum and it could be the case that one is worse than the other. Since the final simultaneous approximation ratio would be the worse of the two, next, we will examine how good A is w.r.t. Sum-Sum(OΣΣ).

$$
Sum-SUM(A) = \sum_{i \in C} f(i, A)
$$
  
= 
$$
\sum_{i \in C} f(i, Q_{M\Sigma}) + \sum_{i \in C} f(i, Q_{\Sigma\Sigma}) + \sum_{i \in C} f(i, O)
$$

Here note that we have previously showed that P <sup>i</sup>∈C f(i, QΣΣ) ≤ 1 2 P <sup>i</sup>∈C f(i, OΣΣ \ O), using the same argument we also have that P <sup>i</sup>∈C f(i, QMΣ) ≤ 1 2 P <sup>i</sup>∈C f(i, OM<sup>Σ</sup> \ O). Therefore,

$$
SUM-SUM(A) \leq \frac{1}{2} \sum_{i \in C} f(i, O_{M\Sigma} \setminus O) + \frac{1}{2} \sum_{i \in C} f(i, O_{\Sigma\Sigma} \setminus O) + \sum_{i \in C} f(i, O)
$$
  
= 
$$
\frac{1}{2} SUM-SUM(O_{M\Sigma}) + \frac{1}{2} SUM-SUM(O_{\Sigma\Sigma})
$$
  
= 
$$
\frac{1}{2} \alpha_{\Sigma\Sigma}(O_{M\Sigma}) \cdot SUM-SUM(O_{\Sigma\Sigma}) + \frac{1}{2} SUM-SUM(O_{\Sigma\Sigma})
$$
  
= 
$$
\frac{1 + \alpha_{\Sigma\Sigma}(O_{M\Sigma})}{2} \cdot SUM-SUM(O_{\Sigma\Sigma})
$$

Hence we can conclude that choosing A is a max 2 + <sup>1</sup> 2αΣΣ(OMΣ) , 1+αΣΣ(OMΣ) 2 approximation of Max-Sum and Sum-Sum. Then, this means that one of OMΣ, OΣΣ, A would give us a

$$
\min\left(\alpha_{\Sigma\Sigma}(O_{M\Sigma}), \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})} + 2, \max\left(2 + \frac{1}{2\alpha_{\Sigma\Sigma}(O_{M\Sigma})}, \frac{1 + \alpha_{\Sigma\Sigma}(O_{M\Sigma})}{2}\right)\right)
$$

approximation of Max-Sum and Sum-Sum. Finally, by Lemma [4.3,](#page-12-0) we have that one of OMΣ, OΣΣ, A would give us at most a (5 + <sup>√</sup> 17)/4 approximation for both objectives.

<span id="page-12-0"></span>Lemma 4.3. For x ≥ 1,

$$
\min\left(x, \frac{1}{x} + 2, \max\left(2 + \frac{1}{2x}, \frac{1+x}{2}\right)\right)
$$

is bounded above by (5 + <sup>√</sup> 17)/4.

Proof. Let f<sup>1</sup> = x, f<sup>2</sup> = 1 <sup>x</sup> + 2, f<sup>3</sup> = 2 + <sup>1</sup> 2x , f<sup>4</sup> = 1+x 2 . Suppose x ≥ 1, we have that f1, f<sup>4</sup> are monotone increasing and f2, f<sup>3</sup> are monotone decreasing. In addition, note that we must have f<sup>2</sup> = 1 <sup>x</sup> + 2 <sup>&</sup>gt; 2 + <sup>1</sup> <sup>2</sup><sup>x</sup> = f<sup>3</sup> when x ≥ 1, these two function would never intersect on x ≥ 1. In addition, note that when <sup>x</sup> <sup>≥</sup> <sup>3</sup>/2 + <sup>√</sup> 13/2, we have f<sup>3</sup> < f4. Therefore, to find the maximum value of min(f1, f2, max(f3, f4)), we consider two cases, (i) <sup>x</sup> <sup>≥</sup> <sup>3</sup>/2 + <sup>√</sup> 13/2, min(f1, f2, max(f3, f4)) = min(f1, f2, f4), (ii) x < <sup>3</sup>/2 + <sup>√</sup> 13/2, min(f1, f2, max(f3, f4)) = min(f1, f2, f3).

First, consider case (i). Here note that when f<sup>1</sup> = f4, we have x = 1, therefore f<sup>4</sup> < f<sup>1</sup> when x ≥ 1. Hence this case reduce to min(f2, f4). We claim that one of f<sup>2</sup> and f<sup>4</sup> must be less than or equal to (5 + <sup>√</sup> 17)/4. To see this, suppose <sup>f</sup><sup>2</sup> <sup>≥</sup> (5 + <sup>√</sup> 17)/4, when <sup>x</sup> <sup>≥</sup> <sup>3</sup>/2 + <sup>√</sup> 13/2, this gives us <sup>3</sup>/2 + <sup>√</sup> <sup>13</sup>/<sup>2</sup> <sup>≤</sup> <sup>x</sup> <sup>≤</sup> <sup>3</sup>/2 + <sup>√</sup> <sup>17</sup>/2, then <sup>f</sup><sup>4</sup> = (1 + <sup>x</sup>)/<sup>2</sup> <sup>≤</sup> <sup>5</sup>/4 + <sup>√</sup> <sup>17</sup>/4 = (5 + <sup>√</sup> 17)/4. Similarly, suppose <sup>f</sup><sup>4</sup> <sup>≥</sup> (5 + <sup>√</sup> 17)/4, when x > <sup>3</sup>/2 + <sup>√</sup> <sup>13</sup>/2, this gives us <sup>x</sup> <sup>≥</sup> <sup>3</sup>/2 + <sup>√</sup> 17/2, then <sup>f</sup><sup>2</sup> = 1/x + 2 <sup>≤</sup> <sup>1</sup>/(3/2 + <sup>√</sup> <sup>17</sup>/2) + 2 = (5 + <sup>√</sup> 17)/4. Therefore we can conclude that when <sup>x</sup> <sup>≥</sup> <sup>3</sup>/2 + <sup>√</sup> <sup>13</sup>/2, min(f1, f2, max(f3, f4)) = min(f1, f2, f4) <sup>≤</sup> (5 + <sup>√</sup> 17)/4.

Then, consider case (ii). Recall that we have shown that f<sup>2</sup> > f<sup>3</sup> for x ≥ 1, this case reduce to min(f1, f3). We claim that one of f<sup>1</sup> and f<sup>3</sup> must be less than or equal to 1 + p 3/2. To see this, suppose f<sup>1</sup> = x ≥ 1 + p <sup>3</sup>/2, when <sup>x</sup> <sup>≥</sup> <sup>3</sup>/2 + <sup>√</sup> 13/2, otherwise we would have f<sup>1</sup> = x < 1 + p 3/2. This means that <sup>f</sup><sup>3</sup> = 2 + 1/(2x) <sup>≤</sup> 2 + 1/(2 + <sup>√</sup> 6) = 1 + p 3/2. Therefore we can conclude that when x < <sup>3</sup>/2 + <sup>√</sup> 13/2, min(f1, f2, max(f3, f4)) = min(f1, f2, f3) ≤ 1 + p 3/2.

Now, combine the above two cases together, we have that

$$
\min(f_1, f_2, \max(f_3, f_4)) \le \max(1 + \sqrt{\frac{3}{2}}, \frac{5 + \sqrt{17}}{4}) = \frac{5 + \sqrt{17}}{4}
$$

as desired.

Now, we consider the case (ii), where k ′ > 1 is odd. Similar to case (i), let QM<sup>Σ</sup> = {o 1 MΣ , o<sup>2</sup> MΣ , · · · , o (k ′−1)/2 <sup>M</sup><sup>Σ</sup> }, QΣΣ = {o 1 ΣΣ, o<sup>2</sup> ΣΣ, · · · , o (k ′+1)/2 ΣΣ }. This means that

$$
Q_{M\Sigma} = \operatorname{argmin}_{A \subseteq O_{M\Sigma} \backslash O:|A| = (k'-1)/2} \sum_{i \in C} f(i, A),
$$
  
$$
Q_{\Sigma\Sigma} = \operatorname{argmin}_{A \subseteq O_{\Sigma\Sigma} \backslash O:|A| = (k'+1)/2} \sum_{i \in C} f(i, A).
$$

<span id="page-13-0"></span>Theorem 4.4. For k ′ > 1 and odd, there always exists a facility location set A ⊆ F such that choosing A would give a 1 + p 5/3 approximation both for minimizing Max-Sum and minimizing Sum-Sum.

Proof. Similar to Theorem [4.2,](#page-10-0) first, note that by Lemma [2.2](#page-6-1) and Theorem [2.1](#page-5-3) we have that

$$
\alpha_{M\Sigma}(O_{\Sigma\Sigma}) \leq \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})} + 2
$$

which implies that we can obtain a min(αΣΣ(OMΣ), 1/(αΣΣ(OMΣ) + 2)) approximation by choosing OM<sup>Σ</sup> or OΣΣ. Now, in addition to OM<sup>Σ</sup> and OΣΣ, we will look at solution A = O ∪ QM<sup>Σ</sup> ∪ QΣΣ. Let RM<sup>Σ</sup> = OM<sup>Σ</sup> \ (QM<sup>Σ</sup> ∪ O), RΣΣ = OΣΣ \ (QΣΣ ∪ O). Note that A is composed of OM<sup>Σ</sup> and OΣΣ with no duplicate facility locations, it is a valid solution. Then, we will analyze how good it is w.r.t. to Max-Sum(OMΣ). Here note that by Lemma [4.1](#page-9-1) and k ′ ≥ 3, we have that f(i, QΣΣ) ≤ f(i, RMΣ) + <sup>2</sup> k ′+1 · f(RMΣ, QΣΣ), f(RMΣ, QΣΣ) ≤ k ′+1 2 · f(i, RMΣ) + <sup>k</sup> ′+1 2 · f(i, QΣΣ) With triangle inequality, we have that

$$
\begin{split}\n\text{MAX-SUM}(A) &= \max_{i \in \mathcal{C}} f(i, A) \\
&= \max_{i \in \mathcal{C}} f(i, O \cup Q_{M\Sigma} \cup Q_{\Sigma\Sigma}) \\
&= \max_{i \in \mathcal{C}} \left( f(i, O) + f(i, Q_{M\Sigma}) + f(i, Q_{\Sigma\Sigma}) \right) \\
&\le \max_{i \in \mathcal{C}} \left( f(i, O) + f(i, Q_{M\Sigma}) + f(i, R_{M\Sigma}) + \frac{2}{k' + 1} \cdot f(R_{M\Sigma}, Q_{\Sigma\Sigma}) \right) \\
&\le \text{MAX-SUM}(O_{M\Sigma}) + \frac{2}{k' + 1} \cdot f(R_{M\Sigma}, Q_{\Sigma\Sigma}) \\
&\le \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{n} \cdot \frac{2}{k' + 1} \left( \frac{k' + 1}{2} \cdot \sum_{i \in \mathcal{C}} f(i, R_{M\Sigma}) + \frac{k' + 1}{2} \cdot \sum_{i \in \mathcal{C}} f(i, Q_{\Sigma\Sigma}) \right) \\
&\le \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{n} \sum_{i \in \mathcal{C}} f(i, R_{M\Sigma}) + \frac{1}{n} \sum_{i \in \mathcal{C}} f(i, Q_{\Sigma\Sigma})\n\end{split}
$$

Recall that we have shown in Theorem [4.2](#page-10-0) that P <sup>i</sup>∈C <sup>f</sup>(i, RMΣ) <sup>≤</sup> <sup>n</sup>·Max-Sum(OMΣ). In addition, we have that

$$
\sum_{i \in \mathcal{C}} f(i, Q_{\Sigma \Sigma}) = \frac{k' + 1}{2k'} \sum_{i \in \mathcal{C}} f(i, Q_{\Sigma \Sigma}) + \frac{k' - 1}{2k'} \sum_{i \in \mathcal{C}} f(i, Q_{\Sigma \Sigma})
$$

Now, recall that we have QΣΣ = argminA⊆OΣΣ\O:|A|=(<sup>k</sup> ′−1)/2 P <sup>i</sup>∈C f(i, A), this means that

$$
\sum_{i \in \mathcal{C}} f(i, Q_{\Sigma \Sigma}) \le \frac{k' + 1}{k' - 1} \sum_{i \in \mathcal{C}} f(i, R_{\Sigma \Sigma}).
$$

We can then see that

$$
\sum_{i \in \mathcal{C}} f(i, Q_{\Sigma \Sigma}) \le \frac{k' + 1}{2k'} \sum_{i \in \mathcal{C}} f(i, Q_{\Sigma \Sigma}) + \frac{k' - 1}{2k'} \cdot \frac{k' + 1}{k' - 1} \sum_{i \in \mathcal{C}} f(i, R_{\Sigma \Sigma})
$$
  
=  $\frac{k' + 1}{2k'} \sum_{i \in \mathcal{C}} f(i, O_{\Sigma \Sigma} \setminus O)$   
 $\le \frac{k' + 1}{2k'} \sum_{i \in \mathcal{C}} f(i, O_{\Sigma \Sigma})$   
 $\le \frac{k' + 1}{2k'} \text{SUM-SUM}(O_{\Sigma \Sigma})$ 

Now, since we have that P <sup>i</sup>∈C f(i, QΣΣ) ≤ k ′+1 2k ′ Sum-Sum(OΣΣ) and P <sup>i</sup>∈C <sup>f</sup>(i, RMΣ) <sup>≤</sup> <sup>n</sup>·Max-Sum(OMΣ), we can derive that

$$
\begin{split} \text{MAX-SUM}(A) &\leq \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{n} \sum_{i\in\mathcal{C}} f(i, R_{M\Sigma}) + \frac{1}{n} \sum_{i\in\mathcal{C}} f(i, Q_{\Sigma\Sigma}) \\ &\leq \text{MAX-SUM}(O_{M\Sigma}) + \frac{1}{n} \cdot n \cdot \text{MAX-SUM}(O_{M\Sigma}) + \frac{k' + 1}{2k'n} \text{SUM-SUM}(O_{\Sigma\Sigma}) \\ &= 2 \cdot \text{MAX-SUM}(O_{M\Sigma}) + \frac{k' + 1}{2k'n} \cdot \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})} \text{SUM-SUM}(O_{M\Sigma}) \\ &\leq 2 \cdot \text{MAX-SUM}(O_{M\Sigma}) + \frac{k' + 1}{2k'n} \cdot \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})} \cdot \frac{n}{1} \text{MAX-SUM}(O_{M\Sigma}) \\ &= \left(2 + \frac{k' + 1}{2k'} \cdot \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})}\right) \text{MAX-SUM}(O_{M\Sigma}) \end{split}
$$

Now, since k ′ ≥ 3, k ′+1 2k ′ ≤ 2 3 , we can observe that

$$
\begin{aligned} \text{Max-Sum}(A) &\leq \left(2 + \frac{k' + 1}{2k'} \cdot \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})}\right) \text{Max-Sum}(O_{M\Sigma}) \\ &\leq \left(2 + \frac{2}{3\alpha_{\Sigma\Sigma}(O_{M\Sigma})}\right) \text{Max-Sum}(O_{M\Sigma}) \end{aligned}
$$

Note that we are calculating the approximation ratio for both Max-Sum and Sum-Sum and it could be the case that one is worse than the other. Since the final simultaneous approximation ratio would be the worse of the two, next, we will examine how good A is w.r.t. Sum-Sum(OΣΣ).

$$
Sum-SUM(A) = \sum_{i \in C} f(i, A)
$$
  
= 
$$
\sum_{i \in C} f(i, Q_{M\Sigma}) + \sum_{i \in C} f(i, Q_{\Sigma\Sigma}) + \sum_{i \in C} f(i, O)
$$

Here note that we have previously showed that P <sup>i</sup>∈C f(i, QΣΣ) ≤ k ′+1 2k ′ P <sup>i</sup>∈C f(i, OΣΣ \O), using the same argument we also have that P <sup>i</sup>∈C f(i, QMΣ) ≤ k ′−1 2k ′ P <sup>i</sup>∈C f(i, OM<sup>Σ</sup> \ O). Therefore,

$$
Sum-SUM(A) \leq \frac{k'-1}{2k'} \sum_{i \in C} f(i, O_{M\Sigma} \setminus O) + \frac{k'+1}{2k'} \sum_{i \in C} f(i, O_{\Sigma\Sigma} \setminus O) + \sum_{i \in C} f(i, O)
$$
  
= 
$$
\frac{k'-1}{2k'} \text{SUM-SUM}(O_{M\Sigma}) + \frac{k'+1}{2k'} \text{SUM-SUM}(O_{\Sigma\Sigma})
$$
  
= 
$$
\frac{k'-1}{2k'} \alpha_{\Sigma\Sigma}(O_{M\Sigma}) \cdot \text{SUM-SUM}(O_{\Sigma\Sigma}) + \frac{k'+1}{2k'} \text{SUM-SUM}(O_{\Sigma\Sigma})
$$
  
= 
$$
\frac{k'+1+(k'-1)\alpha_{\Sigma\Sigma}(O_{M\Sigma})}{2k'} \cdot \text{SUM-SUM}(O_{\Sigma\Sigma})
$$

Recall that we have αΣΣ(OMΣ) ≥ 1, so we have that

$$
SUM-SUM(A) \leq \frac{k' + k'\alpha_{\Sigma\Sigma}(O_{M\Sigma}) + (1 - \alpha_{\Sigma\Sigma}(O_{M\Sigma}))}{2k'} \cdot SUM-SUM(O_{\Sigma\Sigma})
$$
  
$$
\leq \frac{k' + k'\alpha_{\Sigma\Sigma}(O_{M\Sigma})}{2k'} \cdot SUM-SUM(O_{\Sigma\Sigma})
$$
  
$$
= \frac{1 + \alpha_{\Sigma\Sigma}(O_{M\Sigma})}{2} \cdot SUM-SUM(O_{\Sigma\Sigma})
$$

![](_page_15_Figure_0.jpeg)

<span id="page-15-1"></span>Figure 2: An instance with two client locations A, B and three possible facility locations A, B, C with distances between any two adjacent locations as well as the number of clients and/or facilities at each location labeled.

<span id="page-15-2"></span>

|               | OΣΣ<br>√                       | OMΣ      | H<br>√           |
|---------------|--------------------------------|----------|------------------|
| Sum-Sum       | (<br>2<br>−<br>1)n<br>+ 1<br>√ | n        | 2<br>·<br>n<br>√ |
| Max-Sum       | 1 +<br>2                       | 1<br>√   | 2<br>√           |
| αΣΣ, n<br>→ ∞ | 1<br>√                         | 1 +<br>2 | 2 +<br>2<br>√    |
| → ∞<br>αMΣ, n | 1 +<br>2                       | 1        | 2                |

Table 1: An instance for Theorem [4.5,](#page-15-0)with Sum-Sum, Max-Sum, αΣΣ, αM<sup>Σ</sup> listed for OΣΣ, OM<sup>Σ</sup> and H.

Hence we can conclude that choosing A is a max 2 + <sup>2</sup> 3αΣΣ(OMΣ) , 1+αΣΣ(OMΣ) 2 approximation of Max-Sum and Sum-Sum. Then, this means that one of OMΣ, OΣΣ, A would give us a

$$
\min \left( \alpha_{\Sigma\Sigma}(O_{\scriptscriptstyle M\Sigma}), \frac{1}{\alpha_{\Sigma\Sigma}(O_{\scriptscriptstyle M\Sigma})} + 2, \max \left( 2 + \frac{2}{3\alpha_{\Sigma\Sigma}(O_{\scriptscriptstyle M\Sigma})} , \frac{1 + \alpha_{\Sigma\Sigma}(O_{\scriptscriptstyle M\Sigma})}{2} \right) \right)
$$

approximation of Max-Sum and Sum-Sum. Finally, similar to the proof for Lemma [4.3,](#page-12-0) we have that one of OMΣ, OΣΣ, A would give us at most a 1 + p 5/3 approximation for both objectives.

Therefore, with Theorem [4.2](#page-10-0) and Theorem [4.4,](#page-13-0) we can conclude that the upper bound of approximation ratio for both Max-Sum and Sum-Sum when k ′ <sup>&</sup>gt; <sup>1</sup> is (5 + <sup>√</sup> 17)/4 when k ′ is even and it is 1 + p 5/3 when k ′ is odd.

Finally, we consider the remaining case, when k ′ = 1. In this case OΣΣ and OM<sup>Σ</sup> overlap except for one location. We can show that for k > <sup>2</sup> the upper bound is much better than 1 + <sup>√</sup> 2 for approximating both <sup>O</sup>M<sup>Σ</sup> and <sup>O</sup>ΣΣ. However, the 1 + <sup>√</sup> 2 bound is still tight when k = 2.

<span id="page-15-0"></span>Theorem 4.5. No deterministic algorithm can approximate both Max-Sum and Sum-Sum simultaneously within a factor of better than 1 + <sup>√</sup> 2 when k = 2.

Proof. Consider the case where there are four locations on a line as shown in Figure [2.](#page-15-1) There is one client on location A, n − 1 clients on location B, one facility location at A, one facility location at B and one facility location at C (i.e. the multiset of facility locations is F = {A, B, C}). We also have that <sup>d</sup>(A, B) = 1, <sup>d</sup>(C, B) = <sup>√</sup> 2 − 1. Note that in this case OM<sup>Σ</sup> is taking facilities on A, B, OΣΣ is taking facilities on B, C, and we also consider H = {A, C}, which consists one facility in OΣΣ and one in OMΣ. We obtain the values as shown in Table [1.](#page-15-2)

By choosing either <sup>O</sup>ΣΣ or <sup>O</sup>M<sup>Σ</sup> gives us a 1+<sup>√</sup> 2 for approximating both objectives and choosing H gives a worse approximation ratio. Therefore, there does not exist a solution that would give an approximation ratio better than 1 + <sup>√</sup> 2.

Next, to show that we can obtain a better upper bound for k > 2, we will first show a more general result.

<span id="page-16-0"></span>Theorem 4.6. For k ′ ≥ 1, given the optimal facility set OM<sup>Σ</sup> that minimizes Max-Sum and the optimal facility set OΣΣ that minimizes Sum-Sum, we have that αMΣ(OΣΣ) ≤ 1+ <sup>k</sup> ′ k−k ′ +max 1, k ′ k−k ′ · 1 αΣΣ(OMΣ) .

Proof. First, let A = OM<sup>Σ</sup> \ O, C = OΣΣ \ O. Note that we also have |A| = |C| = k ′ , |O| = k − k ′ . With triangle inequality, we have that

$$
\begin{aligned} \text{MAX-SUM}(O_{\Sigma\Sigma}) &= \max_{i \in \mathcal{C}} f(i, O \cup C) \\ &= \max_{i \in \mathcal{C}} \left( f(i, O) + f(i, C) \right) \end{aligned}
$$

Now, by Lemma [4.1,](#page-9-1) we have that f(i, C) ≤ k ′ k−k ′ f(i, O) + <sup>1</sup> k−k ′ f(O, C), f(O, C) ≤ k ′ · f(i, O) + (k − k ′ )f(i, C) for any i ∈ C, so we can see that

$$
\begin{split}\n\text{MAX-SUM}(O_{\Sigma\Sigma}) &\leq \max_{i\in\mathcal{C}} \left( f(i,O) + \frac{k'}{k-k'} f(i,O) + \frac{1}{k-k'} f(O,C) \right) \\
&\leq \left( 1 + \frac{k'}{k-k'} \right) \text{MAX-SUM}(O_{\text{MZ}}) + \frac{1}{k-k'} f(O,C) \\
&\leq \left( 1 + \frac{k'}{k-k'} \right) \text{MAX-SUM}(O_{\text{MZ}}) + \frac{1}{k-k'} \cdot \frac{1}{n} \left( k' \cdot \sum_{i\in\mathcal{C}} f(i,O) + (k-k') \sum_{i\in\mathcal{C}} f(i,C) \right) \\
&\leq \left( 1 + \frac{k'}{k-k'} \right) \text{MAX-SUM}(O_{\text{MZ}}) + \max \left( 1, \frac{k'}{k-k'} \right) \frac{1}{n} \left( \sum_{i\in\mathcal{C}} f(i,O) + \sum_{i\in\mathcal{C}} f(i,C) \right) \\
&\leq \left( 1 + \frac{k'}{k-k'} \right) \text{MAX-SUM}(O_{\text{MZ}}) + \max \left( 1, \frac{k'}{k-k'} \right) \frac{1}{n} \cdot \text{SUM-SUM}(O_{\text{MZ}}) \\
&\leq \left( 1 + \frac{k'}{k-k'} \right) \text{MAX-SUM}(O_{\text{MZ}}) + \max \left( 1, \frac{k'}{k-k'} \right) \frac{1}{n} \cdot \frac{\text{SUM-SUM}(O_{\text{MZ}})}{\alpha_{\Sigma\Sigma}(O_{\text{MZ}})} \\
&\leq \left( 1 + \frac{k'}{k-k'} \right) \text{MAX-SUM}(O_{\text{MZ}}) + \max \left( 1, \frac{k'}{k-k'} \right) \frac{n}{n} \cdot \frac{\text{MAX-SUM}(O_{\text{MZ}})}{\alpha_{\Sigma\Sigma}(O_{\text{MZ}})}\n\end{split}
$$

Then, we divide both sides by Max-Sum(OMΣ), which gives us

$$
\alpha_{M\Sigma}(O_{\Sigma\Sigma}) \le 1 + \frac{k'}{k - k'} + \max\left(1, \frac{k'}{k - k'}\right) \cdot \frac{1}{\alpha_{\Sigma\Sigma}(O_{M\Sigma})}
$$

<span id="page-16-1"></span>Corollary 4.6.1. Let a = k ′ k−k ′ . For a ≤ 1, there always exists a facility set A ⊆ F such that choosing A would give a ( √ a <sup>2</sup> + 2a + 5 + a + 1)/2 approximation both for minimizing Max-Sum and minimizing Sum-Sum. In fact, we would either get a (1,( √ a <sup>2</sup> + 2a + 5+a+1)/2) approximation by choosing <sup>O</sup>M<sup>Σ</sup> or a ((<sup>√</sup> a <sup>2</sup> + 2a + 5+a+ 1)/2, 1) approximation by choosing OΣΣ. In other words, at least one of αΣΣ(OMΣ) or αMΣ(OΣΣ) is always less than or equal to ( √ a <sup>2</sup> + 2a + 5 + a + 1)/2.

Proof. First note that since a ≤ 1, by Theorem [4.6,](#page-16-0) we have that αMΣ(OΣΣ) ≤ 1 + a + 1 αΣΣ(OMΣ) . We will show that max(min(αΣΣ(OMΣ), αMΣ(OΣΣ))) ≤ ( √ a <sup>2</sup> + 2a + 5 + a + 1)/2. To do this, we will consider two cases. First, assume αΣΣ(OMΣ) > ( √ a <sup>2</sup> + 2a + 5 + a + 1)/2, then we would have αMΣ(OΣΣ) ≤ 1 + a + 1 <sup>α</sup>ΣΣ(OMΣ) < 1 + a + <sup>√</sup> 2 a <sup>2</sup>+2a+5+a+1 = (<sup>√</sup> a <sup>2</sup> + 2a + 5 + a + 1)/2. Otherwise, we would have αΣΣ(OMΣ) ≤ ( √ a <sup>2</sup> + 2a + 5 + a + 1)/2. Therefore, we can conclude that

<span id="page-17-1"></span>![](_page_17_Figure_0.jpeg)

Figure 3: An instance with two client locations A, C and two possible facility locations B, D such that B and D can both be chosen 3 times individually, with distances between any two adjacent locations as well as the number of clients or facilities at each location labeled.

max(min(αΣΣ(OMΣ), αMΣ(OΣΣ))) ≤ ( √ a <sup>2</sup> + 2a + 5 + a + 1)/2. This result implies that one of αMΣ(OΣΣ) and αΣΣ(OMΣ) is less than or equal to ( √ a <sup>2</sup> + 2a + 5 + a + 1)/2, as desired.

<span id="page-17-0"></span>Corollary 4.6.2. For k ′ = 1, k ≥ 3, there always exists a facility location set A ⊆ F such that choosing A would give a 2 approximation both for minimizing Max-Sum and minimizing Sum-Sum.

Proof. Let a = 1 k−1 , note that since k ≥ 3, we have a ≤ 1 <sup>2</sup> < 1. Then, by Corollary [4.6.1,](#page-16-1) we have that there always exists a facility location A ∈ F such that choosing A would give a ( √ a <sup>2</sup> + 2a + 5 +a+ 1)/2 approximation both for minimizing Max-Sum and minimizing Sum-Sum. Then, we have that

$$
\frac{\sqrt{a^2+2a+5}+a+1}{2} \le \frac{\sqrt{(1/2)^2+2\cdot(1/2)+5}+(1/2)+1}{2} = 2
$$

as desired.

This means that if we are choosing more than 2 facility locations, if the optimal solution for Max-Sum and Sum-Sum are the same except for one location, then either OM<sup>Σ</sup> or OΣΣ is a 2 approximation for both objectives. In addition, since 2 is smaller than the approximation ratio we found in Theorem [4.2](#page-10-0) and Theorem [4.4,](#page-13-0) these two theorems hold for any k ′ when k > 2 (note that k ′ = 0 implies that OM<sup>Σ</sup> = OΣΣ, therefore it's a 1 approximation for both objectives by choosing OΣΣ).

Recall that by Corollary [2.1.1,](#page-6-2) using previous results and choosing either OM<sup>Σ</sup> or OΣΣ, we cannot get an approximation ratio better than 1 + <sup>√</sup> 2 ≈ 2.41. By choosing the best of these two solutions together with A = O ∪ QM<sup>Σ</sup> ∪ QΣΣ, however, we are able to use Theorems [4.2,](#page-10-0) [4.4,](#page-13-0) and [4.6.2](#page-17-0) to improve this bound to max{(5+<sup>√</sup> 17)/4, 1+p 5/3, 2} = 1+p 5/3 ≈ 2.29 for any k ≥ 3. In addition, we can show that when we are choosing more than 2 facilities, the lower bound is (4+<sup>√</sup> 7)/3 ≈ 2.215.

<span id="page-17-2"></span>Theorem 4.7. No deterministic algorithm can approximate both Max-Sum and Sum-Sum simultaneously within a factor of better than (4 + <sup>√</sup> 7)/3 for k ≥ 3.

Proof. Consider the case where there are four locations on a line as shown in Figure [3.](#page-17-1) There is one client on location A, n − 1 clients on location C, three facility locations at B and three facility locations at D (i.e. the multiset of facility locations is F = {B, B, B, D, D, D}). We also have that <sup>d</sup>(A, B) = <sup>d</sup>(B, C) = 1, <sup>d</sup>(C, D) = (<sup>√</sup> 7 − 2)/3. Note that in this case OM<sup>Σ</sup> is taking all facilities on B, OΣΣ is taking all facilities on D, and we also consider H which consists one facility in OΣΣ and two in OM<sup>Σ</sup> and H′ which consists two facility in OΣΣ and one in OMΣ. We obtain the values as shown in Table [2.](#page-18-0)

Note that in this case, the only options of locations are both facilities on B (OMΣ), both facilities on D (OΣΣ) and a mixture of facilities on B and D (H and H′ ). By choosing either OΣΣ or H

<span id="page-18-0"></span>

|               | OΣΣ                                 | OMΣ      | H<br>√                            | H′                                 |
|---------------|-------------------------------------|----------|-----------------------------------|------------------------------------|
| Sum-Sum       | √<br>(<br>7<br>−<br>2)n<br>+ 6<br>√ | 3n       | <br><br>2<br>7−1<br>n<br>+ 6<br>3 | √<br><br>7+4<br>n<br>+ 2<br>3<br>√ |
| Max-Sum       | 4 +<br>7                            | 3<br>√   | 11+2√<br>7<br>3                   | 2 +<br>7                           |
| αΣΣ, n<br>→ ∞ | 1                                   | 2 +<br>7 | 4+√<br>7<br>3                     | 5+2√<br>7<br>3                     |
| → ∞<br>αMΣ, n | 4+√<br>7<br>3                       | 1        | 11+2√<br>7<br>9                   | 2+√<br>7<br>3                      |

Table 2: An instance for Theorem [4.7,](#page-17-2) with Sum-Sum, Max-Sum, αΣΣ, αM<sup>Σ</sup> listed for OΣΣ, OMΣ, H and H′ .

<span id="page-18-1"></span>![](_page_18_Figure_2.jpeg)

Figure 4: An instance with two client locations A, C and two possible facility locations B, D such that both B and D can be chosen k times individually, with distances between any two adjacent locations as well as the number of clients or facilities at each location labeled.

gives us a (4 + <sup>√</sup> 7)/3 for approximating both objectives and choosing OM<sup>Σ</sup> or H′ gives a worse approximation ratio. Therefore, there does not exist a solution that would give an approximation ratio better than (4 + <sup>√</sup> 7)/3 when k ≥ 3.

### 4.2 Sum-Max and Max-Max

By Lemma [2.3](#page-6-3) and Corollary [2.1.1,](#page-6-2) there exists a solution that is a 1 + <sup>√</sup> 2 approximation for both Sum-Max and Max-Max. Unfortunately, unlike what we showed for Max-Sum and Sum-Sum in the previous section, the best possible upper bound for simultaneously approximating Max-Max and Sum-Max is still 1 + <sup>√</sup> 2, even for larger values of k.

Theorem 4.8. For any deterministic algorithm, there does not exist an approximation bound better than 1 + <sup>√</sup> 2 that simultaneously approximates Max-Max and Sum-Max.

Proof. Consider the case where there are four locations on a line as shown in Figure [4.](#page-18-1) There is one client on location A, n − 1 clients on location C, k facility locations at B and k facility locations at D (i.e the multiset of facility locations is F = {B, . . . , B | {z } k , D, . . . , D | {z } k }). We also have

that <sup>d</sup>(A, B) = <sup>d</sup>(B, C) = 1, <sup>d</sup>(C, D) = <sup>√</sup> 2 − 1. Note that in this case OMM is taking both facilities on B, OΣ<sup>M</sup> is taking both facilities on D, and we also consider H which consists at least one facility in O<sup>n</sup> and at least one facility in O1. We obtain the values as shown in Table [3.](#page-18-2)

<span id="page-18-2"></span>

|         | OΣM<br>√                       | OMM | H      |
|---------|--------------------------------|-----|--------|
| Sum-Max | (<br>2<br>−<br>1)n<br>+ 2<br>√ | n   | ∗<br>√ |
| Max-Max | 1 +                            | 1   | 1 +    |
|         | 2                              | √   | 2      |
| αΣM, n  | 1                              | 1 + | *      |
| → ∞     | √                              | 2   | √      |
| → ∞     | 1 +                            | 1   | 1 +    |
| αMM, n  | 2                              |     | 2      |

Table 3: An instance for Theorem [4.8,](#page-18-2)with Max-Max, Sum-Max, αMM, αΣ<sup>M</sup> listed for OMM, OΣ<sup>M</sup> and H. Note that \* means that the value varies by the choice of H.

Note that in this case, the only options of locations are both facilities on B (OMM), both facilities on D (OΣM) and a combination of at least one facility on B and at least one facility on D (H). By choosing any of <sup>O</sup>ΣM, <sup>O</sup>MM or <sup>H</sup> gives us at least a 1 + <sup>√</sup> 2 approximation for approximating both objectives. Therefore, there does not exist a solution that would give an approximation ratio better than 1 + <sup>√</sup> 2.

## <span id="page-19-0"></span>4.3 Max-Max and Max-Sum

Until now, we have only looked at pairs of objectives that have the same individual cost function for each client but different ones for the overall cost (max and sum respectively). We now consider pairs of objectives that have different individual cost functions for each client but the same cost function for the overall cost. Note that in this case, when k = 1, the two objectives would become exactly the same. For example, consider Max-Max and Max-Sum: when k = 1, both would become minimax, and thus would have the same optimum solution. Because of these differences, previous results from Section [2](#page-5-4) do not apply to this set of objectives, and different techniques are required to form a good approximation for both objectives. We will first consider Max-Max and Max-Sum. Let OM<sup>Σ</sup> be the optimal solution for Max-Sum, and OMM be the optimal solution for Max-Max.

Definition 4.2. We define k1, k<sup>2</sup> as follows:

- 1. k<sup>1</sup> · maxi∈C maxa∈OM<sup>Σ</sup> d(i, a) = maxi∈C P a∈OM<sup>Σ</sup> d(i, a)
- <span id="page-19-1"></span>2. k<sup>2</sup> · maxi∈C maxa∈OMM d(i, a) = maxi∈C P a∈OMM d(i, a)

Note that we have 1 ≤ k<sup>1</sup> ≤ k, 1 ≤ k<sup>2</sup> ≤ k.

First, we make the following simple observation.

<span id="page-19-2"></span>Theorem 4.9. For any k, given the optimal facility set OMM that minimizes Max-Max and the optimal facility set OM<sup>Σ</sup> that minimizes Max-Sum, we have that αMΣ(OMM) · αMM(OMΣ) = <sup>k</sup><sup>2</sup> k1 .

Proof. By Definition [4.2,](#page-19-1) we have that

$$
\max_{i \in \mathcal{C}} \max_{a \in O_{M\Sigma}} d(i, a) = \frac{1}{k_1} \max_{i \in \mathcal{C}} \sum_{a \in O_{M\Sigma}} d(i, a)
$$
$$
= \frac{1}{k_1 \cdot \alpha_{M\Sigma}(O_{MM})} \max_{i \in \mathcal{C}} \sum_{a \in O_{MM}} d(i, a)
$$
$$
= \frac{k_2}{k_1 \cdot \alpha_{M\Sigma}(O_{MM})} \max_{i \in \mathcal{C}} \max_{a \in O_{MM}} d(i, a)
$$

Then, divide both sides by maxi∈C maxa∈OMM d(i, a), we have that

$$
\alpha_{MM}(O_{M\Sigma}) = \frac{k_2}{k_1 \cdot \alpha_{M\Sigma}(O_{MM})}
$$
$$
\alpha_{MM}(O_{M\Sigma}) \cdot \alpha_{M\Sigma}(O_{MM}) = \frac{k_2}{k_1}
$$

as desired.

<span id="page-20-1"></span>Corollary 4.9.1. For any k, there always exists a facility location set A ⊆ F such that choosing A would give a p k2/k<sup>1</sup> approximation both for minimizing Max-Max and minimizing Max-Sum. In fact, we would either get a (1, p k2/k1) approximation by choosing OMM or a ( p k2/k1, 1) approximation by choosing OMΣ. In other words, at least one of αMΣ(OMM) or αMM(OMΣ) is always less than or equal to p k2/k1.

Proof. To show the above result, we will consider two cases. First, assume αMΣ(OMM) > p k2/k1, then we would have αMM(OMΣ) = <sup>k</sup>2/k<sup>1</sup> <sup>α</sup>MΣ(OMM) ≤ p k2/k<sup>1</sup> by Theorem [4.9.](#page-19-2) Otherwise, we would have αMΣ(OMM) ≤ p k2/k1. Thus, we can conclude that at least one of αMΣ(OMM) or αMM(OMΣ) is always less than or equal to p k2/k1.

p Recall that by Definition [4.2,](#page-19-1) we have that 1 ≤ k<sup>1</sup> ≤ k, 1 ≤ k<sup>2</sup> ≤ k. Therefore, we can see that k2/k<sup>1</sup> ≤ p k/1 = <sup>√</sup> k. Hence, we can also conclude the following corollary.

<span id="page-20-0"></span>Corollary 4.9.2. For any k, there always exists a facility location set A ⊆ F such that choosing <sup>A</sup> would give a <sup>√</sup> k approximation both for minimizing Max-Max and minimizing Max-Sum. In fact, we would either get a (1, √ k) approximation by choosing OMM or a ( √ k, 1) approximation by choosing OMΣ. In other words, at least one of αMΣ(OMM) or αMM(OMΣ) is always less than or equal to <sup>√</sup> k.

While Corollary [4.9.2](#page-20-0) gives a good upper bound for the simultaneous approximation ratio for Max-Max and Max-Sum when k ∈ {1, 2, 3, 4}, we can actually show that the upper bound is at most 2 for larger k. To show this, we first observe the following lemmas. Let (i ∗ , b<sup>∗</sup> ) = argmaxi∈C,b∈OM<sup>Σ</sup> d(i, b), (j ∗ , a<sup>∗</sup> ) = argmaxj∈C,a∈OMM d(j, a), B = OM<sup>Σ</sup> \ {b <sup>∗</sup>}.

<span id="page-20-2"></span>Lemma 4.10. For any j ∈ C, we have that

(k − 2)d(j, b<sup>∗</sup> ) ≥ (k − 2k1)d(i ∗ , b<sup>∗</sup> ).

Proof. By triangle inequality, for any b ∈ B, we have that

$$
d(i^*,b^*) \le d(i^*,b) + d(j,b) + d(j,b^*)
$$

Therefore, we can see that

$$
(k-1)d(i^*,b^*) \leq \sum_{b \in B} d(i^*,b) + \sum_{b \in B} d(j,b) + (k-1)d(j,b^*)
$$

Recall that (i ∗ , b<sup>∗</sup> ) = argmaxi∈C,b∈OM<sup>Σ</sup> d(i, b). By Definition [4.2,](#page-19-1) this means that for any j ∈ C, we have that

$$
\sum_{b \in O_{M\Sigma}} d(j, b) \le \max_{i \in C} \sum_{b \in O_{M\Sigma}} d(i, b) = k_1 \cdot d(i^*, b^*)
$$

Hence, we have that

$$
\sum_{b \in O_{M\Sigma}} d(i^*, b) \le k_1 \cdot d(i^*, b^*)
$$
  
$$
\sum_{b \in B} d(i^*, b) \le (k_1 - 1) \cdot d(i^*, b^*)
$$

Now, with the above results, we have that

$$
(k-1)d(i^*,b^*) \le \sum_{b \in B} d(i^*,b) + \sum_{b \in B} d(j,b) + (k-1)d(j,b^*)
$$
  
\n
$$
\le (k_1 - 1)d(i^*,b^*) + \sum_{b \in O_{M\Sigma}} d(j,b) + (k-2)d(j,b^*)
$$
  
\n
$$
\le (k_1 - 1)d(i^*,b^*) + k_1 \cdot d(i^*,b^*) + (k-2)d(j,b^*)
$$
  
\n
$$
\le (2k_1 - 1)d(i^*,b^*) + (k-2)d(j,b^*)
$$

Then, rearrange the above inequality, we have that

$$
(k-2)d(j,b^*) \ge (k-2k_1)d(i^*,b^*)
$$

as desired.

<span id="page-21-0"></span>Lemma 4.11. For any a /∈ OMΣ, there must exist some j such that d(j, b<sup>∗</sup> ) ≤ d(j, a).

Proof. Let A = {a}∪B. First, note that since A is not the optimal solution (or, it is also an optimal solution but A ̸= OMΣ) for Max-Sum, there must exist some j ∈ C such that

$$
\sum_{p \in O_{M\Sigma}} d(j, p) \le \sum_{p \in A} d(j, p)
$$

To see why this is true, assume otherwise, P p∈OM<sup>Σ</sup> d(j, p) > P p∈A d(j, p) for all j ∈ C, this means that maxj∈C P p∈OM<sup>Σ</sup> d(j, p) > maxj∈C P p∈A d(j, p), but since OM<sup>Σ</sup> is the optimal solution for Max-Sum, this is a contradiction. Therefore, we can see that

$$
\sum_{p \in O_{M\Sigma}} d(j, p) \le \sum_{p \in A} d(j, p)
$$
$$
d(j, b^*) + \sum_{p \in B} d(j, p) \le d(j, a) + \sum_{p \in B} d(j, p)
$$
$$
d(j, b^*) \le d(j, a)
$$

as desired.

Finally, we can conclude with the following theorem.

<span id="page-21-1"></span>Theorem 4.12. For any k, there always exists a facility location set A ⊆ F such that choosing A would give a 2 approximation both for minimizing Max-Max and minimizing Max-Sum. In fact, we would either get a (1, 2) approximation by choosing OMM or a (2, 1) approximation by choosing OMΣ. In other words, at least one of αMM(OMΣ) or αMΣ(OMM) is always less than or equal to 2.

Proof. We claim that min(αMM(OMΣ), αMΣ(OMM)) ≤ 2. To show this, recall that by Corollary [4.9.1](#page-20-1) and Definition [4.2,](#page-19-1) we have that min(αMM(OMΣ), αMΣ(OMM)) ≤ p k2/k<sup>1</sup> ≤ p k/k1. Then, we consider two cases. First, consider the case where k − 2k<sup>1</sup> ≤ 0, k ≤ 2k1, this means that min(αMM(OMΣ), αMΣ(OMM)) ≤ p k/k<sup>1</sup> ≤ p 2k1/k<sup>1</sup> = √ 2 < 2. Next, we consider the case where k − 2k<sup>1</sup> > 0, k > 2k1. Assume OMM ̸= OMΣ, otherwise choosing OM<sup>Σ</sup> would be a 1 approximation

for both objectives. This means that there exists some a ∈ OMM, a /∈ OMΣ. Here note that by Lemma [4.11](#page-21-0) and Lemma [4.10,](#page-20-2) there exist some j ∈ C such that

$$
d(i^*, b^*) \le \frac{k-2}{k-2k_1} d(j, b^*)
$$
  
\n
$$
\le \frac{k-2}{k-2k_1} d(j, a)
$$
  
\n
$$
\le \frac{k-2}{k-2k_1} d(j^*, a^*)
$$

Now, divide both side by d(j ∗ , a<sup>∗</sup> ), we have that αMM(OMΣ) ≤ k−2 k−2k<sup>1</sup> , min(αMM(OMΣ), αMΣ(OMM)) ≤ min q <sup>k</sup> k1 , k−2 k−2k<sup>1</sup> . Let k<sup>1</sup> · x = k > 2k1, x > 2. We then have

$$
\min(\alpha_{MM}(O_{M\Sigma}), \alpha_{M\Sigma}(O_{MM})) \le \min\left(\sqrt{\frac{k_1 \cdot x}{k_1}}, \frac{k_1 \cdot x - 2}{k_1 \cdot x - 2k_1}\right)
$$
$$
= \min\left(\sqrt{x}, \frac{k_1 \cdot x - 2}{k_1(x - 2)}\right)
$$
$$
= \min\left(\sqrt{x}, \frac{x}{x - 2} - \frac{2}{k_1(x - 2)}\right)
$$
$$
\le \min\left(\sqrt{x}, \frac{x}{x - 2}\right)
$$

The last inequality holds since k<sup>1</sup> ≥ 1, x > 2. We then consider two cases. First, consider the case x > 4, this means that <sup>x</sup> <sup>x</sup>−<sup>2</sup> <sup>&</sup>lt; <sup>2</sup>. Next, consider the case where <sup>x</sup> <sup>≤</sup> <sup>4</sup>, we have that <sup>√</sup> x ≤ 2. Therefore, we can conclude that min(αMM(OMΣ), αMΣ(OMM)) <sup>≤</sup> min <sup>√</sup> x, <sup>x</sup> x−2 ≤ 2. Finally, since both cases give the result min(αMM(OMΣ), αMΣ(OMM)) ≤ 2, we can see that at least one of αMM(OMΣ) or αMΣ(OMM) is always less than or equal to 2 as desired.

Combining Corollary [4.9.2](#page-20-0) and Theorem [4.12](#page-21-1) together, we can conclude that there always exists a solution that is a min(<sup>√</sup> k, 2) approximation for both Max-Max and Max-Sum. In fact, we can show that this bound is tight when k = 2.

<span id="page-22-0"></span>Theorem 4.13. When k = 2, no deterministic algorithm can approximate both Max-Max and Max-Sum simultaneously within a factor of better than <sup>√</sup> 2.

Proof. Consider the case where there are four locations on a triangle as shown in Figure [5.](#page-23-0) There is one client on location C, one client on location D, two facility locations at A, one facility location at C and one facility location at D (i.e. the multiset of facility locations is F = {A, A, C, D}). We also have that <sup>d</sup>(A, C) = <sup>d</sup>(A, D) = <sup>√</sup> 2/2, d(C, D) = 1. Note that in this case OMM is taking both facilities on A, OM<sup>Σ</sup> is taking facilities on C and D. W.l.o.g, we also consider solution H which chooses a facility on location A and a facility C. We obtain the values as shown in Table [4.](#page-23-1)

Note that in this case, the only options of locations are both facilities on A (OMM), facilities on C, D (OMΣ) and one facility on <sup>A</sup> and one on <sup>C</sup> or <sup>D</sup> (H). By choosing <sup>O</sup>MM, <sup>O</sup>M<sup>Σ</sup> gives us a <sup>√</sup> 2 for approximating both objectives and choosing H would lead to a worse result. Therefore, there does not exist a solution that would give an approximation ratio better than <sup>√</sup> 2.

![](_page_23_Figure_0.jpeg)

Figure 5: An instance with two client locations C, D and three possible facility locations A, C, D such that C and D can be chosen once but A can be chosen twice, with distances between any two adjacent locations as well as the number of clients and/or facilities at each location labeled.

<span id="page-23-1"></span><span id="page-23-0"></span>

|         | OMΣ    | OMM<br>√    | H<br>√             |
|---------|--------|-------------|--------------------|
| Max-Sum | 1      | 2<br>√      | 2<br>1 +<br>2      |
| Max-Max | 1      | 2<br>2<br>√ | 1<br>√             |
| αMΣ     | 1<br>√ | 2           | 2<br>1 +<br>√<br>2 |
| αMM     | 2      | 1           | 2                  |

Table 4: An instance for Theorem [4.13,](#page-22-0)with Max-Max, Max-Sum, αMM, αM<sup>Σ</sup> listed for OMM, OM<sup>Σ</sup> and H.

### 4.4 Sum-Sum and Sum-Max

Lastly, we look at Sum-Sum and Sum-Max. As we have have discussed in Section [4.3,](#page-19-0) note that when k = 1, both objectives become the same (minisum) and previous results from Section [2](#page-5-4) do not apply to this set of objectives. Let OΣΣ be the optimal solution for Sum-Sum, OΣ<sup>M</sup> be the optimal solution for Sum-Max. Similar to Theorem [4.9,](#page-19-2) we can obtain the following result.

Theorem 4.14. For any k, given the optimal facility set OΣ<sup>M</sup> that minimizes Sum-Max and the optimal facility set OΣΣ that minimizes Sum-Sum, we have that αΣΣ(OΣM) · αΣM(OΣΣ) = k.

Proof. Similar to the proof of Theorem [4.9,](#page-19-2) we can show that

$$
\sum_{i \in \mathcal{C}} \max_{a \in O_{\Sigma\Sigma}} d(i, a) \le \sum_{i \in \mathcal{C}} \sum_{a \in O_{\Sigma\Sigma}} d(i, a)
$$
$$
= \frac{1}{\alpha_{\Sigma\Sigma}(O_{\Sigma M})} \sum_{i \in \mathcal{C}} \sum_{a \in O_{\Sigma M}} d(i, a)
$$
$$
\le \frac{k}{\alpha_{\Sigma\Sigma}(O_{\Sigma M})} \sum_{i \in \mathcal{C}} \max_{a \in O_{\Sigma M}} d(i, a)
$$

Then, divide both sides by P <sup>i</sup>∈C maxa∈OΣ<sup>M</sup> d(i, a), we have that

$$
\alpha_{\Sigma M}(O_{\Sigma\Sigma}) = \frac{k}{\alpha_{\Sigma\Sigma}(O_{\Sigma M})}
$$
$$
\alpha_{\Sigma\Sigma}(O_{\Sigma M}) \cdot \alpha_{\Sigma M}(O_{\Sigma\Sigma}) = k
$$

as desired.

Then, using the same proof as we have shown in Corollary [4.9.1,](#page-20-1) we can see that

<span id="page-24-2"></span>Corollary 4.14.1. For any k, there always exists a facility location set A ⊆ F such that choosing <sup>A</sup> would give a <sup>√</sup> k approximation both for minimizing Sum-Max and minimizing Sum-Sum. In fact, we would either get a (1, √ k) approximation by choosing OΣ<sup>M</sup> or a ( √ k, 1) approximation by choosing OΣΣ. In other words, at least one of αΣΣ(OΣM) or αΣM(OΣΣ) is always less than or equal to <sup>√</sup> k.

Recall that Theorem [3.2](#page-7-1) gives a 3 approximation for approximating both Sum-Sum and Sum-Max simultaneously. Now, combined with Corollary [4.14.1,](#page-24-2) we can conclude that there always exists a solution that is a min(<sup>√</sup> k, 3) approximation for both OΣ<sup>M</sup> and OΣΣ.

#### 5 Conclusion

We studied the metric facility location problem (or, equivalently, the committee selection problem), and showed that various natural objectives are compatible with each other, meaning that we can always find a solution which is close-to-optimal for both of these objectives. We did this by first generalizing some existing work to placing multiple facilities, but our main contributions are the more precise and intricate analysis of how Sum-Sum and Max-Sum can be simultaneously approximated when placing multiple facilities. Moreover, we provided new approaches for analysing objectives with different individual costs for the clients, such as Max-Max and Max-Sum, and proved that such objectives can both be approximated within a small constant factor; no previous work has approximated these objectives together to the best of our knowledge.

Despite our results, many interesting open questions remain. While some of our bounds are tight and thus the best possible, it may still be possible to improve some of our other bounds, as shown in Figure [1.](#page-2-0) We believe that many of our results and techniques can be greatly generalized to other objective functions; this is the subject of our current work. Finally, although Sum-Sum and Max-Max can be computed in polynomial time, it would be very interesting to see how well multiple objectives in our facility location setting can be approximated efficiently, as most of our current results only show existence of a good solution, and not its efficient computation.

#### Acknowledgments

This work was partially supported by National Science Foundation award CCF-2006286.

#### References

- <span id="page-24-1"></span>[1] Soroush Alamdari and David Shmoys. A bicriteria approximation algorithm for the k-center and k-median problems. In International Workshop on Approximation and Online Algorithms, pages 66–75. Springer, 2017.
- <span id="page-24-0"></span>[2] Hyung-Chan An and Ola Svensson. Recent developments in approximation algorithms for facility location and clustering problems. In Combinatorial Optimization and Graph Algorithms: Communications of NII Shonan Meetings, pages 1–19. Springer, 2017.

- <span id="page-25-13"></span>[3] Elliot Anshelevich, Aris Filos-Ratsikas, Nisarg Shah, and Alexandros A Voudouris. Distortion in social choice problems: The first 15 years and beyond. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence Survey Track., 2021.
- <span id="page-25-8"></span>[4] Ioannis Caragiannis, Nisarg Shah, and Alexandros A Voudouris. The metric distortion of multiwinner voting. Artificial Intelligence, 313:103802, 2022.
- <span id="page-25-6"></span>[5] Deeparnab Chakrabarty and Chaitanya Swamy. Interpolating between k-median and k-center: Approximation algorithms for ordered k-median. In 45th International Colloquium on Automata, Languages, and Programming (ICALP 2018). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.
- <span id="page-25-1"></span>[6] Hau Chan, Aris Filos-Ratsikas, Bo Li, Minming Li, and Chenhao Wang. Mechanism design for facility location problems: A survey. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4356–4365. International Joint Conferences on Artificial Intelligence Organization, 2021.
- <span id="page-25-7"></span>[7] Hau Chan, Zifan Gong, Minming Li, Chenhao Wang, and Yingchao Zhao. Facility location games with ordinal preferences. Theoretical Computer Science, 979:114208, 2023.
- <span id="page-25-3"></span>[8] Zhihuai Chen, Ken CK Fong, Minming Li, Kai Wang, Hongning Yuan, and Yong Zhang. Facility location games with optional preference. Theoretical Computer Science, 847:185–197, 2020.
- <span id="page-25-12"></span>[9] Argyrios Deligkas, Mohammad Lotfi, and Alexandros A. Voudouris. Agent-constrained truthful facility location games. Journal of Combinatorial Optimization, 49(2):24, 2025.
- <span id="page-25-10"></span>[10] Matthias Ehrgott and Xavier Gandibleux. A survey and annotated bibliography of multiobjective combinatorial optimization. OR-spektrum, 22(4):425–460, 2000.
- <span id="page-25-2"></span>[11] James M. Enelow and Melvin J. Hinich. The spatial theory of voting: An introduction. CUP Archive, 1984.
- <span id="page-25-0"></span>[12] Reza Zanjirani Farahani, Maryam SteadieSeifi, and Nasrin Asgari. Multiple criteria facility location problems: A survey. Applied mathematical modelling, 34(7):1689–1709, 2010.
- <span id="page-25-9"></span>[13] Ling Gai, Mengpei Liang, and Chenhao Wang. Two-facility-location games with mixed types of agents. Applied Mathematics and Computation, 466:128479, 2024.
- <span id="page-25-11"></span>[14] Vasilis Gkatzelis, Daniel Halpern, and Nisarg Shah. Resolving the optimal metric distortion conjecture. In Proceedings of the 61st IEEE Annual Symposium on Foundations of Computer Science (FOCS), pages 1427–1438, 2020.
- <span id="page-25-5"></span>[15] Yue Han, Christopher Jerrett, and Elliot Anshelevich. Optimizing multiple simultaneous objectives for voting and facility location. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 5665–5672, 2023.
- <span id="page-25-4"></span>[16] Panagiotis Kanellopoulos, Alexandros A Voudouris, and Rongsen Zhang. Truthful two-facility location with candidate locations. In International Symposium on Algorithmic Game Theory, pages 365–382. Springer, 2023.
- <span id="page-25-14"></span>[17] Fatih Erdem Kizilkaya and David Kempe. Plurality veto: A simple voting rule achieving optimal metric distortion. In International Joint Conference on Artificial Intelligence, 2022.

- <span id="page-26-6"></span>[18] Amit Kumar and Jon Kleinberg. Fairness measures for resource allocation. SIAM Journal on Computing, 36(3):657–680, 2006.
- <span id="page-26-3"></span>[19] Mohammad Lotfi and Alexandros A Voudouris. On truthful constrained heterogeneous facility location with max-variant cost. Operations Research Letters, 52:107060, 2024.
- <span id="page-26-12"></span>[20] Leon F McGinnis and John A White. A single facility rectilinear location problem with multiple criteria. Transportation Science, 12(3):217–231, 1978.
- <span id="page-26-2"></span>[21] Samuel Merrill III, Samuel Merrill, and Bernard Grofman. A unified theory of voting: Directional and proximity spatial models. Cambridge University Press, 1999.
- <span id="page-26-13"></span>[22] Yoshiaki Ohsawa. A geometrical solution for quadratic bicriteria location models. European Journal of Operational Research, 114(2):380–388, 1999.
- <span id="page-26-9"></span>[23] Paolo Serafino and Carmine Ventre. Heterogeneous facility location without money. Theoretical Computer Science, 636:27–46, 2016.
- <span id="page-26-1"></span>[24] David B. Shmoys. Approximation algorithms for facility location problems. In Klaus Jansen and Samir Khuller, editors, Approximation Algorithms for Combinatorial Optimization, pages 27–32, Berlin, Heidelberg, 2000. Springer Berlin Heidelberg.
- <span id="page-26-7"></span>[25] Krzysztof Sornat. Approximation Algorithms for Multiwinner Elections and Clustering Problems. PhD thesis, University of Wrocław, 2019.
- <span id="page-26-8"></span>[26] Arie Tamir. The k-centrum multi-facility location problem. Discrete Applied Mathematics, 109(3):293–307, 2001.
- <span id="page-26-11"></span>[27] Toby Walsh. Strategy proof mechanisms for facility location at limited locations. In Pacific Rim International Conference on Artificial Intelligence, pages 113–124. Springer, 2021.
- <span id="page-26-0"></span>[28] David P Williamson and David B Shmoys. The design of approximation algorithms. Cambridge university press, 2011.
- <span id="page-26-10"></span>[29] Xinping Xu, Bo Li, Minming Li, and Lingjie Duan. Two-facility location games with minimum distance requirement. Journal of Artificial Intelligence Research, 70:719–756, 2021.
- <span id="page-26-4"></span>[30] Qi Zhao, Wenjing Liu, Qingqin Nong, and Qizhi Fang. Constrained heterogeneous facility location games with max-variant cost. Journal of Combinatorial Optimization, 45(3):90, 2023.
- <span id="page-26-5"></span>[31] Qi Zhao, Wenjing Liu, Qingqin Nong, and Qizhi Fang. Constrained heterogeneous two-facility location games with sum-variant. Journal of Combinatorial Optimization, 47(4):65, 2024.

### Appendix

#### A Extension to the l-centrum Problem

As we have briefly mentioned in Section [2,](#page-5-4) the extension we showed in that section not only hold for Max-f and Sum-f as defined in Definition [2.1,](#page-5-5) but also for all l-centrum problems. This is due to the fact that [\[15\]](#page-25-5) considers the l-centrum problem while Max-f and Sum-f are merely special cases of such family of problems. We define the l-centrum problem as follows.

<span id="page-27-0"></span>Definition A.1. Let A ⊆ F, |A| = k, 1 ≤ l ≤ n. We order the n clients v 1 , v<sup>2</sup> , · · · , v<sup>n</sup> such that

$$
f(v^1, A) \ge f(v^2, A) \ge ... \ge f(v^n, A)
$$

Where f : M × M<sup>k</sup> → R <sup>+</sup> ∪ {0}. Then, we define l-centrum (cl) as

$$
c_l(A) = \sum_{i=1}^l f(v^i, A).
$$

#### B Polynomial-time 3-Distortion of Multi-Winner Voting

As we have discussed in the Introduction, the facility location problems can also be applied in a spacial voting setting where candidates and voters are located in some metric space (M, d). Under this setting, in this section, we will consider the case where we don't know the voter's exact locations but their ordinal preferences for the candidates, namely, distortion (see the survey [\[3\]](#page-25-13)). We define distortion as follows.

Definition B.1. Let (M, D) be the set of all metric spaces that are consistent with the voters' ordinal preferences for the candidates that are given, and O<sup>d</sup> ′ be the optimal k-candidate set for metric (M′ , d′ ). Denote the cost function by c, then the distortion of a feasible solution A is

$$
\sup_{d' \in \mathcal{D}} \frac{c(A, d')}{c(O_{d'}, d')}.
$$

Now, if we are given a set of n voters C and a set of candidates F in metric space (M, d), the goal is to choose k candidates that forms a committee A ⊆ F such that it minimizes some objectives. Instead of considering Max-f and Sum-f as we did in Section [2,](#page-5-4) we would instead consider a generalized version of those two objectives named the l-centrum problem as defined in Definition [A.1.](#page-27-0) As before, f can be any function that satisfies the triangle Inequality [1,](#page-5-1) including max and sum functions.

Note that with Definition [A.1,](#page-27-0) Max-f is equivalent to c<sup>1</sup> and Sum-f is equivalent to cn. [\[4\]](#page-25-8) presented a 9-distortion polynomial-time multi-winner voting rule for f being the q-social cost of a committee of size k with q > k/2 with c<sup>n</sup> in Corollary 3 of their paper. They have also shown that q-social cost with q > k/2 obeys Inequality [1](#page-5-1) in Lemma 2 of their paper, and thus satisfies the desirable property for our function f. Because of this, with the mechanism proposed in [\[17\]](#page-25-14), we can improve the distortion for their objective function to 3, as shown in the following more general theorem. This theorem provides a mechanism for multi-winner voting with distortion at most 3. It works for any objective function where the function f determining the cost of a committee for a single voter obeys the triangle inequality (e.g., max, sum, or q-social cost with q > k/2), and the objective function combining the costs of individual voters is any l-centrum objectives c<sup>l</sup> .

Theorem B.1. For any 1 ≤ l ≤ n, consider l-centrum objective c<sup>l</sup> with any f such that

$$
f(v, A) \le f(v, B) + f(p, B) + f(p, A) \qquad \forall v, p \in \mathcal{C}, A \subseteq \mathcal{F}, B \subseteq \mathcal{F}.
$$

Then there exists a polynomial-time multi-winner voting rule for c<sup>l</sup> with distortion at most 3.

Proof. We will utilize Plurality Veto presented in [\[17\]](#page-25-14). The voting role is as follows. For each voter i, let A<sup>i</sup> be the top k-committee for i with respect to f, since f measures how much each voter likes each set of k candidates. Then, run Plurality Veto on a set of new candidates F ′ = {A1, A2, · · · , An} and voters C with the same preference profile induced by the original problem, and the winner would be the winner of original problem.

First note that this would reduce the problem to a voting problem with n voters and n candidates, which means that running Plurality Veto on it would take polynomial-time. To show that this mechanism has distortion at most 3, let A be the winner, O be the optimal solution, and order the clients as v 1 , v<sup>2</sup> , · · · , v<sup>n</sup> w.r.t. A as defined in Definition [A.1.](#page-27-0) Note that by construction of Plurality Veto, A must be in {A1, A2, · · · , An}. Besides, we know that there exists a matching which maps each voter v i to some voters p[i] ∈ C such that v <sup>i</sup> prefers A over p[i]'s favorite candidate Ap[i] (this is by standard properties of Plurality Veto). Therefore, we have

$$
c_l(A) = \sum_{i=1}^l f(v^i, A)
$$
  
$$
\leq \sum_{i=1}^l f(v^i, A_{p[i]})
$$

.

Now, since we have that f(v, A) ≤ f(v, B) + f(p, B) + f(p, A), ∀v, p ∈ C, A ⊆ F, B ⊆ F, we have that

$$
c_l(A) \leq \sum_{i=1}^l f(v^i, O) + \sum_{i=1}^l f(p[i], O) + \sum_{i=1}^l f(p[i], A_{p[i]}).
$$

Then, since Ap[i] is p[i]'s favorite candidate, we must have that P<sup>l</sup> <sup>i</sup>=1 f(p[i], Ap[i] ) ≤ P<sup>l</sup> <sup>i</sup>=1 f(p[i], O). In addition, since the mapping p[i] forms a matching, and by definition of c<sup>l</sup> , we have that P<sup>l</sup> <sup>i</sup>=1 f(p[i], O) ≤ cl(O) and P<sup>l</sup> <sup>i</sup>=1 f(v i , O) ≤ cl(O). Thus, we can then see that

$$
c_l(A) \leq c_l(O) + c_l(O) + c_l(O)
$$
  
= 3 \cdot c\_l(O)

Therefore, this voting rule has distortion at most 3, as desired.