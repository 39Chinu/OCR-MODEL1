# <span id="page-0-2"></span>Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility

Melih Barsbey Lucas Prieto Stefanos Zafeiriou Tolga Birdal Imperial College London

## Abstract

*Robustness and resource-efficiency are two highly desirable properties for modern machine learning models. However, achieving them jointly remains a challenge. In this paper, we position high learning rates as a facilitator for simultaneously achieving robustness to spurious correlations and network compressibility. We demonstrate that large learning rates also produce desirable representation properties such as invariant feature utilization, class separation, and activation sparsity. Importantly, our findings indicate that large learning rates compare favorably to other hyperparameters and regularization methods, in consistently satisfying these properties in tandem. In addition to demonstrating the positive effect of large learning rates across diverse spurious correlation datasets, models, and optimizers, we also present strong evidence that the previously documented success of large learning rates in standard classification tasks is likely due to its effect on addressing hidden/rare spurious correlations in the training dataset.*

## <span id="page-0-3"></span>1. Introduction

The requirement to function well in novel circumstances and being resource-efficient are two central challenges for modern machine learning (ML) systems, which are expected to perform critical functions with less resources and on smaller hardware, in the face of limited access to computational resources, environmental concerns about energy consumption, and tightening bottlenecks around computational hardware [\[10,](#page-8-0) [11,](#page-8-1) [22\]](#page-8-2). As ML-based technologies increasingly permeate every aspect of daily and industrial life [\[25\]](#page-8-3), it becomes urgent to discover inductive biases that help the models address both challenges together.

Although no learner can be expected to perform well under arbitrary changes in the environment [\[76\]](#page-10-0), they are expected to do so under "reasonable" distribution shifts – a capability commonly exhibited by numerous animal species. Such robustness to distributions that differ from the training data in principled ways have been studied under the umbrella term out-of-distribution (OOD) generalization [\[75\]](#page-10-1).

<span id="page-0-1"></span>![](_page_0_Figure_9.jpeg)

Figure 1. (Top left) When trained on the Double MNIST spurious correlation (SC) dataset with higher learning rates (LR), models tend to be more robust against SCs (higher accuracy), more compressible, and have more favorable core/invariant feature utilization and class separation. (Top right) Given images containing core and spurious features (bright vs. dim digit), high LR models are more likely to be attuned to the core feature vs. spurious feature, unlike low LR models. (Bottom) Our findings extend to standard classification tasks since vulnerability to SCs, such as background, strongly associate with accuracy drops. All ranges normalized in y-axis to highlight relationship to LR.

Although scaling models and datasets, as well as targeting specific parts of this issue have shown some success [\[7,](#page-8-4) [43\]](#page-9-0), OOD generalization remains an open problem both theoretically and in practice [\[13\]](#page-8-5).

A critical obstacle for OOD generalization is the presence of spurious correlations[1](#page-0-0) (SCs) in training data, which can intuitively be described as the relationships between the input features and output label in the training set that do not transfer to the test set. A canonical example is highlighted by [\[5\]](#page-8-6), where models are likely to misclassify a cow

<span id="page-0-0"></span><sup>1</sup>Unless noted otherwise, throughout the paper any references to robustness or OOD generalization will be in the context of spurious correlations.

<span id="page-1-0"></span>on sand as a camel, due to learning the misleading statistical association between camels and desert backgrounds in the training data. [\[63\]](#page-10-2) show empirically and theoretically how overparameterization can cause pathological learning dynamics, amplifying the emphasis on SCs and degrading OOD performance. Similarly, [\[18,](#page-8-7) [36\]](#page-9-1) show that AI systems and large language models (LLMs) can be susceptible to large performance drops if training data contains easily exploitable patterns not carrying over to new environments.

The challenge of OOD generalization becomes even more pronounced when coupled with the need for model compressibility, given the increased demands of resourceefficiency on modern ML systems. It remains unclear whether these two objectives –robustness and efficiency– are inherently in conflict or can actually complement each other [\[15,](#page-8-8) [19\]](#page-8-9). We posit that understanding this interplay is critical for the design of next generation ML models.

To date, explaining the counterintuitive observation that overparameterization often improves rather than harms generalization, especially under SCs, remains to be illuminated, as does understanding when and why models become compressible [\[3,](#page-8-10) [4,](#page-8-11) [6,](#page-8-12) [16,](#page-8-13) [52,](#page-9-2) [58\]](#page-10-3). One key factor in this puzzle is the learning rate (LR), or step size, used during gradientbased training. Various studies have highlighted the impact of large LRs on generalization [\[39,](#page-9-3) [40,](#page-9-4) [50\]](#page-9-5), model compressibility [\[4\]](#page-8-11), and representation sparsity [\[2\]](#page-8-14).

In light of these, in this paper we hypothesize that *in deep neural networks, learning rate might play a pivotal role in achieving robustness to SCs without sacrificing efficiency*. We then confirm this hypothesis via extensive analyses, making the following key contributions:

- 1. We establish that large LRs can simultaneously and consistently promote both compressibility and robustness specifically to SCs across a wide range of architectures, datasets, and optimizers.
- 2. We identify that these effects are accompanied by improved core feature utilization, class separation, and compressibility in the learned representations.
- 3. We show that large LRs produce a unique combination of the aforementioned desirable properties, in comparison to other major hyperparameters and regularizers.
- 4. We provide strong evidence that the robustness against SCs that large LRs confer, contribute to their previously documented success in standard generalization tasks.
- 5. Our investigation into the mechanisms reveal the importance of confident mispredictions of bias-conflicting samples under high LRs.

See Fig. [1](#page-0-1) for an overview of our results. We now move on to a review of the relevant literature.

## <span id="page-1-1"></span>2. Related Work

Robustness to Spurious Correlations. Overreliance on simple, easily exploitable features that have limited bearing on a test set have been pointed out by [\[23\]](#page-8-15) and [\[65\]](#page-10-4). [\[63\]](#page-10-2) examine the role of overparametrization in producing models that rely on spurious features, and [\[51\]](#page-9-6) point out how two features of the data distribution (that they name geometrical and statistical skews) might lead to a max-margin classifier ending up utilizing spurious features. [\[54\]](#page-9-7) point out the importance of features learned in early training, where easy-to-learn, spurious features might not be replaced by better-generalizing features. Various methods have been previously proposed to alleviate this problem. Methods that assume access to spurious feature labels/annotations exploit this information in different ways to improve worst group or unbiased test set performance [\[27,](#page-9-8) [62\]](#page-10-5). In the absence of group annotations, alternative methods rely on assumptions about the nature of the spurious features and the inductive biases of the learning algorithms [\[43,](#page-9-0) [56,](#page-10-6) [72\]](#page-10-7).

Inductive Bias of Large Learning Rates, Model Compressibility. [\[40\]](#page-9-4) provide one of the earliest set of findings regarding the inductive bias of LRs in standard machine learning tasks, and examine how large vs. small LRs lead to qualitatively different features to be learned by the neural network. [\[28\]](#page-9-9) point out how LRs in early training prevents the iterates from being locked into narrow valleys in the loss landscape, where the curvature in certain directions are high, to the detriment of the conditioning of the gradient covariance matrix. [\[39,](#page-9-3) [50\]](#page-9-5) point out the importance of large LRs in early training [\[29\]](#page-9-10). [\[60\]](#page-10-8) demonstrates the crucial role of spurious / opposing signals in early training, and how progressive sharpening [\[9,](#page-8-16) [41\]](#page-9-11) of the loss landscape in the directions that pertain to the representation of these features lead to the eventual downweighting of such nonrobust features. [\[15\]](#page-8-8) find that lottery-ticket style pruning methods present the most advantageous trade-off between performance, robustness, and compressibility. To date, no study has systematically explored how LR influences robustness to SCs or how it interacts with compressibility. See our suppl. material for an extended literature review.

## <span id="page-1-2"></span>3. Setup

We consider a classification setting, where the task of a model (e.g*.,*, a neural network) f is to predict the discrete label y ∈ Y, given a d-dimensional input x ∈ R d . In order to assess the quality of a neural network represented by its weights w, we consider a loss function ℓ : Y × Y 7→ R<sup>≥</sup>0, such that ℓ(y, fw(x)) measures the error incurred by predicting the label of x as arg max<sup>j</sup> fw(x)[j], when the true label is y. We define an unknown *data distribution* µ<sup>Z</sup> over Z, and a *training dataset* with n elements, *i.e.*,, S = {z1, . . . , zn}, where each z<sup>i</sup> := (x<sup>i</sup> , yi) i.i.d. ∼ µZ. We then denote the *population* and *empirical risks* as R(w) := <sup>E</sup>x,y [ℓ(y, fw(x))] and <sup>R</sup>b(w) := 1 n P<sup>n</sup> <sup>i</sup>=1 ℓ(y<sup>i</sup> , fw(xi)). Unless otherwise noted, we utilize the (minibatch) stochastic gradient descent (SGD) algorithm for empirical risk min<span id="page-2-2"></span>imization: wt+1 = w<sup>t</sup> − η∇w<sup>t</sup> 1 b P <sup>i</sup>∈Ω<sup>t</sup> ℓ(y<sup>i</sup> , fw(xi)), where Ω t is a randomly sampled fixed-size subset of the training set, b := |Ω t | is batch size, and η is learning rate (LR; aka step size).

#### 3.1. Spurious Correlations

OOD generalization describes the case whenever we have access to S = {z1, . . . , zn}, z<sup>i</sup> := (x<sup>i</sup> , yi) i.i.d. ∼ µ train Z , yet we are interested in evaluating risk under a different distribution R(w) := Ex,y∼µtest Z [ℓ(y, fw(x))], where µ train Z ̸= µ test Z . This inequality in training and test distributions is called *distribution shift*. Some examples of distribution shift include *subpopulation shift*, where µ train Z (y) ̸= µ test Z (y), and *mechanism shift*, where µ train Z (x|y) ̸= µ test Z (x|y). A subtype of mechanism shift is of special importance in the literature and of particular importance for this paper: A model trained on S is said to potentially be vulnerable to *spurious correlation*s [2](#page-2-0) in the training dataset when it is possible to define subsets of x, x c and x s , where µ train Z (x c |y) = µ test Z (x c |y), and µ train Z (x s |y) ̸= µ test Z (x s |y). In such cases, x c are frequently called *core feature*s (aka invariant features), and x s are frequently called *spurious feature*s.

In previous research that addressed this problem, the assumed data distributions can vary considerably [\[53,](#page-9-12) [75\]](#page-10-1). In this study we focus on one of the most canonical cases, previously called "perception tasks" [\[56\]](#page-10-6) or "easy-to-learn" tasks [\[51\]](#page-9-6), where the core features are perfectly informative with respect to the label in the training set, and spurious features imperfectly so. In such tasks, the former are construed to be more difficult to learn. We assume a separate generative model for spurious features, called bias label b ∈ Y, as opposed to the class label y ∈ Y for core features, admitting the decomposition µZ(x c , x s , y, b) = µZ(x c |y)µZ(x s |b)µZ(y, b), and while µ test Z (y, b) = µ test Z (y)µ test Z (b), we have µ train Z (y, b) ̸= µ train Z (y)µ train Z (b). More specifically, we further assume µ train Z (y = a, b = a) ≫ µ train Z (y = a)µ train Z (b = a), where the mutual information between y and b in the training set presents a challenge for the learner. The value ρ train := 1−µ train Z (y = b) determines the rate of *bias-conflicting* examples in the training dataset, which a learner can exploit to avoid utilizing spurious features. Although there does not exist a canonical definition of easy vs. difficult-to-learn features in the literature [\[57\]](#page-10-9), they can be construed as the difficulty of estimating µ train Z (y|x c ) vs. µ train Z (b|x s ).

#### 3.2. Compressibility

A frequently used metric for characterizing compressibility, especially for activations, is *sparsity*. Given a d-dimensional vector z ∈ R d it can be defined as

<span id="page-2-1"></span>![](_page_2_Figure_7.jpeg)

Figure 2. Example images from semi-synthetic SC datasets.

Sparsity(z) = 1 − ∥z∥0/d. Although this is an intuitive and useful metric, more nuanced notions of compressibility are desirable, e.g. when considering activation functions like sigmoid which do not produce 0 values. This is because a (deterministic or probabilistic) vector might be "summarizable" with a small subset of its elements regardless of the number of 0 entries in the vector. Therefore, we use (q, κ)-Compressibility inspired by [\[24\]](#page-8-17): (q, κ)−Compressibility(z) = 1 − inf∥y∥0≤⌈κd⌉ ∥z−y∥<sup>q</sup> ∥z∥<sup>q</sup> . Intuitively, if a vector's (2, 0.1)-Compressibility is high, this means that this vector can be approximated with little error (in an ℓ<sup>2</sup> sense) by using 0.1 of its elements.

While sparsity or (q, κ)-Compressibility can be considered satisfactory for analyzing representations, the same cannot be said for network parameters. Given our interest in compressibility in relation OOD generalization performance, quantifying network/parameter compressibility while taking the downstream effects of compression on performance into account is crucial. For this purpose we define κ-Prunability; given a predictor f and its (unbiased) test accuracy Accµtest (f) it is defined as κ−Prunability = Accµtest (f (κ) ) / Accµtest (f), where f (κ) corresponds to f with κ ∈ [0, 1] of its parameters pruned (set to 0). This measures the retention of (unbiased) test accuracy after pruning a fraction κ of parameters (neurons in FC layers, nodes/kernels in our structured pruning approach). Overall robustness to pruning is obtained as the sum of these retention ratios over a range of κ (from 0 to 0.9). Given its computationally desirable properties, we utilize node/kernel pruning and take the sum of a range of κ ∈ [0, 1] to measure models' prunability. Given our chosen metric, we will use the terms network prunability and network compressibility interchangeably throughout the rest of the text. In suppl. material we show qualitatively identical results with other relevant notions of compressibility such as (q, κ)-Compressibility, sparsity, and PQ-Index [\[14\]](#page-8-18).

<span id="page-2-0"></span><sup>2</sup>As is common in the literature, here we use the term spurious *correlations* to also include non-linear relationships induced by a confounder.

<span id="page-3-1"></span><span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)

Figure 3. (Left) Effects of learning rate on OOD performance (unbiased test acc.), network prunability, and representation properties with the parity dataset. See suppl. material for min. and max. values. (Right) Prediction attributions (i.e. feature importance) for core, spurious, and noise pixels throughout training for low and high LR models.

#### 3.3. Metrics for Representation Analysis

We now define metrics we use when analyzing learned representations beyond compressibility. Unless otherwise noted, all analyses on representations target post-activation values in the penultimate neural network layer, often referred to as *learned representations* in the literature.

Class Separation. We use different metrics to quantify how much representations are affected by inputs belonging to different classes. The first metric we utilize is class separation R<sup>2</sup> , where we adopt the definition of [\[33\]](#page-9-13), who investigate this notion in relation to model generalization performance as well as representations' transferability under different training losses: R<sup>2</sup> = 1 − ¯dwithin/ ¯dtotal, where ¯dwithin = P<sup>K</sup> k=1 PN<sup>k</sup> m=1 PN<sup>k</sup> n=1 1−sim(xk,m,xk,n) KN<sup>2</sup> k and ¯dtotal = P<sup>K</sup> k=1 P<sup>K</sup> j=1 PN<sup>j</sup> m=1 PN<sup>k</sup> n=1 1−sim(xj,m,xk,n) K2NjN<sup>k</sup> , with K denoting number of classes, N<sup>j</sup> number of samples with label j, and sim a similarity metric such as cosine similarity. Core vs. Spurious Feature Utilization. While useful, class separation does not quantify sensitivity of specific neurons to classes. For this, we use *class-selectivity index* (CSI) [\[37,](#page-9-14) [59\]](#page-10-10): CSI = ρmax πmax−π−max πmax+π−max+ϵ . As in prior works, we take πmax to be largest class-conditional activation mean for a given neuron, where the means are computed over a sample of inputs, and π<sup>−</sup>max is the mean of the remaining classes. In contrast to previous use of this metric, given the prevalence of activation sparsity in our experiments, we multiply this fraction with ρmax, which corresponds to the ratio of instances belonging to the said class for which the neuron is activated. This factors in the "coverage" of a particular neuron for the instances belonging to a class, and prevents computing high class-selectivity for a neuron that fires barely at all. When computed over an unbiased test set, this metric serves to characterize a neuron's *sensitivity to core features*. This is especially useful when the core features and spurious features overlap and attribution methods cannot be reliably used to investigate a specific neuron's responsiveness to core features. We average over neurons' class sensitivities to characterize a network's core feature utilization. Note that in the presence of spurious feature labels the same can be computed for spurious features to produce what we call bias-selectivity index (BSI).

Input Image Attributions. To quantize the influence of particular input pixels or features over models' predictions, we utilize the commonly used interpretability method Integrated Gradients (IG) [\[69\]](#page-10-11) to compute pixel-wise attributions over 10 random seeds. See suppl. material for details of IG, identical results under other interpretability methods, and how it provides convergent results with the aforementioned CSI regarding core/spurious feature utilization.

## <span id="page-3-2"></span>4. Datasets, Models, and Training Procedure

#### 4.1. Datasets

We investigate the effect of LR on the model behavior on four classes of datasets: 1- Synthetic SC, 2- Semi-synthetic SC, 3- Naturalistic SC, and 4- Naturalistic classification. We describe each class of datasets below. As described in our setup, all SC datasets will involve a simple (spurious) feature that is predictive of the true label in the training set but not in the test set, and a more complex (core) feature that is predictive of the label in both training and test.

Synthetic SC data. We utilize two synthetic datasets from the literature to investigate the effects of LR on robustness to SCs and compressibility. These are the *parity dataset* proposed in [\[57\]](#page-10-9) and the *moon-star dataset* proposed in [\[23\]](#page-8-15). The advantage of utilizing synthetic datasets is the clear and simple definition of *simple* vs. *complex* features they enable, as well as total control they afford over biasconflicting sample ratio (ρ). In the parity dataset, the core and spurious features are binary vectors of size C and S, with their parity bits (i.e. whether the vectors include odd number of 1's) corresponding to the true label y vs. spurious label b. Setting C > S leads the spurious feature to be simpler than the complex feature. The moon-star dataset is a binary classification task where the classifier is expected to distinguish moon shaped objects from star shaped objects, with the spurious feature being the quadrant of the image on which the object is located. For both datasets we set biasconflicting sample ratios as ρ train = 0.1 and ρ test = 0.5.

Semi-synthetic SC data. These are arguably the most commonly used dataset types in research on SCs [\[31,](#page-9-15) [53,](#page-9-12) [56,](#page-10-6) [57,](#page-10-9) [63\]](#page-10-2). The reason for their popularity is the attractive combination they provide in the form of having relatively realistic inputs with a decent control over dataset proper-

<span id="page-4-3"></span><span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)

Figure 4. Effects of learning rate on OOD performance (unbiased test acc.), network prunability, and representation (activation) compressibility in semi-synthetic SC data. y-axes are normalized within each figure for each variable, see suppl. material for min. and max. values.

ties such as bias-conflicting sample ratios and the complexity of the core vs. spurious features. We present the bulk of our results on four such datasets: Colored MNIST, Corrupted CIFAR-10, MNIST-CIFAR, and Double MNIST (see Fig. [2](#page-2-1) for examples from each). The former two were proposed by [\[53\]](#page-9-12), and have been frequently used in the literature. MNIST-CIFAR dataset is the extension of the domino dataset proposed by [\[65\]](#page-10-4) to all 10 classes of MNIST and CIFAR-10, while Double MNIST has been designed by the authors for this paper. Each dataset includes some combination and/or modification of the well-known image datasets MNIST [\[38\]](#page-9-16) and CIFAR-10 [\[34\]](#page-9-17). The core and spurious features for these datasets can be specified as digit shape vs. digit color for Colored MNIST, object vs. corruption type for Corrupted CIFAR-10, left digit vs. (brighter) right digit for Double MNIST, and CIFAR-10 vs. MNIST targets for MNIST-CIFAR. We set ρ train = 0.025 for Colored MNIST and Double MNIST datasets, and ρ train = 0.1 for Corrupted CIFAR-10 and MNIST-CIFAR, given the higher baseline difficulty of the latter two.

Naturalistic SC data. We also investigate the effect of LR on two naturalistic image classification datasets CelebA [\[44\]](#page-9-18) and Waterbirds [\[73\]](#page-10-12), both of which have been frequently used in research on SCs [\[62\]](#page-10-5). In the CelebA dataset, spurious and core features are the hair color and gender of the person in the images respectively. In the Waterbirds dataset, these correspond to the background of the pictured bird and their natural habitat (water vs. land). In keeping with literature [\[78\]](#page-10-13), we examine worst-group test accuracy in our experiments with CelebA and Waterbirds [\[62\]](#page-10-5), while for the rest we use unbiased test set accuracy [\[53\]](#page-9-12).

Standard classification data. As we extend our inquiry from SC datasets to naturalistic classification tasks, we investigate the effects of LR on model behavior using CIFAR-10, CIFAR-100, and ImageNet-1k datasets [\[12,](#page-8-19) [34\]](#page-9-17). These

<span id="page-4-1"></span>![](_page_4_Figure_5.jpeg)

Figure 5. Effects of learning rate on OOD performance (unbiased test acc.) and representation properties in semi-synthetic SC datasets. y-axes are normalized within each figure for each variable, see suppl. material for min. and max. values.

datasets include 32 × 32 images of 10 and 100 classes of objects or animals, respectively. We use the traditional train-test splits in our experiments, consisting of 50000 and 10000 samples. ImageNet-1k consists of 1,331,167 color images scaled to 256 × 256 belonging to 1000 classes. We use the traditional train-validation split with 1,281,167 and 50000 samples each.

## <span id="page-4-2"></span>4.2. Architectures, Training, & Implementation

To showcase the diversity of contexts in which LR has the aforementioned effects, we utilize a variety of architectures. These include fully connected networks (FCN) with ReLU activation, convolutional neural networks (CNN) with similar architectures to VGG11 [\[67\]](#page-10-14), ResNet18 and ResNet50 [\[26\]](#page-8-20), and Wide ResNet-101-2 [\[80\]](#page-10-15). We further include

<span id="page-5-2"></span><span id="page-5-1"></span>![](_page_5_Figure_0.jpeg)

Figure 6. Comparing hyperparameters, regularization methods, and losses in terms of OOD robustness, compressibility, and core feature utilization. y-axes are comparable within variables across rows, see suppl. material for min. and max. values.

<span id="page-5-0"></span>![](_page_5_Figure_2.jpeg)

Figure 7. Effects of LR with a Swin Transformer and CelebA dataset on (top) model attributions and (bottom) network statistics.

a large-scale vision transformer model, namely the Swin Transformer by [\[45\]](#page-9-19). Unless otherwise stated, all models are trained with a constant LR until 100% accuracy with no explicit L1/L2 regularization. Batch sizes are set to 16 for CelebA and Waterbirds experiments, and 100 for the remaining experiments. Our suppl. material provides additional details for our experiments and implementation, while showing that using different convergence criteria (*i.e*. training models longer after convergence), an LR annealing scheme, or using the popular Adam optimizer [\[30\]](#page-9-20) leads to qualitatively identical results. An important exception to this scheme is the training of the Swin Transformer, where we utilize an AdamW optimizer combined with an ImageNet-1K pretraining initialization. This is done to test the applicability of our conclusions to realistic deployment scenarios. Unless otherwise noted, the results presented are the average of experiments run with at least 3 different random seeds. Since high LRs can lead to divergence and/or drastic performance loss at the extremes, we utilize LRs up to the point of divergence or drastic performance loss (> 25% in test accuracy) among any one of the seeds. Lastly, since our results involve the examination of a number of variables simultaneously, in figures y-axes are normalized for each variable to highlight the effects of LR, and value ranges for all variables are separately provided in the supp. material for readability.

## <span id="page-5-3"></span>5. Results

How does LRs affect robustness to SC and compressibility? We first investigate the effects of LR on compressibility and robustness to SCs, using the parity dataset in Fig. [3,](#page-3-0) with a fully connected network (FCN) with 3 hidden layers of width 200. The results show a clear effect of LR on robustness to SC and compressibility: Robustness and compressibility increases as a function of LR. The same figure also shows how the attributions to core, spurious, and noise pixels/features evolve for low and high LR models. Moving on to semi-synthetic SC datasets. Fig. [4](#page-4-0) shows that across datasets and architectures, larger LRs lead to increased robustness to SCs, strongly supporting our central hypothesis. This increase in robustness is also accompanied by increased network and activation compressibility. Recall that the notion of κ-Prunability we employ here is computed under an unbiased test set, emphasizing the high LRs effect on achieving these aims jointly. See suppl. material for additional details and similar results with moon-star and Corrupted CIFAR-10 datasets.

How does LR affect learned representations? Our results regarding representation properties induced by high LRs are presented in Fig. [5.](#page-4-1) The results demonstrate that the increase in unbiased test performance and compressibility is accompanied by an increase in core feature utilization and class separation in the learned representations. Given recent results demonstrating learned representations' insensitivity to explicit SC interventions [\[31\]](#page-9-15), it is important to highlight that the positive effects of high LRs are reflected in the learned representations themselves.

Do the observed effects of LR generalize to realistic scenarios? To make sure our results are not limited only to (semi-)synthetic data, we investigate the effect of LRs under a more realistic, up-to-date machine learning pipeline. As described in Sec. [4.2,](#page-4-2) we train a Swin Transformer on the CelebA dataset, and present its effects on worst-group performance, compressibility, and representation properties in Fig. [7.](#page-5-0) The results show that high LRs co-achieve robustness and compressibility in this scenario as well, further supporting our motivating hypothesis. See suppl. material for similar results on Waterbirds dataset.

How does LR behave when compared to and combined with other hyperparameters? We then move on to investigate how large LRs compare to other important hyperparameters in creating these effects. The comparison set includes batch size, momentum, L1 and L2 regularization of the parameters, an alternative loss function designed for better exploitation of rare difficult examples (focal loss; FL [\[42\]](#page-9-21)), and an optimization procedure for finding well-

<span id="page-6-3"></span><span id="page-6-1"></span>![](_page_6_Figure_0.jpeg)

Figure 8. Attributions of a trained ResNet18 model on different datasets. For all datasets, top row includes original images; middle and bottom rows includes attributions of a model trained under low vs. high learning rate respectively.

<span id="page-6-0"></span>![](_page_6_Figure_2.jpeg)

Figure 9. Training loss, unbiased test acc., and spurious feature utilization landscapes for models with different LRs and FL.

<span id="page-6-2"></span>![](_page_6_Figure_4.jpeg)

Figure 10. Higher LRs yield favorable generalization, compressibility, and core feature utilization for CIFAR datasets.

generalizing (wide) minima in the training loss landscape (adaptive sharpness aware minimization; ASAM [\[20,](#page-8-21) [35\]](#page-9-22)). We repeat our experiments by varying the each hyperparameter in question (In the case of FL and ASAM these correspond to their central hyperparameters respectively, γ and ρ). Fig. [6](#page-5-1) demonstrates our results using a ResNet18 model: High LRs consistently facilitate a combination of OOD generalization, network compressibility, and core feature utilization, either on par with or surpassing other interventions. We also observe that other interventions (e.g. L1/L2 regularization) can be combined with high LRs to achieve even better trade-offs. Importantly, in Fig. [9](#page-6-0) we highlight how different methods create robustness through different dynamics in the loss and feature utilization landscape, as discussed further in Sec. [6.](#page-7-0) See suppl. material for details, further results under other models and datasets as well as more background on FL and SAM.

Does LR have similar effects on generalization in standard classification tasks? We next investigate whether the robustness against SCs afforded by high LRs could help explain their success in generalization under standard classification tasks [\[4,](#page-8-11) [40,](#page-9-4) [48\]](#page-9-23). A positive answer to this question would imply that in realistic machine learning settings, even a seemingly IID generalization task can involve OOD generalization subtasks due to implicit and/or rare SCs in the training distribution (see e.g. [\[61\]](#page-10-16)). We start our analyses with CIFAR-10 and CIFAR-100, and then move on to ImageNet-1k.

To qualitatively investigate whether the generalization advantage of large LRs in naturalistic classification tasks pertain to SCs, under a ResNet18 model, we examine input attributions for samples most likely to be predicted correctly by high LR models compared to low LRs (see suppl. material for details), presented in Fig. [8.](#page-6-1) Our results show that high LR models are likelier to focus on core vs. spurious features not only in the semi-synthetic MNIST-CIFAR dataset, but also in the CIFAR-10 and CIFAR-100 datasets as well. In both these datasets, low LR models focus much more on backgrounds and/or the color/texture of the foreground object, whereas the high LR models are likelier to focus on the object contours. The results strongly suggest that high LRs might be obtaining increased generalization through robustness to hidden SCs in the training data. We thus proceed to test this hypothesis quantitatively.

Quantitatively measuring SCs in naturalistic datasets remains an open problem [\[79\]](#page-10-17). Nevertheless, here we develop two complementary metrics to assess the effects of LR on spurious feature utilization. The first, *attribution entropy* relies on computing the entropy of a normalized input attribution map since exploiting background information or object textures/colors would lead to more dispersed attribution maps as opposed focusing on contours of an object. We complement this relatively simple metric with a more targeted one in *background attribution percentage*: Utilizing the DeepLabV3 segmentation algorithm to extract background maps [\[8\]](#page-8-22), we compute the percentage of attributions that fall within vs. outside these image background segments. We present the results in Fig. [10.](#page-6-2) The results not only replicate the benevolent effects of large LRs seen in previous datasets, they also show that higher LR models are less likely to use background/texture information.

ImageNet-1k. We lastly examine the effects of large LRs on models trained on ImageNet-1k dataset. The results presented in Fig. [11](#page-7-1) clearly show that the previously observed effects of LR extend to larger datasets, furthering the implications of our findings to realistic scenarios.

<span id="page-7-6"></span><span id="page-7-1"></span>![](_page_7_Figure_0.jpeg)

Figure 11. Effects of LR with ImageNet-1k dataset and ResNet18 model on model statistics and attributions.

<span id="page-7-3"></span>![](_page_7_Figure_2.jpeg)

Figure 12. Unbiased test accuracy strongly correlates with maximum bias-conflicting loss ratio during training.

# <span id="page-7-0"></span>6. On the mechanism of large LRs

We now investigate the mechanisms through which large LRs create their effects, and provide theoretical evidence for our findings. Numerous works indicate that simple spurious features are learned earlier than more complex core features [\[54,](#page-9-7) [57\]](#page-10-9), regardless of LR [\[77,](#page-10-18) Theorem 4.1]: this produces a phase in learning where bias-aligned (BA) samples are predicted correctly, while bias-conflicting (BC) samples are misclassified. We find that at this stage, another effect of large LRs, namely fast *norm growth* of model parameters and logits, become important[3](#page-7-2) . This is because with crossentropy (CE) loss, up-scaled logits under high LRs lead to *confident mispredictions of BC samples*. Due to the nonlinearty of CE, the mini-batch loss (and thus the gradient) is then increasingly dominated by mispredicted BC samples, corresponding to an implicit *reweighting of the dataset in favor of BC samples*. We now formalize this statement.

<span id="page-7-5"></span>Proposition 1 *Let* f<sup>w</sup> *be a predictor in a binary classification task, trained under the cross-entropy loss* ℓ*. Let* Ω = {z<sup>1</sup> . . . z<sup>|</sup>Ω<sup>|</sup>} *be a training mini-batch such that* (x<sup>i</sup> , y<sup>i</sup> , bi) *i.i.d.* ∼ µ train Z *. Let* Ωbc := {z<sup>i</sup> ∈ Ω|y<sup>i</sup> ̸= bi} *and* Ωba := {z<sup>i</sup> ∈ Ω|y<sup>i</sup> = bi} *represent the bias-conflicting and bias-aligned subsets of* Ω*. If* f<sup>w</sup> *predicts according to the spurious decision rule,* i.e*.* b = arg max<sup>j</sup> fw(x)[j]*, then for some* α > 0*, as the logit-scaling factor* k → ∞

$$
\frac{\sum_{\mathbf{x}, y \in \Omega_{\text{bc}}} \ell(y, k f_{\mathbf{w}}(\mathbf{x}))}{\sum_{\mathbf{x}, y \in \Omega_{\text{ba}}} \ell(y, k f_{\mathbf{w}}(\mathbf{x}))} = \mathcal{O}(k e^{\alpha k}).
$$
 (1)

The proof is deferred to the suppl. material, which also presents another proposition, showing that this results in an increase in the gradient norms to the subnetworks that utilize spurious vs. core features. This implies that large LRs generate strong gradients that discourage reliance on

<span id="page-7-4"></span>![](_page_7_Figure_10.jpeg)

Figure 13. Large LRs lead to losses from bias-conflicting samples to dominate the gradient, resulting in increased utilization of core features. (Left) Attribution maps (right) model statistics for Colored MNIST and CIFAR-10 through training.

spurious features, whereas small LRs lack this pressure, allowing models to retain spurious subnetworks while memorizing BC samples. Figs. [12](#page-7-3) and [13](#page-7-4) demonstrate the consistency of our empirical findings with the implications of Prop. [1.](#page-7-5) Experiments with ResNet18, Fig. [12](#page-7-3) demonstrate that unbiased test accuracy *very strongly* correlates with the maximum loss ratio (*i.e*. BC loss/total minibatch loss) encountered during training across datasets, providing support for our proposed mechanism (see suppl. material for similar results with additional datasets and models). Fig. [13](#page-7-4) presents a visualization of training dynamics under a large vs. small LR (η = 0.001 vs 0.1). We observe that early focus on spurious features are weaned off under large LRs, as bias-conflicting samples dominate the loss due to confident mispredictions, both in the semi-synthetic Colored MNIST dataset *and* the standard classification CIFAR-10 dataset.

We can interpret the large, disruptive updates to spurious subnetworks as a recurring *lottery ticket* type scenario [\[21\]](#page-8-23), where spurious subnetworks are effectively reset by large gradient updates whenever they rely too much on spurious feature and lead to confident mispredictions, similar to the "catapult mechanism", proposed by [\[39\]](#page-9-3).

## 7. Conclusion

<span id="page-7-7"></span>Robustness to SCs and compressibility are crucial requirements for advancing safe, trustworthy, and resourceefficient deep learning. In the absence of strong theoretical guarantees, our controlled experiments demonstrate the efficacy of LR as a significant implicit bias for addressing these concerns simultaneously. We show that high LRs jointly obtain robustness to spurious correlations, network compressibility, and favorable representation properties.

Limitations and future work. Our work is far from an end-to-end, convergent explanation involving optimization dynamics, parameter loss landscapes, and representations. Future work should study how multiple/hierarchical SCs interact with training dynamics, and design explicit training procedures for more robust, efficient models.

<span id="page-7-2"></span><sup>3</sup>While norm growth under large LRs is well-established by previous research [\[46,](#page-9-24) [49,](#page-9-25) [64\]](#page-10-19), its effects are underexplored in robust generalization.

## Acknowledgments

S. Zafeiriou and part of the research was funded by the EPSRC Fellowship DEFORM (EP/S010203/1) and EPSRC Project GNOMON (EP/X011364/1). T. Birdal acknowledges support from the Engineering and Physical Sciences Research Council [grant EP/X011364/1]. T. Birdal was supported by a UKRI Future Leaders Fellowship [grant MR/Y018818/1] as well as a Royal Society Research Grant [RG/R1/241402].

## References

- <span id="page-8-25"></span>[1] Kumar Abhishek and Deeksha Kamath. Attribution-based XAI Methods in Computer Vision: A Review, 2022. [5](#page-4-3)
- <span id="page-8-14"></span>[2] Maksym Andriushchenko, Aditya Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. SGD with Large Step Sizes Learns Sparse Features, 2023. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-8-10"></span>[3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger Generalization Bounds for Deep Nets via a Compression Approach. In *International Conference on Learning Representations*, 2018. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-8-11"></span>[4] Melih Barsbey, Milad Sefidgaran, Murat A Erdogdu, Gael¨ Richard, and Umut S¸ims¸ekli. Heavy Tails in SGD and Compressibility of Overparametrized Neural Networks. In *Advances in Neural Information Processing Systems*, pages 29364–29378. Curran Associates, Inc., 2021. [2,](#page-1-0) [7,](#page-6-3) [1](#page-0-2)
- <span id="page-8-6"></span>[5] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in Terra Incognita. In *European Conference on Computer Vision*, pages 472–489, 2018. [1](#page-0-2)
- <span id="page-8-12"></span>[6] Tolga Birdal, Aaron Lou, Leonidas J Guibas, and Umut S¸ims¸ekli. Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks. In *Advances in Neural Information Processing Systems*, pages 6776–6789, Vancouver, Canada, 2021. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-8-4"></span>[7] Sebastien Bubeck and Mark Sellke. A Universal Law of Robustness via Isoperimetry. In *Advances in Neural Information Processing Systems*, pages 28811–28822. Curran Associates, Inc., 2021. [1](#page-0-2)
- <span id="page-8-22"></span>[8] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking Atrous Convolution for Semantic Image Segmentation, 2017. [7](#page-6-3)
- <span id="page-8-16"></span>[9] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability. In *International Conference on Learning Representations*, 2021. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-8-0"></span>[10] Jude Coleman. AI's Climate Impact Goes beyond Its Emissions. https://www.scientificamerican.com/article/aisclimate-impact-goes-beyond-its-emissions/, 2023. [1](#page-0-2)
- <span id="page-8-1"></span>[11] Josh Constine and Veronica Mercado. The AI compute shortage explained by Nvidia, Crusoe, & MosaicML | AI Venture Capital. https://www.signalfire.com/blog/ai-computeshortage, 2023. [1](#page-0-2)
- <span id="page-8-19"></span>[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In *2009 IEEE Conference on Computer Vision and Pattern Recognition*, pages 248–255, 2009. [5](#page-4-3)

- <span id="page-8-5"></span>[13] Benoit Dherin, Michael Munn, Mihaela Rosca, and David Barrett. Why neural networks find simple solutions: The many regularizers of geometric complexity. *Advances in Neural Information Processing Systems*, 35:2333–2349, 2022. [1](#page-0-2)
- <span id="page-8-18"></span>[14] Enmao Diao, Ganghua Wang, Jiawei Zhan, Yuhong Yang, Jie Ding, and Vahid Tarokh. Pruning Deep Neural Networks from a Sparsity Perspective, 2023. [3,](#page-2-2) [5](#page-4-3)
- <span id="page-8-8"></span>[15] James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A Winning Hand: Compressing Deep Networks Can Improve Out-of-Distribution Robustness. In *Advances in Neural Information Processing Systems*, pages 664–676. Curran Associates, Inc., 2021. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-8-13"></span>[16] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp Minima Can Generalize For Deep Nets. *arXiv:1703.04933 [cs]*, 2017. [2](#page-1-0)
- <span id="page-8-24"></span>[17] Thu Dinh, Bao Wang, Andrea Bertozzi, Stanley Osher, and Jack Xin. Sparsity Meets Robustness: Channel Pruning for the Feynman-Kac Formalism Principled Robust Deep Neural Nets. In *Machine Learning, Optimization, and Data Science*, pages 362–381, Cham, 2020. Springer International Publishing. [1](#page-0-2)
- <span id="page-8-7"></span>[18] Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia Hu. Shortcut Learning of Large Language Models in Natural Language Understanding, 2023. [2](#page-1-0)
- <span id="page-8-9"></span>[19] Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi, Xia Hu, and Ahmed Hassan Awadallah. Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding. In *Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics*, pages 1766–1778, Dubrovnik, Croatia, 2023. Association for Computational Linguistics. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-8-21"></span>[20] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-Aware Minimization for Efficiently Improving Generalization, 2021. [7](#page-6-3)
- <span id="page-8-23"></span>[21] Jonathan Frankle and Michael Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In *International Conference on Learning Representations*, New Orleans, Louisiana, 2019. OpenReview.net. [8](#page-7-6)
- <span id="page-8-2"></span>[22] Brian Fung. The big bottleneck for AI: A shortage of powerful chips | CNN Business. https://www.cnn.com/2023/08/06/tech/ai-chips-supplychain/index.html, 2023. [1](#page-0-2)
- <span id="page-8-15"></span>[23] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, ¨ Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks. *Nature Machine Intelligence*, 2(11):665–673, 2020. [2,](#page-1-0) [4,](#page-3-1) [1](#page-0-2)
- <span id="page-8-17"></span>[24] Remi Gribonval, Volkan Cevher, and Mike E Davies. ´ Compressible distributions for high-dimensional statistics. *IEEE Transactions on Information Theory*, 58(8):5016– 5034, 2012. [3](#page-2-2)
- <span id="page-8-3"></span>[25] Katherine Haan. Most Common Way Consumers Plan to Use Artificial Intelligence. https://datawrapper.dwcdn.net/R4rT3/4/, 2024. [1](#page-0-2)
- <span id="page-8-20"></span>[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In *Proceed-*

*ings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 770–778, 2016. [5](#page-4-3)

- <span id="page-9-8"></span>[27] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In *Proceedings of the First Conference on Causal Learning and Reasoning*, pages 336– 351. PMLR, 2022. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-9-9"></span>[28] Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof Geras. The Break-Even Point on Optimization Trajectories of Deep Neural Networks, 2020. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-9-10"></span>[29] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. *arXiv:1609.04836 [cs, math]*, 2017. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-9-20"></span>[30] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *International Conference on Learning Representations (ICLR)*, San Diega, CA, USA, 2015. [6](#page-5-2)
- <span id="page-9-15"></span>[31] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations, 2022. [4,](#page-3-1) [6](#page-5-2)
- <span id="page-9-27"></span>[32] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. Captum: A unified and generic model interpretability library for pytorch, 2020. [8](#page-7-6)
- <span id="page-9-13"></span>[33] Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why Do Better Loss Functions Lead to Less Transferable Features? In *Advances in Neural Information Processing Systems*, pages 28648–28662. Curran Associates, Inc., 2021. [4](#page-3-1)
- <span id="page-9-17"></span>[34] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. *Communications of the ACM*, 60(6):84–90, 2017. [5](#page-4-3)
- <span id="page-9-22"></span>[35] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks, 2021. [7](#page-6-3)
- <span id="page-9-1"></span>[36] Anisio Lacerda, Daniel Ayala, Francisco Malaguth, and Fabio Kanadani. An Empirical Analysis of Vision Transformers Robustness to Spurious Correlations in Health Data. In *2023 International Joint Conference on Neural Networks (IJCNN)*, pages 1–8, 2023. [2](#page-1-0)
- <span id="page-9-14"></span>[37] Matthew L. Leavitt and Ari S. Morcos. Selectivity considered harmful: Evaluating the causal impact of class selectivity in DNNs. In *International Conference on Learning Representations*, 2021. [4](#page-3-1)
- <span id="page-9-16"></span>[38] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. *Proceedings of the IEEE*, 86(11):2278–2324, 1998. [5](#page-4-3)
- <span id="page-9-3"></span>[39] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: The catapult mechanism, 2020. [2,](#page-1-0) [8,](#page-7-6) [1](#page-0-2)
- <span id="page-9-4"></span>[40] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks. In *Neural Information Processing Systems*, 2019. [2,](#page-1-0) [7,](#page-6-3) [1](#page-0-2)

- <span id="page-9-11"></span>[41] Zhouzi Li, Zixuan Wang, and Jian Li. Analyzing Sharpness along GD Trajectory: Progressive Sharpening and Edge of Stability, 2022. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-9-21"></span>[42] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal Loss for Dense Object Detection, 2018. ´ [6](#page-5-2)
- <span id="page-9-0"></span>[43] Evan Z. Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just Train Twice: Improving Group Robustness without Training Group Information. In *Proceedings of the 38th International Conference on Machine Learning*, pages 6781–6792. PMLR, 2021. [1,](#page-0-2) [2,](#page-1-0) [3](#page-2-2)
- <span id="page-9-18"></span>[44] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In *2015 IEEE International Conference on Computer Vision (ICCV)*, pages 3730–3738, 2015. [5](#page-4-3)
- <span id="page-9-19"></span>[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, 2021. [6](#page-5-2)
- <span id="page-9-24"></span>[46] Clare Lyle, Zeyu Zheng, Khimya Khetarpal, James Martens, Hado van Hasselt, Razvan Pascanu, and Will Dabney. Normalization and effective learning rates in reinforcement learning. In *The Thirty-eighth Annual Conference on Neural Information Processing Systems*, 2024. [8](#page-7-6)
- <span id="page-9-26"></span>[47] Hannes Mandler and Bernhard Weigand. A review and benchmark of feature importance methods for neural networks. *ACM Comput. Surv.*, 56(12):318:1–318:30, 2024. [5,](#page-4-3) [8](#page-7-6)
- <span id="page-9-23"></span>[48] Charles H. Martin and Michael W. Mahoney. Traditional and Heavy-Tailed Self Regularization in Neural Network Models, 2019. [7](#page-6-3)
- <span id="page-9-25"></span>[49] William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah Smith. Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent, 2023. [8](#page-7-6)
- <span id="page-9-5"></span>[50] Amirkeivan Mohtashami, Martin Jaggi, and Sebastian Stich. Special Properties of Gradient Descent with Large Learning Rates. In *Neural Information Processing Systems*, 2023. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-9-6"></span>[51] Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure modes of out-ofdistribution generalization. In *International Conference on Learning Representations*, 2022. [2,](#page-1-0) [3,](#page-2-2) [1](#page-0-2)
- <span id="page-9-2"></span>[52] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep Double Descent: Where Bigger Models and More Data Hurt. In *Eighth International Conference on Learning Representations*, 2020. [2](#page-1-0)
- <span id="page-9-12"></span>[53] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from Failure: Training Debiased Classifier from Biased Classifier. In *Advences in Neural Information Processing Systems*, 2020. [3,](#page-2-2) [4,](#page-3-1) [5,](#page-4-3) [1,](#page-0-2) [2](#page-1-0)
- <span id="page-9-7"></span>[54] Mohammad Pezeshki, Sekou-Oumar Kaba, Yoshua Ben- ´ gio, Aaron Courville, Doina Precup, and Guillaume Lajoie. Gradient Starvation: A Learning Proclivity in Neural Networks. In *Advances in Neural Information Processing Systems*, 2021. [2,](#page-1-0) [8,](#page-7-6) [1](#page-0-2)

- <span id="page-10-20"></span>[55] Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale Feature Learning Dynamics: Insights for Double Descent. In *Proceedings of the 39th International Conference on Machine Learning*, pages 17669– 17690. PMLR, 2022. [1](#page-0-2)
- <span id="page-10-6"></span>[56] Aahlad Puli, Lily Zhang, Yoav Wald, and Rajesh Ranganath. Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy, 2023. [2,](#page-1-0) [3,](#page-2-2) [4,](#page-3-1) [1](#page-0-2)
- <span id="page-10-9"></span>[57] GuanWen Qiu, Da Kuang, and Surbhi Goel. Complexity matters: Dynamics of feature learning in the presence of spurious correlations. *arXiv preprint arXiv:2403.03375*, 2024. [3,](#page-2-2) [4,](#page-3-1) [8](#page-7-6)
- <span id="page-10-3"></span>[58] Anant Raj, Melih Barsbey, Mert Gurbuzbalaban, Lingjiong Zhu, and Umut S¸ims¸ekli. Algorithmic Stability of Heavy-Tailed Stochastic Gradient Descent on Least Squares. In *Proceedings of The 34th International Conference on Algorithmic Learning Theory*, pages 1292–1342, 2023. [2](#page-1-0)
- <span id="page-10-10"></span>[59] Omkar Ranadive, Nikhil Thakurdesai, Ari S. Morcos, Matthew L. Leavitt, and Stephane Deny. On the special role of class-selective neurons in early training. *Transactions on Machine Learning Research*, 2023. [4](#page-3-1)
- <span id="page-10-8"></span>[60] Elan Rosenfeld and Andrej Risteski. Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization, 2023. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-10-16"></span>[61] Elan Rosenfeld and Andrej Risteski. Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization. In *The Twelfth International Conference on Learning Representations*, 2024. [7,](#page-6-3) [2](#page-1-0)
- <span id="page-10-5"></span>[62] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. In *International Conference on Learning Representations*. arXiv, 2020. [2,](#page-1-0) [5,](#page-4-3) [1](#page-0-2)
- <span id="page-10-2"></span>[63] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In *Proceedings of the 37th International Conference on Machine Learning*, pages 8346–8356. JMLR.org, 2020. [2,](#page-1-0) [4,](#page-3-1) [1](#page-0-2)
- <span id="page-10-19"></span>[64] Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks, 2016. [8](#page-7-6)
- <span id="page-10-4"></span>[65] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The Pitfalls of Simplicity Bias in Neural Networks. In *Advances in Neural Information Processing Systems*, pages 9573–9585, 2020. [2,](#page-1-0) [5,](#page-4-3) [1](#page-0-2)
- <span id="page-10-26"></span>[66] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning Important Features Through Propagating Activation Differences, 2019. [8](#page-7-6)
- <span id="page-10-14"></span>[67] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition, 2015. [5,](#page-4-3) [3](#page-2-2)
- <span id="page-10-25"></span>[68] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In *Proceedings of the 34th International Conference on Machine Learning - Volume 70*, pages 3319–3328, Sydney, NSW, Australia, 2017. JMLR.org. [5,](#page-4-3) [8](#page-7-6)
- <span id="page-10-11"></span>[69] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks, 2017. [4](#page-3-1)

- <span id="page-10-21"></span>[70] Taiji Suzuki, Hiroshi Abe, Tomoya Murata, Shingo Horiuchi, Kotaro Ito, Tokuma Wachi, So Hirai, Masatoshi Yukishima, and Tomoaki Nishimura. Spectral Pruning: Compressing Deep Neural Networks via Spectral Analysis and its Generalization Error. In *Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence*, pages 2839–2846. International Joint Conferences on Artificial Intelligence Organization, 2020. [1](#page-0-2)
- <span id="page-10-22"></span>[71] Taiji Suzuki, Hiroshi Abe, and Tomoaki Nishimura. Compression based bound for non-compressed network: Unified generalization error analysis of large compressible deep neural network. In *International Conference on Learning Representations*, 2020. [1](#page-0-2)
- <span id="page-10-7"></span>[72] Rishabh Tiwari and Pradeep Shenoy. Overcoming simplicity bias in deep networks using a feature sieve. In *International Conference on Machine Learning*, pages 34330– 34343. PMLR, 2023. [2,](#page-1-0) [1](#page-0-2)
- <span id="page-10-12"></span>[73] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. [5](#page-4-3)
- <span id="page-10-23"></span>[74] Luyu Wang, Gavin Weiguang Ding, Ruitong Huang, Yanshuai Cao, and Yik Chau Lui. Adversarial Robustness of Pruned Neural Networks. 2018. [1](#page-0-2)
- <span id="page-10-1"></span>[75] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. A Fine-Grained Analysis on Distribution Shift. In *International Conference on Learning Representations*, 2021. [1,](#page-0-2) [3](#page-2-2)
- <span id="page-10-0"></span>[76] D.H. Wolpert and W.G. Macready. No free lunch theorems for optimization. *IEEE Transactions on Evolutionary Computation*, 1(1):67–82, 1997. [1](#page-0-2)
- <span id="page-10-18"></span>[77] Yu Yang, Eric Gan, Gintare Karolina Dziugaite, and Baharan Mirzasoleiman. Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias. In *Proceedings of The 27th International Conference on Artificial Intelligence and Statistics*, pages 2953–2961. PMLR, 2024. [8](#page-7-6)
- <span id="page-10-13"></span>[78] Wenqian Ye, Guangtao Zheng, Xu Cao, Yunsheng Ma, and Aidong Zhang. Spurious Correlations in Machine Learning: A Survey, 2024. [5](#page-4-3)
- <span id="page-10-17"></span>[79] Wenqian Ye, Guangtao Zheng, Xu Cao, Yunsheng Ma, and Aidong Zhang. Spurious Correlations in Machine Learning: A Survey, 2024. [7](#page-6-3)
- <span id="page-10-15"></span>[80] Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks, 2017. [5](#page-4-3)
- <span id="page-10-24"></span>[81] Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin. Catapults in SGD: Spikes in the training loss and their impact on generalization through feature learning, 2024. [1](#page-0-2)

# Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility

Supplementary Material

In this document, we present in Sec. [A](#page-0-3) an extended review of the preceding research to contextualize our paper's motivation and findings. Building on this, Sec. [B](#page-1-1) we highlight how our findings extend and improve upon previous literature, and point toward fruitful future research directions. After providing additional details regarding our experiment settings in Sec. [C,](#page-1-2) we provide additional results and statistics regarding the main paper's findings in Sec. [D.](#page-3-2) Lastly, Sec. [E](#page-5-3) reviews our attribution visualization methodology, and provides extensive additional visual evidence for our claims.

## A. Extended Related Work

A large amount of recent work on spurious correlations (SCs) have focused on the "default" tendency of neural networks, trained under gradient-based empirical risk minimization (ERM), to exploit simple features in the training datasets at the expense of more complex yet robust/invariant ones. These include [\[5\]](#page-8-6), who highlight the overreliance of vision models on background information; [\[65\]](#page-10-4), who emphasize the "simplicity bias" of neural networks in preferring simple features over more complex and informative ones, and [\[23\]](#page-8-15), who emphasize the tendency of neural networks in engaging "shortcut learning" in various modalities of application. Ensuing research proposed several explanations for this phenomenon. For example, [\[51,](#page-9-6) [56,](#page-10-6) [63\]](#page-10-2) highlight the inductive bias of a maximum margin classifier as the primary reason for the exploitation of spurious features. Alternatively, [\[51,](#page-9-6) [54\]](#page-9-7) emphasize the dynamics of gradientbased learning in creating this effect, where early adoption of simple (and spurious) features harms the later learning of more complex and more informative features.

Building on diagnoses, such as those mentioned above, for the cause of this unintended learning of spurious features, other research propose interventions to mitigate this problem. For example, [\[56\]](#page-10-6) propose new losses that optimize for a *uniform* margin solution rather than a maxmargin solution. On the other hand, [\[43,](#page-9-0) [53\]](#page-9-12) propose twostage methods that reweight the dataset by deemphasizing samples that are learned earlier, and [\[72\]](#page-10-7) discourage neural network to produce representations predictive of the label early in the neural network. Other methods assume access to spurious feature labels at training time, and exploit these in various ways to improve robustness [\[27,](#page-9-8) [62\]](#page-10-5). While to our knowledge no previous research *systematically* investigates the effect of LR on generalization under SCs (and in relation to compressibility), some previous research hint at the outsized impact of LR on such behavior. [\[40\]](#page-9-4) examine the effect of large LRs on feature learning and generalization, without explicitly addressing the implications in an OOD generalization context. While [\[55\]](#page-10-20) speculate about the potential effects of LR tuning on OOD generalization, [\[27\]](#page-9-8) empirically find that LR is most likely to affect robustness to SCs, and [\[56\]](#page-10-6) speculate that large LRs might lead to improved performance due to inability of models trained thereunder to approximate a max-margin solution.

While previous research showed a positive relationship between compressibility and generalization through theoretical and empirical findings [\[3,](#page-8-10) [4,](#page-8-11) [6,](#page-8-12) [70,](#page-10-21) [71\]](#page-10-22), it is much less clear how well this applies to OOD generalization. Indeed, existing research provides at best an ambivalent picture regarding the simultaneous achieveability of generalization, robustness, and compressibility [\[15,](#page-8-8) [17,](#page-8-24) [19,](#page-8-9) [74\]](#page-10-23). Various studies have highlighted the impact of large LRs on generalization [\[39,](#page-9-3) [40,](#page-9-4) [50\]](#page-9-5), model compressibility [\[4\]](#page-8-11), and representation sparsity [\[2\]](#page-8-14); making it a prime candidate for further investigation regarding its ability to facilitate compressibility and robustness in tandem. [\[28\]](#page-9-9) point out how LRs in early training prevents the iterates from being stuck in narrow valleys in the loss landscape, where the curvature in certain directions are high. [\[39,](#page-9-3) [50\]](#page-9-5) point out the importance of large LRs in early training, where basinjumping behavior leads to better generalizing and/or flatter minima [\[29\]](#page-9-10). While [\[2,](#page-8-14) [40,](#page-9-4) [81\]](#page-10-24) focus on the effect of large LRs on feature learning, [\[60\]](#page-10-8) demonstrate the crucial role of spurious / opposing signals in early training, and how progressive sharpening [\[9,](#page-8-16) [41\]](#page-9-11) of the loss landscape in the directions that pertain to the representation of these features lead to the eventual downweighting of such nonrobust features. [\[60\]](#page-10-8) further observe that this is due to discrete nature of practical steepest ascent methods (GD, SGD), as it is not observed in gradient flow regime, implicating learning rate as a prime candidate for modulating this behavior.

## B. Extended Discussion of Our Contributions

In this paper, we demonstrate through systematic experiments the unique role large learning rates (LRs) in simultaneously achieving robustness and resource-efficiency. More concretely, we demonstrate that:

- Large LRs simultaneously facilitate robustness to SCs and compressibility in a variety of datasets, model architectures, and training schemes.
- Increase in robustness and compressibility is accompanied by increased core (aka stable, invariant) feature uti-

<span id="page-12-1"></span>![](_page_12_Figure_0.jpeg)

Figure 14. Visualizing attributions on a CIFAR-10 dataset with ResNet18 models using Integrated Gradients (left) and DeepLift (right).

<span id="page-12-0"></span>![](_page_12_Figure_2.jpeg)

Figure 15. (Left) Effects of learning rate on OOD performance (unbiased test acc.), network prunability, and representation properties with the moon-star dataset.

lization and class separation in learned representations.

- Large LRs are unique in consistently achieving these properties across datasets compared to other interventions, and can be combined with explicit regularization for even better performance.
- Large LRs have a similar effect in naturalistic classification tasks by addressing hidden/rare spurious correlations in the dataset.

We now discuss further implications of our results in light of recent findings in the literature.

Inductive biases of SGD. Our findings call into question the assumptions regarding the inductive biases of "default" SGD. We find that LR selection can change the unbiased test set accuracy by *up to* ∼*50%* (See Table S1). This has two major implications: (i) Any method that relies on assumptions regarding default behavior of neural networks (e.g. [\[43,](#page-9-0) [53\]](#page-9-12)) should consider the fact that the said defaults can vary considerably based on training hyperparameters, including but not limited to LR. (ii) Any proposed intervention for improving robustness to SCs should consider utilizing large LRs as a strong baseline against the proposed method.

Overparametrization and robustness. We observe a strong interaction between overparametrization and LR in robustness to SCs. Our findings show that (Table S1) the range of test accuracies that can be obtained by tuning LRs increase as the models get more expressive. For example, in Colored MNIST dataset, while the difference between the highest vs. lowest performing models is ∼4% for a fully connected network (FCN), this increases to ∼23% for ResNet18 and ∼31% for ResNet50. This implies that while overparametrization indeed seems to play an important role in robustness to SCs as suggested by some previous work [\[63\]](#page-10-2) (cf. [\[56\]](#page-10-6)), this needs to be considered in the context of central training hyperparameters, as they can modulate this vulnerability to a great degree.

Mechanism of LR's Effects. As noted above, [\[56\]](#page-10-6) argue that max-margin classification inevitably leads to exploitation of spurious features, and LRs might protect against SCs by creating models closer to a uniform margin solution. To support their conjecture, they present evidence that shows average losses incurred from bias-aligned (BA) vs. bias-conflicting (BC) samples are closer in large LR models compared to small LR models. However, we find that their finding do not generalize across different data distributions. Computing avg. loss for BA samples / avg. loss for BC samples, we find that in both Colored MNIST and Double MNIST datasets, low LR models produce average losses that are closer in ratio (3.9×10<sup>−</sup>3 vs. 3.4×10<sup>−</sup>3 in Colored MNIST and 9.5×10<sup>−</sup>3 vs. 7.4×10<sup>−</sup>6 in Double MNIST). This highlights the importance of testing such claims across diverse settings, and emphasizes the need for novel and systematic explanations for the effect of large LRs on robustness to SCs.

Compressibility and generalization. [\[2\]](#page-8-14) argue that large LRs create models with sparse representations. Our findings support their claim across diverse settings. However, we also observe that LRs' effects on unbiased test accuracy and network compressibility (i.e. prunability) precede that of activation sparsity (i.e. there are LRs that are large enough to increase test accuracy and prunability but not large enough to increase representation sparsity). This strongly implies that the representation sparsity is a downstream effect of large LRs, rather than being a mediator of generalization and network compressibility. On the other hand, our findings include initial evidence for wide minima [\[29\]](#page-9-10) found by large LRs to be associated with increased core feature utilization. Examining the interaction of parameter and representation space properties produced by LRs (see e.g. [\[61\]](#page-10-16)) is a promising future direction for understanding the inductive bias of large LRs and SGD in general.

<span id="page-13-0"></span>![](_page_13_Figure_0.jpeg)

Figure 16. Effects of LR on OOD performance (unbiased test acc.), network prunability, and representation (activation) compressibility in Corrupted CIFAR-10 dataset. x-axes correspond to learning rate (η), y-axes are normalized within each figure for each variable.

![](_page_13_Figure_2.jpeg)

Figure 17. Effects of learning rate on representation (activation) statistics for semi-synthetic datasets. y-axes are normalized within each figure for each variable.

# C. Further Details on Experiment Settings

We use Python programming language for all experiments included in this paper. For experiments with semisynthetic, realistic SC, and naturalistic datasets we use the versions of ResNet18, ResNet50, Wide ResNet101-2, Swin Transformer (tiny) as included in the Python package torchvision, as well as a FCN with two hidden layers of width 1024, ReLU as the activation activation function, and with no bias. We also use a CNN with a similar architecture to VGG11 [\[67\]](#page-10-14), with a single linear layer following the convolutional layers instead of three, and this version includes no bias terms. Due to the worse default performance of FCNs in the more difficult semi-synthetic datasets MNIST-CIFAR and Corrupted CIFAR-10, we increase bias-conflicting ratio ρ to 0.25, and increase network width to 2048 for these datasets. The synthetic dataset experiments have been conducted with an FCN of two hidden layers and 200 width.

For Colored MNIST and Corrupted CIFAR-10 we use the train/test splits from the original papers [\[43\]](#page-9-0). Double MNIST and MNIST-CIFAR are created using the canonical splits of these datasets. The training/test splits for these datasets are 60000/10000, 50000/10000, 60000/10000, and 50000/10000, respectively. While we use the original splits for CelebA and Waterbirds datasets, we use a 10000/10000 split for the synthetic parity dataset. The learning rate ranges for the experiments are provided in Tab. [1](#page-16-0) and Tab. [2,](#page-16-1) while all experiments included a batch size of 100, except for experiments with Swin Transformer, where we utilize a batch size of 16. For computing activation statistics, we obtain the post-activation values for the penultimate layer, and compute the compressibility values for 1000 randomly sampled input from the test set, and present the average of these values.

The experiments in this paper were run on 4 NVIDIA A100-PCIE-40GB GPUs for 400 total hours of computation. We will make our source code public upon publication to allow for the replication of our results.

## D. Additional Results and Statistics

#### D.1. Proof of Proposition 1

Proof 1 *Let* y *and* y ′ *for the correct and incorrect classes for a sample,* i.e*.* b = y ′ *for bias-conflicting samples, and* b = y *otherwise. For the mispredicted (bias-conflicting) examples, let* fw[y ′ ] − fw[y] = β > 0*. This implies the following softmax (*π*) output for* y

$$
\pi_y = \frac{e^{kf_{\mathbf{w}}[y]}}{e^{kf_{\mathbf{w}}[y']} + e^{kf_{\mathbf{w}}[y]}} = \frac{1}{1 + e^{k\beta}}.
$$
 (2)

*Notice that* π<sup>y</sup> ≈ e <sup>−</sup>kβ*, as* k → ∞*. Then we can say* ℓ(y, fw(x)) = − log π<sup>y</sup> ≈ kβ.

*For the correctly predicted samples, let* fw[y]−fw[y ′ ] = α > 0*. Similarly to above, note that*

$$
\pi_y = 1 - \pi_{y'} = 1 - \frac{e^{kf_{\mathbf{w}}[y']}}{e^{kf_{\mathbf{w}}[y']} + e^{kf_{\mathbf{w}}[y]}}
$$
(3)

$$
= 1 - \frac{1}{1 + e^{k(f_{\mathbf{w}}[y] - f_{\mathbf{w}}[y'])}} \tag{4}
$$

$$
=1-\frac{1}{1+e^{k\alpha}}\tag{5}
$$

$$
\approx 1 - e^{-\alpha k} \tag{6}
$$

*as* k → ∞*. As* k → ∞*,* − log(πy) ≈ − log(1 − e <sup>−</sup>kα) ≈ e <sup>−</sup>kα*. Assume (without loss of generality) that the margins* β *and* α *are shared by all bias-conflicting and bias-aligned samples in the minibatch. Then, as* k → ∞*,*

$$
\frac{\sum_{\mathbf{x}, y \in \Omega_{\text{bc}}} \ell(y, k f_{\mathbf{w}}(\mathbf{x}))}{\sum_{\mathbf{x}, y \in \Omega_{\text{ba}}} \ell(y, k f_{\mathbf{w}}(\mathbf{x}))} \approx \frac{|\Omega_{\text{bc}}| \cdot k \beta}{|\Omega_{\text{ba}}| \cdot e^{-\alpha k}} = \mathcal{O}(ke^{\alpha k}),\tag{7}
$$

*proving Eq.* [\(1\)](#page-7-7)*.*

#### D.2. Additional Theoretical Results

To investigate the effects of mispredicted bias-conflicting samples on the gradients of subnetworks that rely on core vs. spurious features, we first define *bias-decomposable network*s.

Definition 1 f<sup>w</sup> *is called a bias-decomposable network if* fw(x) = fc(x)+fs(x)+fn(x)*. Here,* f<sup>c</sup> *is the core feature subnetwork,* fc(x)[y]−fc(x)[b] ≥ 0 *with equality iff* y = b*.* fc(x) *is assumed to have converged to a stable decision making rule, i.e.* ∇c(fc(x)[y] − fc(x)[b]) ≈ 0*. In contrast,* f<sup>s</sup> *is the spurious feature subnetwork,* fs(x)[y]−fs(x)[b] ≤ 0 *with equality iff* y = b*.*

Note that although this decomposition is inevitably a simplification, it is a reasonable one in light of previous findings that demonstrate such modularity [\[31\]](#page-9-15), and similar decompositions have been utilized in related research previously [\[57\]](#page-10-9)

In the following proposition, we investigate how the gradient norms for core and spurious subnetworks scale based on this definition and the results in the main paper.

Proposition 2 *Assume* f<sup>w</sup> *is bias-decomposable network. If* f<sup>w</sup> *predicts according to the spurious decision rule,* i.e*.* b = arg max<sup>j</sup> fw(x)[j]*, then for some* α > 0*, as the logitscaling factor* k → ∞*:*

$$
\frac{\sum_{\mathbf{x}, y \in \Omega} \|\nabla_{\mathbf{s}} \ell(y, k f_{\mathbf{w}}(\mathbf{x}))\|}{\sum_{\mathbf{x}, y \in \Omega} \|\nabla_{\mathbf{c}} \ell(y, k f_{\mathbf{w}}(\mathbf{x}))\|} = \mathcal{O}(e^{\alpha k}),\tag{8}
$$

*for some* α > 0*, where* ∥ · ∥ *stands for Frobenius norm.*

Proof 2 *The proof follows from the gradient of the crossentropy loss with respect to the parameters w*<sup>c</sup> *and w*s*. The softmax probability* π<sup>j</sup> *for a class* j *is given by:*

$$
\pi_j = \frac{e^{z_j}}{\sum_i e^{z_i}} \quad \text{where } z_i = k f_w(\mathbf{x})[i] \tag{9}
$$

*The gradient of the loss* ℓ *can be expressed as:*

$$
\nabla_c \ell = k \sum_j (\pi_j - \delta_{jy}) \nabla_c f_c(\mathbf{x})[j] \tag{10}
$$

$$
\nabla_s \ell = k \sum_j (\pi_j - \delta_{jy}) \nabla_s f_s(\mathbf{x})[j] \tag{11}
$$

*where* δjy *is the Kronecker delta, defined as:*

$$
\delta_{jy} = \begin{cases} 1 & \text{if } j = y \\ 0 & \text{if } j \neq y \end{cases}
$$
 (12)

*For bias-conflicting samples, i.e.* x ∈ Ωbc*, as* k → ∞*, the softmax probability* π<sup>b</sup> → 1 *and* π<sup>j</sup> → 0 *for* j ̸= b*. The gradient for the core feature subnetwork converges to:*

$$
\nabla_c \ell \approx k \left( \nabla_c f_c[b] - \nabla_c f_c[y] \right) \tag{13}
$$

<span id="page-14-0"></span>
$$
\approx 0,\tag{14}
$$

*with the latter due to Definition 1. Note that for the spurious feature subnetwork Eq.* [\(13\)](#page-14-0) *implies that the gradient norm scales linearly with* k*:*

$$
\|\nabla_s \ell_{bc}\| = \mathcal{O}(k) \tag{15}
$$

*For the correctly classified bias-aligned samples with margin* α*, the gradient norms for both subnetworks are scaled by this vanishing factor:*

$$
\|\nabla_c \ell_{ba}\| = \mathcal{O}(ke^{-k\alpha})\tag{16}
$$

$$
\|\nabla_s \ell_{ba}\| = \mathcal{O}(ke^{-k\alpha})\tag{17}
$$

<span id="page-15-0"></span>![](_page_15_Figure_0.jpeg)

Figure 18. (Left) Effects of learning rate on OOD performance (unbiased test acc.), network prunability, and representation properties with the Waterbirds dataset.

*As we sum the norms over the minibatch* Ω = Ωbc ∪Ωba*, the total spurious gradient norm (numerator) is:*

$$
\sum_{\Omega} \|\nabla_s \ell\| = \sum_{\Omega_{bc}} \mathcal{O}(k) + \sum_{\Omega_{ba}} \mathcal{O}(ke^{-k\alpha}) = \mathcal{O}(k) \quad (18)
$$

*The total core gradient norm (denominator) is:*

$$
\sum_{\Omega} \|\nabla_c \ell\| = \sum_{\Omega_{bc}} 0 + \sum_{\Omega_{ba}} \mathcal{O}(ke^{-k\alpha}) = \mathcal{O}(ke^{-k\alpha}) \quad (19)
$$

*The final ratio is the ratio of their asymptotic behaviors:*

$$
\frac{\sum_{\Omega} \|\nabla_s \ell\|}{\sum_{\Omega} \|\nabla_c \ell\|} = \frac{\mathcal{O}(k)}{\mathcal{O}(ke^{-k\alpha})} = \mathcal{O}(e^{\alpha k})
$$
 (20)

#### D.3. Value Ranges for Figures

Given that our figures depict multiple variables at the same time, and the results are normalized according to experiments to illuminate the patterns that LR and other interventions create, we present the min and max values the independent and dependent variables take in Tab. [1](#page-16-0) and Tab. [2.](#page-16-1)

#### D.4. Additional Experiment Results

Here we present additional experimental results that were omitted in the main paper due to space concerns. Fig. [15](#page-12-0) present our results with the synthetic moon-star dataset, Fig. [16](#page-13-0) presents our results with the Corrupted CIFAR-10 dataset, Fig. [18](#page-15-0) presents our results with the Waterbirds dataset, and Fig. [19](#page-17-0) includes additional results for comparing the effects of various hyperparameters and regularization methods. Moreover, Fig. [20](#page-17-1) and Fig. [21](#page-17-2) provide a more in-depth look at the performance of models in terms of unbiased test accuracy under various pruning ratios, using column and magnitude pruning respectively.

To show that our results are not due to the utilization of SGD with a constant LR, we present qualitatively identical experiment results in Fig. [22](#page-18-0) using the Colored MNIST dataset and ResNet18 model, where the initial learning rate (x-axis) is multiplied by 0.1 after 1000th iteration. Similar results using Adam optimization algorithm are presented in Fig. [23.](#page-18-1) Additionally, using the same setting, in Fig. [24](#page-18-2) we show that training models for longer according to an additional criterion (CE loss < 1e − 5) produces qualitatively identical results as test accuracy changes very little beyond convergence for both low and high LR models. Finally, Fig. [25](#page-19-0) demonstrates that alternative choices to characterize parameter and representation compressibility, such as (q, κ)-Compressibility, sparsity, and the recently proposed PQ-Index [\[14\]](#page-8-18) produce qualitatively identical results.

Optimizers & LR schedules. We confirm our findings on robustness to SCs and comp. extend to modern and standard training setups. Fig. [28](#page-19-1) (top, left) shows that core benefits persist with ResNet50 (Colored MNIST) using the PSGD (Kron) optimizer and a WSD LR schedule. Fig. [28](#page-19-1) (top, center) shows that our key findings also hold under a standard CIFAR-100 setup with AdamW, cosine annealing, weight decay, and validation set based model selection, addressing concerns about reliance on constant LR SGD. The effects of LR were even more dramatic in both cases, e.g. in the former, the range between low vs. high LRs was ∼22% - ∼91% compared to the original ∼55% - ∼86%. Fig. [28](#page-19-1) (top, right) also shows the performance of the final model after LR reduction by iteration i: the effects of LR are almost completely integrated by step 1000.

Spurious feature utilization of a single neuron. Fig. [28](#page-19-1) (bottom) plots the spurious feature utilization (SFU; computed via attribution methods) of a single neuron under high LRs. The neuron is effectively "reset" with large updates as its SFU increases. Our revision will include a class and bias specific analyses of neuron gradients.

# E. Additional Details and Results Regarding Neural Network Attribution

#### E.1. Attribution Methods in Neural Networks

Attribution methods are a prominent methodology within explainable artificial intelligence (XAI) [\[47\]](#page-9-26). Given a fixed predictor, e.g. a trained neural network, such methods aim to measure the sensitivity of the neural networks' internal representations or output to changes in the input or internal representations. The most straightforward notion of attribution involves the relationship between input and output (called primary or input attribution). For example, in an image classifier, this corresponds to quantifying the sensitivity of the prediction with respect to every pixel and channel. Various methods with their unique advantages and disadvantages have been proposed in the literature; see [\[1\]](#page-8-25).

One of the most commonly used methods include Integrated Gradients (IG) [\[68\]](#page-10-25). Given a predictor f, an input x, and a baseline input x ′ , IG for the i'th component of x is computed as follows:

IG<sub>i</sub>(**x**) := 
$$
(x_i - x'_i) \times \int_{\alpha=0}^1 \frac{\partial f(\mathbf{x}' + \alpha(\mathbf{x} - \mathbf{x}'))}{\partial x_i} d\alpha
$$
.

Intuitively, this corresponds to integrating the sensitivity of the output to changes in x<sup>i</sup> throughout linear interpolation

<span id="page-16-0"></span>

| Dataset       | Model            |        | LR     | Test Acc. |        | Prunability |       | Act. Comp. |       | Avg. CSI |       | Class Sep. |       |
|---------------|------------------|--------|--------|-----------|--------|-------------|-------|------------|-------|----------|-------|------------|-------|
|               |                  | Min.   | Max.   | Min.      | Max.   | Min.        | Max.  | Min.       | Max.  | Min.     | Max.  | Min.       | Max.  |
| MNIST-CIFAR   | FCN              | 0.001  | 0.01   | 35.247    | 35.369 | 0.93        | 0.94  | 0.191      | 0.215 | 0.108    | 0.136 | 0.13       | 0.143 |
| MNIST-CIFAR   | CNN              | 0.001  | 0.2    | 24.507    | 41.717 | 0.326       | 0.902 | 0.173      | 0.513 | 0.079    | 0.225 | 0.049      | 0.149 |
| MNIST-CIFAR   | ResNet18         | 0.001  | 0.25   | 26.233    | 47.513 | 0.311       | 0.737 | 0.294      | 0.343 | 0.225    | 0.371 | 0.115      | 0.255 |
| MNIST-CIFAR   | ResNet50         | 0.005  | 0.1    | 23.287    | 34.358 | 0.378       | 0.515 | 0.25       | 0.34  | 0.143    | 0.25  | 0.065      | 0.143 |
| MNIST-CIFAR   | Wide ResNet101-2 | 0.005  | 0.1    | 24.855    | 33.537 | 0.418       | 0.522 | 0.252      | 0.34  | 0.166    | 0.247 | 0.082      | 0.149 |
| CelebA        | Swin Transformer | 1e-07  | 0.0001 | 40.889    | 48.21  | 0.344       | 0.609 | 0.265      | 0.584 | 0.122    | 0.305 | 0.06       | 0.164 |
| Colored MNIST | FCN              | 0.001  | 0.5    | 72.727    | 76.11  | 0.935       | 0.968 | 0.285      | 0.389 | 0.243    | 0.381 | 0.291      | 0.387 |
| Colored MNIST | CNN              | 0.01   | 0.35   | 88.2      | 91.48  | 0.532       | 0.931 | 0.372      | 0.643 | 0.302    | 0.576 | 0.376      | 0.692 |
| Colored MNIST | ResNet18         | 0.001  | 0.25   | 68.343    | 91.543 | 0.252       | 0.771 | 0.381      | 0.527 | 0.38     | 0.63  | 0.212      | 0.635 |
| Colored MNIST | ResNet50         | 0.001  | 0.1    | 55.687    | 86.38  | 0.284       | 0.489 | 0.334      | 0.524 | 0.198    | 0.458 | 0.07       | 0.509 |
| Colored MNIST | Wide ResNet101-2 | 0.001  | 0.25   | 61.643    | 85.25  | 0.258       | 0.559 | 0.337      | 0.787 | 0.215    | 0.473 | 0.084      | 0.569 |
| Moon-Star     | FCN              | 0.01   | 0.75   | 74.315    | 81.156 | 0.954       | 0.971 | 0.375      | 0.457 | 0.257    | 0.34  | 0.2        | 0.326 |
| Cor. CIFAR-10 | FCN              | 0.0001 | 0.005  | 46.92     | 51.93  | 0.574       | 0.847 | 0.202      | 0.252 | 0.131    | 0.183 | 0.169      | 0.182 |
| Cor. CIFAR-10 | CNN              | 0.001  | 0.1    | 43.555    | 48.503 | 0.39        | 0.861 | 0.189      | 0.341 | 0.144    | 0.387 | 0.14       | 0.329 |
| Cor. CIFAR-10 | ResNet18         | 0.001  | 0.5    | 35.153    | 47.795 | 0.267       | 0.803 | 0.382      | 0.634 | 0.266    | 0.393 | 0.115      | 0.205 |
| Cor. CIFAR-10 | ResNet50         | 0.001  | 0.1    | 36.3      | 45.52  | 0.391       | 0.556 | 0.334      | 0.475 | 0.203    | 0.304 | 0.081      | 0.134 |
| Cor. CIFAR-10 | Wide ResNet101-2 | 0.001  | 0.25   | 37.827    | 47.153 | 0.399       | 0.559 | 0.339      | 0.653 | 0.215    | 0.348 | 0.09       | 0.189 |
| Double MNIST  | FCN              | 0.001  | 0.5    | 69.287    | 72.47  | 0.986       | 1.029 | 0.236      | 0.406 | 0.22     | 0.485 | 0.428      | 0.481 |
| Double MNIST  | CNN              | 0.005  | 0.4    | 83.987    | 94.57  | 0.383       | 0.916 | 0.235      | 0.772 | 0.187    | 0.476 | 0.204      | 0.55  |
| Double MNIST  | ResNet18         | 0.001  | 0.1    | 85.44     | 95.577 | 0.285       | 0.706 | 0.306      | 0.365 | 0.467    | 0.602 | 0.485      | 0.776 |
| Double MNIST  | ResNet50         | 0.001  | 0.1    | 42.045    | 95.733 | 0.228       | 0.498 | 0.16       | 0.256 | 0.152    | 0.392 | 0.172      | 0.68  |
| Double MNIST  | Wide ResNet101-2 | 0.001  | 0.1    | 43.12     | 96.08  | 0.195       | 0.508 | 0.161      | 0.298 | 0.156    | 0.416 | 0.188      | 0.703 |
| Parity        | FCN              | 0.01   | 0.75   | 55.31     | 85.663 | 0.56        | 0.782 | 0.333      | 0.679 | 0.032    | 0.152 | 0.008      | 0.136 |

Table 1. Minimum and maximum values for each dataset-model combination included in our main experiments.

Table 2. Minimum and maximum values for each dataset-model combination included in our regularization experiments.

<span id="page-16-1"></span>

| Dataset       | Model             | HP     |        |        | Test Acc. | Prunability |       | Avg. CSI |       |
|---------------|-------------------|--------|--------|--------|-----------|-------------|-------|----------|-------|
|               |                   | Min.   | Max.   | Min.   | Max.      | Min.        | Max.  | Min.     | Max.  |
| Colored MNIST | Learning Rate     | 0.001  | 0.15   | 72.297 | 92.203    | 0.262       | 0.8   | 0.441    | 0.644 |
| Colored MNIST | High LR + L2 Reg. | 1e-05  | 0.001  | 92.183 | 94.25     | 0.79        | 0.87  | 0.651    | 0.848 |
| Colored MNIST | Batch Size        | 0.01   | 0.3333 | 68.363 | 90.303    | 0.253       | 0.661 | 0.383    | 0.587 |
| Colored MNIST | Momentum          | 0.1    | 0.99   | 68.38  | 91.197    | 0.256       | 0.748 | 0.387    | 0.633 |
| Colored MNIST | L1 Regularization | 1e-05  | 0.001  | 72.887 | 90.347    | 0.275       | 0.864 | 0.458    | 0.859 |
| Colored MNIST | L2 Regularization | 0.0001 | 0.05   | 71.727 | 89.687    | 0.263       | 0.76  | 0.432    | 0.873 |
| Colored MNIST | γ<br>(Focal Loss) | 0.001  | 25.0   | 68.203 | 91.8      | 0.254       | 0.645 | 0.377    | 0.626 |
| Colored MNIST | ρ<br>(ASAM)       | 0.01   | 10.0   | 68.723 | 90.977    | 0.256       | 0.522 | 0.38     | 0.508 |
| MNIST-CIFAR   | Learning Rate     | 0.001  | 0.1    | 24.327 | 47.497    | 0.293       | 0.69  | 0.24     | 0.376 |
| MNIST-CIFAR   | High LR + L2 Reg. | 1e-06  | 0.0001 | 46.947 | 48.327    | 0.677       | 0.727 | 0.384    | 0.497 |
| MNIST-CIFAR   | Batch Size        | 0.01   | 0.3333 | 25.95  | 46.24     | 0.29        | 0.529 | 0.201    | 0.262 |
| MNIST-CIFAR   | Momentum          | 0.1    | 0.99   | 25.887 | 46.397    | 0.29        | 0.666 | 0.226    | 0.354 |
| MNIST-CIFAR   | L1 Regularization | 1e-05  | 0.001  | 24.65  | 46.097    | 0.28        | 0.845 | 0.24     | 0.486 |
| MNIST-CIFAR   | L2 Regularization | 0.0001 | 0.075  | 24.18  | 47.13     | 0.287       | 0.619 | 0.189    | 0.533 |
| MNIST-CIFAR   | γ<br>(Focal Loss) | 0.01   | 25.0   | 25.87  | 40.393    | 0.283       | 0.597 | 0.223    | 0.314 |
| MNIST-CIFAR   | ρ<br>(ASAM)       | 0.01   | 10.0   | 26.3   | 34.053    | 0.286       | 0.769 | 0.225    | 0.261 |
| Double MNIST  | Learning Rate     | 0.001  | 0.25   | 88.13  | 96.39     | 0.282       | 0.801 | 0.529    | 0.631 |
| Double MNIST  | High LR + L2 Reg. | 1e-05  | 0.001  | 96.68  | 97.75     | 0.664       | 0.89  | 0.641    | 0.836 |
| Double MNIST  | Batch Size        | 0.01   | 0.3333 | 85.39  | 96.62     | 0.279       | 0.514 | 0.375    | 0.471 |
| Double MNIST  | Momentum          | 0.1    | 0.99   | 85.58  | 95.71     | 0.278       | 0.755 | 0.471    | 0.57  |
| Double MNIST  | L1 Regularization | 1e-06  | 0.001  | 87.64  | 95.87     | 0.28        | 0.932 | 0.517    | 0.914 |
| Double MNIST  | L2 Regularization | 0.0001 | 0.05   | 86.83  | 97.08     | 0.285       | 0.727 | 0.504    | 0.896 |
| Double MNIST  | γ<br>(Focal Loss) | 0.001  | 25.0   | 85.01  | 95.89     | 0.275       | 0.661 | 0.456    | 0.58  |
| Double MNIST  | ρ<br>(ASAM)       | 0.01   | 10.0   | 85.71  | 95.82     | 0.277       | 0.626 | 0.467    | 0.488 |

<span id="page-17-0"></span>![](_page_17_Figure_0.jpeg)

Figure 19. Comparing various hyperparameters, regularization methods, and losses in terms of OOD robustness, compressibility, and core feature utilization in Double MNIST dataset with a ResNet18 model (top), and Colored MNIST dataset with a ResNet50 model (bottom). y-axes are normalized within each figure for each variable.

<span id="page-17-1"></span>![](_page_17_Figure_2.jpeg)

Figure 20. Effects of column pruning on models trained on Double MNIST (top) and Colored MNIST (bottom) datasets, under various learning rates. x-axes are modified for visualization.

<span id="page-17-2"></span>![](_page_17_Figure_4.jpeg)

Figure 21. Effects of magnitude pruning on models trained on Double MNIST (top) and Colored MNIST (bottom), under various LRs.

<span id="page-18-0"></span>![](_page_18_Figure_0.jpeg)

Figure 22. Effects of learning rate on OOD performance (unbiased test acc.), network prunability, and representation properties with a learning rate annealing setting, where the LR is multiplied by 0.1 after 1000th iteration.

<span id="page-18-1"></span>![](_page_18_Figure_2.jpeg)

Figure 23. Effects of learning rate on OOD performance (unbiased test acc.), network prunability, and representation properties with an Adam optimizer, with β<sup>1</sup> = 0.9, β<sup>2</sup> = 0.999.

<span id="page-18-2"></span>![](_page_18_Figure_4.jpeg)

Figure 24. (Top) Effects of learning rate on OOD performance (unbiased test acc.), network prunability, and representation properties when trained for 100% training accuracy and < 0.00001 training loss. (Bottom) Test accuracy does not meaningfully change beyond convergence (vertical lines correspond to the point where 100% was reached).

from x ′ to x. See [\[68\]](#page-10-25) for a justification of IG's methodology, and see [\[47\]](#page-9-26) for strengths and weaknesses of various attribution methods. To investigate whether our results are an artifact of using IG as our attribution method, we visually compare the attributions computed by Integrated Gradients (IG) and another prominent attribution method, DeepLift [\[66\]](#page-10-26), for CIFAR-10 samples under ResNet18 models in Fig. [14.](#page-12-1) The two methods produce identical results for the purposes of this paper. Both methods are implemented using the captum package for PyTorch framework [\[32\]](#page-9-27).

We can utilize attribution methods for convergent vali-

dation of class-selectivity index as a measure of spurious feature utilization. Although in datasets such as Colored MNIST pixels for spurious and core features overlap, they are distinct in others such as Double MNIST. Thus, we can compute input attribution on Double MNIST and through normalization we can determine how much (*i.e*. what percentage) of models' attribution is on the spurious vs. core feature. We can then see whether the patterns demonstrated by CSI parallel that computed through input attribution. Fig. [26](#page-19-2) shows a comparison of the two metrics across five datasets and LRs for Double MNIST dataset. Remarkably, the two demonstrate qualitatively identical patterns, confirming CSI as a useful metric of core feature utilization.

Creation of attribution maps for CIFAR datasets. We train a ResNet18 model using a low vs. high LR with *10 different seeds* on CIFAR-10 and CIFAR-100 datasets. Then, we extract those samples in the test set which have been correctly predicted by > .75 of the high LR models and < .25 of low LR ones. Then, we investigate the attribution maps of low vs. high LR models in Fig. [8.](#page-6-1)

#### E.2. Additional Attribution Visualizations

We provide additional visualization of attributions for our experiments in the main paper; for Colored MNIST (Fig. [29\)](#page-20-0), MNIST-CIFAR (Fig. [30\)](#page-20-1), Double MNIST (Fig. [31\)](#page-21-0), CelebA (Fig. [32\)](#page-21-1), CIFAR-10 (Fig. [33\)](#page-22-0), and CIFAR-100 (Fig. [34\)](#page-22-1) datasets. Notice that as in the main paper, low learning rate (LR) models are more likely to focus on spurious features compared to high LR models.

<span id="page-19-0"></span>![](_page_19_Figure_0.jpeg)

Figure 25. Utilizing alternative notions of parameter and representation compressibility such as prunability, (q, κ)-Compressibility (with q = 2, κ = 0.1), sparsity, and the recently proposed PQ-Index (with p = 2, q = 1).

<span id="page-19-2"></span>![](_page_19_Figure_2.jpeg)

Figure 26. Comparing CSI vs. input attribution to core features (%), using Integrated Gradients.

![](_page_19_Figure_4.jpeg)

Figure 27. Examining the effect of LR on unbiased test accuracy and BC loss ratio in ResNet18 and Double MNIST dataset, as well as ResNet50 and Colored MNIST dataset.

<span id="page-19-1"></span>![](_page_19_Figure_6.jpeg)

Figure 28. Additional experiments and analyses.

<span id="page-20-0"></span>![](_page_20_Figure_0.jpeg)

Figure 29. Attributions of trained ResNet18 models on Colored MNIST dataset.

<span id="page-20-1"></span>![](_page_20_Figure_2.jpeg)

Figure 30. Attributions of trained ResNet18 models on MNIST-CIFAR dataset.

<span id="page-21-0"></span>![](_page_21_Figure_0.jpeg)

Figure 31. Attributions of trained ResNet18 models on MNIST-CIFAR dataset.

<span id="page-21-1"></span>![](_page_21_Figure_2.jpeg)

Figure 32. Attributions of trained Swin Transformer models on CelebA dataset.

<span id="page-22-0"></span>![](_page_22_Figure_0.jpeg)

Figure 33. Attributions of trained ResNet18 models on CIFAR-10 dataset.

<span id="page-22-1"></span>![](_page_22_Figure_2.jpeg)

Figure 34. Attributions of trained ResNet18 models on CIFAR-100 dataset.